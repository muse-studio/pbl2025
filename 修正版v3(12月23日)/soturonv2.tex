% !TEX root = _main.tex
% ========================================
% 卒業論文　本文
% ========================================
\section{はじめに}
舞台演出やライブパフォーマンスにおいて、照明は音楽表現を視覚的に拡張し、観客の没入感を高める重要な要素である。照明の色や明暗、動きは演奏の意図や感情を強調し、音響のみでは伝わりにくい表現を補完する役割を果たす。しかし、従来の舞台照明は照明オペレーターによる手動操作に大きく依存しており、演奏内容に即応した柔軟な制御が難しいという課題がある。特に、小規模な演奏環境や個人演奏においては、専任の照明オペレーターを配置することが困難であり、演奏者自身の表現意図を照明に反映させる手段が限られている。

このような背景から、演奏そのものを入力として照明を自動制御する仕組みが求められている。音楽演奏には音響的特徴だけでなく、身体動作に基づく表現も含まれており、これらを照明制御に統合することで、より一体感のある演出が実現できると考えられる。しかし、音響信号や演奏動作と照明演出との対応関係は複雑であり、単純なルールベースの制御では十分に表現できない可能性がある。

そこで本研究では、ヴァイオリン演奏に着目し、音響信号と身体動作を組み合わせたリアルタイム照明制御システムの構築を目的とする。具体的には、マイクから入力されたヴァイオリン音響信号から、スペクトル重心、スペクトル帯域幅、MFCC、ZCR などの音響特徴量を抽出し、機械学習モデルを用いて照明の RGB 値、明るさ、点滅速度といった照明特徴量を予測する。また、スマートフォンのモーションセンシングアプリ ZIG-SIM\cite{zigsim} を用いて、演奏者のヴァイオリン演奏における弓の上下動作を角速度として取得し、照明の上下動作に直接反映させる。

本研究の着想は、筆者自身の「自分のヴァイオリン演奏だけで照明を直接動かしてみたい」という個人的関心に基づいている。ヴァイオリンを対象とした理由として、筆者自身が演奏経験を有しており、演奏者の立場からシステムの反応性や表現性を評価できる点が挙げられる。

以上より本研究では、ヴァイオリン演奏音から抽出した音響特徴量に基づいて照明の色、明るさ、点滅速度を制御すると同時に、演奏者の運弓動作を照明の上下動作として直接反映することで、音楽表現と身体表現が一体となった照明演出の実現を目指す。

\section{関連研究}
音楽ライブやコンサートにおける照明演出は、観客の没入感や一体感を高める重要な要素であり、従来は照明デザイナーやオペレータの経験や感性に基づいて設計されてきた。しかし、小規模公演や即興性の高い演奏では十分な準備時間や人員を確保することが難しく、音楽情報処理技術を用いた照明演出の自動化が有効な手段として注目されている。
\subsection{音響信号に基づく照明演出の自動化}

音響信号に基づく照明制御に関する研究として、月東らは楽曲音源からリズムの強調箇所を検出し、それを照明演出へ反映する手法を提案している\cite{1}。
この研究では、楽曲をドラム、ベース、ギター、ボーカルの各パートに音源分離し、それぞれのオンセットエンベロープを算出した上で、複数パートで同時に強いオンセットが観測されるタイミングを「キメ」と定義している。検出されたキメを基点として、照明のフラッシュや明滅といった瞬間的な演出を付与し、主観評価実験によりリズムの一体感や迫力の向上を確認している。

この手法は高い時間解像度でリズム的アクセントを捉えられる点に特徴がある一方で、照明制御はキメの有無に依存した離散的なものに留まっており、演奏中の表現変化を連続的に反映することは難しい。また、演奏者の身体的動作は入力として扱われていない。

\subsection{楽曲の印象・意味内容に基づく照明生成}
楽曲全体の印象や意味内容を照明演出に反映する研究も多く報告されている。神野らは、歌詞および曲調の印象に基づいて照明演出を自動生成するシステムを提案している\cite{2}。

この研究では、歌詞を形態素解析した上で分散表現ベクトルに変換し、言語イメージスケールを用いて照明色候補を選択する。さらに、楽曲の長短調やテンポに基づいて色の優先度や配置を決定し、BPMや歌詞のネガティブ・ポジティブ度を用いて明度を調整することで、照明の色、配置、明るさを総合的に決定している。この手法により、照明デザイナーの感性に依存しない一貫性のある演出が実現されている。

一方で、照明変化は小節やフレーズ単位といった比較的長い時間スケールで行われるため、演奏中の瞬間的な表現変化や演奏者の身体動作を直接反映することは想定されていない。

\subsection{深層学習を用いた照明演出生成}
近年では、照明演出を生成タスクとして捉え、深層学習モデルを用いて照明パラメータを自動生成する研究も行われている。月東らの研究内でも言及されている Zhao らの手法では、楽曲特徴量系列を入力として照明の色や明るさの時系列データを生成する生成モデルが提案されている\cite{5}。

このようなアプローチは、表現力や多様性の高い照明演出を実現できる可能性を示しているが、学習データへの依存度が高く、生成過程の解釈が困難であるという課題がある。また、演奏者の身体動作は入力として扱われていない。

\subsection{演奏者操作によるリアルタイム照明制御}
演奏者自身が照明演出に直接関与する研究として、瀧口らはドラム音やMIDIフットコントローラーを用いて演奏中に照明を操作可能なシステムを提案している\cite{4}。
この研究では、バスドラム音の音量が閾値を超えた際に照明を点灯させる手法や、足元のコントローラー操作によって照明の色や輝度、ストロボ効果を変更する仕組みが実装されている。

この手法により、即興演奏に柔軟に対応できる照明演出が実現されている一方で、演奏者が照明操作を意識する必要があり、演奏負荷が増加する可能性がある。また、演奏動作そのものを自動的に取得・利用する仕組みは含まれていない。

\subsection{身体動作と照明演出の連動}
演奏者の身体動作と照明演出を連動させる研究として、浅田らはオンラインライブ空間において、演奏者の身体情報を用いて照明や振動装置を制御するシステムを提案している\cite{5}。
この研究では、ギター演奏者の腕の角度に応じてムービングライトの向きを変化させるなど、身体動作を照明の物理的な動きに対応付けることで、演奏者と観客の一体感を高めている。

しかし、音響特徴量と身体動作を統合的に扱う設計にはなっておらず、音楽表現と照明表現の対応関係は限定的である。

\subsection{本研究の位置づけ}
以上の関連研究から、音響特徴量に基づく照明制御、楽曲印象に基づく照明生成、演奏者操作型および身体動作連動型の照明演出はそれぞれ発展してきたものの、音響情報と演奏者の身体動作を同時に統合し、連続的かつリアルタイムに照明を制御する研究は十分に行われていないことが分かる。

そこで本研究では、ヴァイオリン演奏音の特徴量と演奏者の運弓動作に応じてステージ上の照明がリアルタイムに変化する照明制御システムを提案する。

\section{リアルタイム照明制御システムの実装}
本章では、本研究で提案したヴァイオリン演奏に基づくリアルタイム照明制御手法を実現するために構築したシステム構成(図1)について詳述する。
本システムは、ヴァイオリン演奏に伴う音響特徴量および演奏動作情報を入力とし、機械学習モデルによって推定された照明特徴量をDMX信号として出力することで、演奏に同期した照明演出を自動生成するものである。

\subsection{システム設計の基本方針}
本システムの設計にあたり、以下の方針を重視した。
\begin{enumerate}
	\item[(1)]演奏中に人手操作を必要としないこと
\end{enumerate}

本システムでは、演奏者が演奏中に照明操作を意識したり、追加の入力デバイスを操作したりする必要がない自律的な構成を目指した。これにより、演奏者は音楽表現そのものに集中でき、照明演出が演奏の妨げとなることを防いでいる。
\begin{enumerate}
	\item[(2)]遅延を極力抑え、リアルタイム性を確保すること 
\end{enumerate}

ヴァイオリン演奏の微細な表現変化を照明に即座に反映させるため、システム全体の処理遅延を可能な限り低減する設計とした。
\begin{enumerate}
	\item[(3)]実演が可能な安定した動作を実現すること
\end{enumerate}

本研究では実際の演奏会形式での使用を想定しているため、長時間の動作においても処理が停止せず、安定して照明制御が行われることを重要な設計要件とした。
そのため、計算負荷の大きい処理を最小限に抑え、汎用的なハードウェア環境上でも安定して動作する構成を採用している。

\begin{figure}[H]
	\centering
	\includegraphics[width=\hsize]{system-structure.png}
	\caption{リアルタイム照明制御システム全体処理フロー図}
	\label{fig:system-structure}
\end{figure}


\subsection{ハードウェア構成}
本システムは以下のハードウェアで構成される。
\begin{enumerate}
	\item[・]PC(Windows 11)
	\item[・]コンデンサマイク(音声入力)
	\item[・]スマートフォン(ZIG-SIM用)
	\item[・]USB-DMXインタフェース
	\item[・]DMX対応照明機器4台(図2)
\end{enumerate}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.25\textwidth]{light.png}
	\caption{DMX対応照明機器}
	\label{fig:light}
\end{figure}


マイクは演奏音をリアルタイムで取得するために使用し、スマートフォンはヴァイオリン演奏時の運弓動作を取得するために使用する。
照明機器は地面にステージ四方に配置され、床から天井に向けて演奏者を中心に照射する構成とした。

\subsection{ソフトウェア構成}
本システムはPython\cite{python}で実装され、すべての処理を単一プログラム上で統合している。
使用した主なライブラリを以下に示す。
\begin{enumerate}
	\item[・]sounddevice\cite{sounddevice}：音声ストリーム取得
	\item[・]librosa\cite{librosa}：音響特徴量抽出
	\item[・]numpy\cite{numpy}, pandas\cite{pandas}：数値処理、データ管理
	\item[・]scikit-learn\cite{scikitlearn}：機械学習モデル推論
	\item[・]socket\cite{socket}：UDP通信
	\item[・] ftd2xx\cite{ftd2xx}：DMX信号出力
\end{enumerate}

\subsection{音声入力処理}
音声入力は sounddevice.InputStream を用いて行う。
サンプリング周波数は22,050 Hz、モノラル入力とし、一定数のサンプルが蓄積されるとコールバック関数内でリングバッファに格納される。

リングバッファを採用することで、音声の欠落を防ぎつつ、安定したフレーム分割が可能となっている。
バッファ内のデータが22,050サンプルに達した時点で、1秒分の音声フレームとして切り出される。

また、音声フレームごとにRMS値を算出し、あらかじめ設定した閾値以下の場合は無音区間と判定する。
無音区間では照明更新を行わず、直前の照明状態を保持する。

\subsection{音響特徴量抽出}
各音声フレームに対して、librosaライブラリを用いて10種類の音響特徴量を算出する。
本研究で使用した特徴量それぞれの算出方法は以下の通りである。
\subsubsection{ゼロ交差率(ZCR)}

ゼロ交差率（ZCR）は、時間領域信号が正から負、または負から正へ符号反転する回数を表す特徴量であり、音の粗さやノイズ成分の多さを示す指標である。
フレーム長を N、時間領域信号をx[n] とすると、ZCR は次式で定義される。

\begin{figure}[H]
    \centering
    \includegraphics[width=\hsize]{zcr.png}
    \caption{ゼロ交差率 (ZCR) の定義式}
    \label{fig:zcr}
\end{figure}

ここで$\mathrm{sgn}(\cdot)$は符号関数である。
本研究では、librosa の librosa.feature.zero\_crossing\_rate() を用いて ZCR を算出し、各フレームにおける平均値を特徴量として採用した。

\subsubsection{スペクトル重心(Spectral Centroid)}

スペクトル重心は、周波数スペクトルにおけるエネルギー分布の重心位置を示す特徴量であり、音の明るさや鋭さと関連がある。
短時間フーリエ変換（STFT）によって得られた振幅スペクトルを $|X(k)|$、周波数ビンを$k$とすると、スペクトル重心 $C$ は次式で定義される。

\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\hsize]{spectralcentroid.png}
	\caption{スペクトル重心 (Spectral Centroid) の定義式}
	\label{fig:spectralcentroid}
\end{figure}

本研究では、librosa の librosa.feature.spectral\_centroid()を用いてスペクトル重心を算出し、各フレームにおける平均値を特徴量とした。

\subsubsection{スペクトル帯域幅(SpectralBandwidth)}
スペクトル帯域幅は、スペクトル重心を中心とした周波数成分の広がりを表す特徴量であり、音色の拡散度合いを示す指標である。
次数 p=2 の場合、スペクトル帯域幅 B は次式で表される。

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\hsize]{spectralbandwidth.png}
    \caption{スペクトル帯域幅 (Spectral Bandwidth) の定義式}
    \label{fig:spectralbandwidth}
\end{figure}

本研究では、librosa の librosa.feature.spectral\_bandwidth()を用いて算出した値の平均を特徴量として用いた。

\subsubsection{スペクトルコントラスト(SpectralContrast)}
スペクトルコントラストは、周波数帯域ごとのピーク成分と谷成分の差を表す特徴量であり、倍音構造の明瞭さや音色のコントラストを表現する。
librosaのlibrosa.feature.spectral\_contrast()を用いて複数帯域に分割したスペクトルコントラストを算出し、その平均値を特徴量として使用した。

\subsubsection{スペクトルフラットネス(SpectralFlatness)}

スペクトルフラットネスは、スペクトルの平坦さを示す特徴量であり、音が純音的かノイズ的かを判別する指標である。
振幅スペクトルを$|X(k)|$とすると、スペクトルフラットネス F は次式で定義される。

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\hsize]{spectralflatness.png}
    \caption{スペクトルフラットネス (Spectral Flatness) の定義式}
    \label{fig:spectralflatness}
\end{figure}

本研究では、librosa の librosa.feature.spectral\_flatness()を用いて算出した。

\subsubsection{スペクトルロールオフ（SpectralRolloff）}

スペクトルロールオフは、全スペクトルエネルギーの一定割合が含まれる周波数を示す特徴量である。
本研究では、全エネルギーの 85\% を含む周波数をロールオフ周波数として定義し、librosa のlibrosa.feature.spectral\_rolloff()を用いて算出した。

\subsubsection{音量}

音量（RMS）は、音声信号の振幅エネルギーを表す特徴量である。
時間領域信号 x[n] に対して、RMS は次式で定義される。

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\hsize]{rms.png}
    \caption{音量（RMS）の定義式}
    \label{fig:rms}
\end{figure}

本研究では、librosa の librosa.feature.rms()を用いて算出した。

\subsubsection{メル周波数ケプストラム係数（MFCC）}

MFCC は、人間の聴覚特性を考慮したメル周波数尺度に基づく特徴量であり、音色を表現する代表的な指標である。
本研究では、音声信号に対してメルフィルタバンクを適用し、対数パワースペクトルに離散コサイン変換（DCT）を施すことで MFCC を算出する。
librosa の librosa.feature.mfcc() を用いて 20 次元の MFCC を算出し、各次元の平均値を特徴量として採用した。

さらに、時間的変化を捉えるため、MFCC の一次差分および二次差分を以下の式で定義し、librosa.feature.delta() を用いて算出した。

\begin{figure}[H]
    \centering
    \includegraphics[width=\hsize]{mfccdelta.png}
    \caption{MFCC の一次差分および二次差分の定義式}
    \label{fig:mfccdelta}
\end{figure}

\subsubsection{クロマ特徴量（Chroma）}

クロマ特徴量は、12 音階ごとのエネルギー分布を表す特徴量であり、音高や和声的特徴を反映する。
本研究では、librosa の librosa.feature.chroma\_stft()を用いてクロマ特徴量を算出し、その平均値を特徴量として使用した。

\subsubsection{トーナルネットワーク特徴量（Tonnetz）}
トーナルネットワーク特徴量（Tonnetz）は、調性や和声構造を表現する特徴量である。
調波成分を抽出した音声信号に対して、librosa の librosa.feature.tonnetz()を用いて6 次元の Tonnetz 特徴量を算出し、各次元の平均値を特徴量として採用した。

\subsection{演奏動作取得}
本システムでは、演奏者の弓の上下運動(上げ弓・下げ弓)(図9・図10)を取得するために、スマートフォンをZIG-SIMセンサとして利用している。
スマートフォンにはジャイロセンサが搭載されており、X軸方向の角速度を取得することで弓の前後運動を検出する。

\begin{figure}[H]
    \centering
    \includegraphics[width=\hsize]{up.png}
    \caption{上げ弓}
    \label{fig:up}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\hsize]{down.png}
    \caption{下げ弓}
    \label{fig:down}
\end{figure}

\subsubsection{ハードウェア配置}
スマートフォンは演奏者の右腕に装着し、弓を持つ手首付近に位置させる。
これにより、弓の上下運動が腕の回転として反映されるため、ジャイロセンサで容易に検出可能となる。

\subsubsection{データ通信方式}
スマートフォンから取得したジャイロデータはUDP通信を用いてPCに送信する。
UDPはTCPに比べて通信遅延が少なく、リアルタイム性を重視した設計である。
送信ポートは55555番を使用し、PC側は非同期で受信する専用スレッドを設け、常時データを待ち受ける。

受信データはJSON形式で送信され、ジャイロ情報は "sensordata":{"gyro":{"x":値}} という構造になっている。
PC側では、受信データをデコードしてPython辞書型に変換後、X軸の角速度値を取り出す。

\subsubsection{運弓方向判定アルゴリズム}
弓方向の判定は単純な閾値判定ではなく、以下の2つの条件を組み合わせて精度を向上させている。

\begin{enumerate}
	\item[\textbf{(1)}]\textbf{角速度の絶対値による判定}
\end{enumerate}

閾値 $\mathrm{THRESH\_GX} = 0.01$ を設定

$\mathrm{gx} > \mathrm{THRESH\_GX}$ なら上げ弓、$\mathrm{gx} < -\mathrm{THRESH\_GX}$ なら下げ弓

\begin{enumerate}
	\item[\textbf{(2)}]\textbf{角速変化量による判定}
\end{enumerate}

前フレームとの変化量 $\mathrm{delta} = \mathrm{gx} - \mathrm{prev\_gx}$ を計算

閾値 $\mathrm{THRESH\_DELTA} = 0.009$ を超えた場合も上げ弓／下げ弓と判定

この2つを併用することで、微小な振動や演奏の微妙なタッチでも誤判定を避け、弓の動作を正確に捉えることができる。

\subsubsection{リアルタイム処理}
ZIG-SIMデータ受信スレッドは10 ms周期でUDPパケットを受信し、取得した角速度に基づいて弓方向を更新する。
最新の弓方向はグローバル変数$\mathrm{bow\_direction}$に保存され、照明制御スレッドで参照される。

$\mathrm{bow\_direction}$ の値は以下の2状態を持つ。

\begin{enumerate}
	\item[(1)]1：上げ弓
	\item[(2)]-1：下げ弓
\end{enumerate}

これにより、演奏中の弓の動きに応じてライトの上下動作を同期させることが可能となる。

このように、本システムではジャイロセンサによる演奏動作取得をリアルタイムで行い、音響特徴量と組み合わせて照明制御に反映している。

\subsection{機械学習モデルによる照明特徴量推定}
抽出された音響特徴量を用いて、機械学習モデルにより照明特徴量を推定する。
モデルは以下の3つの照明特徴量を出力する。また。機械学習モデルについては次章で詳述する。

\begin{enumerate}
	\item[・]RGB(色)
	\item[・]Brightness(明るさ)
	\item[・]点滅速度
\end{enumerate}
推定値は0〜1の正規化値として出力され、DMX出力時に0〜255の整数値へスケーリングされる。

\subsection{DMX信号生成と照明制御}
本節では、機械学習モデルおよび演奏動作推定によって得られた照明特徴量を、実際の照明機器に反映させるためのDMX信号生成および制御方法について詳細に述べる。
DMX512は舞台照明分野において広く用いられている通信規格であり、本研究ではこの標準規格を用いることで、実際のステージ環境に近い条件での実演を可能にしている。

\subsubsection{DMX512プロトコルの概要}
DMX512は、最大512チャンネルの制御データを1フレームとして送信する一方向通信プロトコルである。
各チャンネルは0〜255の8bit値を持ち、照明機器の色、明るさ、点滅速度を制御することができる。

\subsubsection{DMXハードウェアインタフェース}
DMX信号の出力には、USB接続のDMXインタフェースを使用した。
本研究ではFTDI社製チップを搭載したデバイスを用い、Pythonからftd2xxライブラリを介して制御を行っている。

このインタフェースでは、DMX信号送信の前にブレーク信号を発生させる必要がある。
プログラム内では、setBreakOn()およびsetBreakOff()を用いてブレークを明示的に制御し、その後に512チャンネル分のデータを一括送信する構成とした。

\subsubsection{照明特徴量からDMX値への変換}
照明特徴量（RGB値, Brightness, 点滅速度）は、DMXチャンネルに対応する値へ変換される。

これらは連続値として出力されるため、DMXチャンネルに割り当てる際には0〜255の整数値へ正規化・クリッピング処理を行う。

具体的には、各値に対して以下の処理を適用する。

\begin{enumerate}
	\item[(1)]正規化：各特徴量の最小値・最大値を基に0〜1の範囲にスケーリング
	\item[(2)]スケーリング：0〜1の値を255倍して0〜255の範囲に変換
	\item[(3)]クリッピング：小数点以下を切り捨て、整数値に変換
\end{enumerate}

これにより、モデルの出力値が異常な場合でも照明機器に過度な負荷がかからないよう安全性を確保している。

\subsubsection{複数照明機器への同時制御}

本システムでは、4台の照明機器を同時に制御する構成とした。
各照明機器は異なるDMXアドレスに割り当てられており、同一の照明特徴量を用いて各機器へ同時に信号を送信する。

DMX配列は513バイトで構成され、照明機器ごとに以下のようなチャンネル割り当てを行っている。

\begin{enumerate}
	\item[・　チャンネル1]：未使用
	\item[・　チャンネル2]：上下動作
	\item[・　チャンネル3]：点滅速度
	\item[・　チャンネル4]：R成分
	\item[・　チャンネル5]：G成分
	\item[・　チャンネル6]：B成分
	\item[・　チャンネル7]：明るさ
\end{enumerate} 

\subsubsection{演奏動作情報との統合制御}
音響特徴量から推定された照明特徴量とは別に、ZIG-SIMによって取得した運弓方向情報をDMX制御に直接反映させている。

具体的には、運弓方向に応じて上下動作のDMXチャンネル（チャンネル2）の値を変更し、照明の上下動作を演奏者の弓の動きに同期させている。
\begin{enumerate}
	\item[・]上げ弓：DMX値を120に設定（照明上昇）(図11参照)
	\item[・]下げ弓：DMX値を0に設定（照明下降）(図11参照)
\end{enumerate}

とすることで、演奏者の身体動作と照明の物理的動きを同期させている。
この設計により、音だけでなく演奏動作そのものが照明演出に影響を与える構造となっている。

\begin{figure}[H]
	\centering
	\includegraphics[width=0.3\textwidth]{lightupdown.png}
	\caption{照明上下動作}
	\label{fig:lightupdown}
\end{figure}

\subsubsection{リアルタイム出力制御と更新周期}
DMX信号の送信は約10ms周期で実行される。
これは、人間が遅延として知覚しにくい範囲であり、演奏と照明がほぼ同時に変化しているように感じられる。

また、照明出力処理は独立したスレッドで動作し、音声処理やモデル推論の遅延の影響を受けにくい設計となっている。
これにより、安定した照明制御が可能となった。

\section{照明予測モデル}
本章では、3.7節で述べたヴァイオリン演奏音から照明特徴量を予測する機械学習モデルの構築方法について詳述する。
\begin{figure}[H]
	\centering
	\includegraphics[width=\hsize]{dataflow.png}
	\caption{照明予測モデル構築のためのデータ生成フロー}
	\label{fig:dataflow}
\end{figure}

本研究では、音響特徴量を入力とし、それらの照明特徴量を予測するモデルを用いて照明の制御を行っている。
本研究で機械学習モデルを用いたのは、先行研究には音響特徴量から照明特徴量を直接予測し、リアルタイムの自動照明制御として機能させる学習ベースの枠組みが十分に見られず、新しいアプローチとなると考えたためである。
最終的に構築されたモデルは、リアルタイムの演奏音を入力として照明を自動制御する「照明予測モデル」として機能する。
図12に、照明予測モデル構築のためのデータ生成フローを示す。この図に基づき、各ステップを詳細に説明する。

\subsection{データベースの構築}
まずは、モデルを学習させるためのデータベースの構築を行う。

\subsubsection{映像データの収集}
照明予測モデルの学習には、音響特徴量と対応する照明特徴量のペアが必要である。
そのためのデータソースとして、実際のヴァイオリン演奏におけるライブ映像データを用意した。
これらは実際の照明を伴う演奏映像である。
各映像の詳細は以下の通りである。

\begin{figure}[H]
	\centering
	\includegraphics[width=\hsize]{senbonzakura.png}
	\caption{*千本桜* 演奏映像}
	\label{fig:senbonzakura}
\end{figure}

ライブ映像データの1つ目は、ヴァイオリン演奏曲「千本桜\cite{senbonzakura}」の演奏を記録した映像(図13)である\cite{stradivariussenbonzakura}。

2つ目のライブ映像データは、ヴァイオリンソロによるクラシック楽曲「ソナタ第三番\cite{recochokusonata}」を演奏した映像(図14)である\cite{stradivariussonata}。
\begin{figure}[H]
	\centering
	\includegraphics[width=\hsize]{sonata.png}
	\caption{*Sonata* 演奏映像}
	\label{fig:sonata}
\end{figure}

3つ目のライブ映像データは、ヴァイオリンソロによる楽曲「スプラッシュ\cite{recochokusplash}」を演奏した映像(図15)である\cite{stradivariussplash}。

\begin{figure}[H]
	\centering
	\includegraphics[width=\hsize]{splash.png}
	\caption{*スプラッシュ* 演奏映像}
	\label{fig:splash}
\end{figure}

これらの映像に対して、音響特徴量と照明特徴量を抽出するための処理を行い、照明予測モデルの学習データを生成する。

\subsubsection{音響特徴量の抽出}
\begin{enumerate}
	\item[\textbf{(1)}]\textbf{ヴァイオリン音源の分離}
\end{enumerate}

ライブ映像には、会場の反響音、観客ノイズ、伴奏音など、ヴァイオリン以外の音が含まれる。
音響特徴量は不要な音が混入したまま学習するとモデルの精度劣化を招く。
そこで音源分離モデルDemucs\cite{mucs}を用い、音声からヴァイオリン成分を抽出した。

\begin{enumerate}
	\item[\textbf{(2)}]\textbf{ヴァイオリン特徴量の抽出}
\end{enumerate}

抽出したヴァイオリン音声に対して、1秒ごとに音響特徴量を算出した。
本節ではその具体的な処理内容を述べる。

まず、音声ファイルを Pythonのlibrosaライブラリ を用いて読み込む。
この際、サンプリングレートは 44.1 kHz、チャンネル数は モノラル に統一する。
読み込んだ波形データおよびサンプリングレートから音声全体の長さ（秒）を算出し、その長さに応じて 0 秒から終端まで 1 秒刻みの時間インデックスを生成する。

次に、生成した各時間インデックスに対して以下の処理を行う。

まずは各 1 秒セグメントごとに extractAdvancedFeatures 関数を用いて3.5節で述べた音響特徴量を算出する。
算出されたすべての音響特徴量は一つの辞書にまとめられ、最終的に「1 行を 1 秒セグメント、1 列を 1 種類の音響特徴量」とする特徴量テーブルを生成する。

この特徴量テーブルは CSV 形式で保存され、学習用 CSV データとして構築される。
表1に、各ライブ映像から抽出された音響特徴量のCSVデータの一部を示す。

\begin{table*}[t]
\centering
\small
\caption{各映像の音響特徴量のCSVデータ(一部抜粋)}

\begin{tabular}{c|cccccccccc}
\hline
\multicolumn{11}{c}{千本桜} \\
\hline
\hline
Time [s] & Centroid & Contrast & Bandwidth & Rolloff & ZCR & RMS &
MFCC\_1 & Delta\_MFCC\_1 & Chroma\_1 & Tonnetz\_1 \\
\hline
1.0 & 8894 & 14.5 & 6149 & 16606 & 0.28 & 0.001 & -948 & -4.54 & 0.67 & 0.03 \\
2.0 & 6722 & 17.9 & 3928 & 11385 & 0.23 & 0.005 & -661 & 4.44 & 0.76 & 0.12 \\
3.0 & 5257 & 23.2 & 3046 & 8144.0 & 0.18 & 0.033 & -385 & 0.52 & 0.57 & 0.20 \\
4.0 & 4459 & 21.7 & 2653 & 6389.1 & 0.17 & 0.023 & -398 & 2.95 & 0.75 & 0.26 \\
5.0 & 3987 & 19.5 & 2593 & 5936.9 & 0.12 & 0.028 & -358 & -1.11 & 0.28 & 0.32 \\
6.0 & 3678 & 18.3 & 3031 & 7210.9 & 0.16 & 0.050 & -347 & -0.45 & 0.37 & 0.20 \\
7.0 & 3521 & 17.8 & 2742 & 7049.2 & 0.17 & 0.039 & -410 & -0.38 & 0.30 & 0.27 \\
8.0 & 3400 & 16.7 & 2896 & 7186.3 & 0.14 & 0.008 & -471 & -2.29 & 0.43 & 0.18 \\
\hline
\multicolumn{11}{c}{ソナタ第三番} \\
\hline
\hline
Time [s] & Centroid & Contrast & Bandwidth & Rolloff & ZCR & RMS &
MFCC\_1 & Delta\_MFCC\_1 & Chroma\_1 & Tonnetz\_1 \\
\hline
1.0 & 1947 & 9.04 & 1705 & 3958.3 & 0.03 & 0.001 & -1068 & 3.25 & 0.04 & -0.05 \\
2.0 & 2898 & 20.1 & 3562 & 4256.3 & 0.07 & 0.000 & -809.0 & 1.43 & 0.12 & -0.20 \\
3.0 & 1695 & 19.9 & 2284 & 2615.9 & 0.03 & 0.001 & -675.9 & 1.79 & 0.35 & -0.07 \\
4.0 & 1560 & 20.1 & 1951 & 2630.2 & 0.03 & 0.002 & -618.0 & -0.26 & 0.43 & 0.27 \\
5.0 & 1728 & 19.1 & 1909 & 2767.1 & 0.05 & 0.002 & -606.8 & -0.07 & 0.56 & -0.04 \\
6.0 & 1887 & 19.5 & 1940 & 2967.1 & 0.05 & 0.002 & -604.9 & -0.06 & 0.28 & 0.02 \\
7.0 & 2113 & 19.0 & 2080 & 3132.7 & 0.06 & 0.002 & -585.3 & 0.05 & 0.22 & 0.03 \\
8.0 & 2475 & 19.9 & 2305 & 3629.7 & 0.07 & 0.002 & -602.9 & -0.41 & 0.08 & -0.01 \\
\hline
\multicolumn{11}{c}{スプラッシュ} \\
\hline
\hline
Time [s] & Centroid & Contrast & Bandwidth & Rolloff & ZCR & RMS &
MFCC\_1 & Delta\_MFCC\_1 & Chroma\_1 & Tonnetz\_1 \\
\hline
1.0 & 1002 & 0.00 & 579.2 & 1750.0 & 0.00 & 0.000 & -1131 & 0.00 & 0.08 & -0.01 \\
2.0 & 3582 & 8.98 & 3553 & 8076.6 & 0.00 & 0.008 & -1055 & 1.65 & 0.27 & 0.07 \\
3.0 & 1603 & 2.24 & 1542 & 3564.3 & 0.00 & 0.001 & -1109 & -0.29 & 0.16 & -0.02 \\
4.0 & 1564 & 15.0 & 2032 & 2707.4 & 0.03 & 0.032 & -774.5 & 4.52 & 0.23 & 0.14 \\
5.0 & 1798 & 18.4 & 2226 & 2553.0 & 0.05 & 0.054 & -641.0 & 1.15 & 0.37 & 0.31 \\
6.0 & 1893 & 20.1 & 2182 & 2354.2 & 0.05 & 0.055 & -667.7 & -0.62 & 0.39 & 0.27 \\
7.0 & 2060 & 20.1 & 2190 & 2775.3 & 0.05 & 0.057 & -634.1 & -0.16 & 0.30 & 0.33 \\
8.0 & 759.7 & 19.8 & 2239 & 3193.1 & 0.05 & 0.062 & -621.3 & -1.14 & 0.25 & 0.17 \\
\hline
\end{tabular}

\end{table*}


\subsubsection{照明特徴量の抽出}
続いて、各映像からRGB値 (色)、Brightness(明るさ)、点滅速度の 3 種類の照明特徴量を1秒ごとに抽出し、学習用CSVデータとして構築した。
本節では、照明特徴量の抽出手法、CSVデータ構造について詳述する。

\begin{enumerate}
	\item[\textbf{(1)}]\textbf{人体検出による照明領域の特定}
\end{enumerate}

照明特徴量の抽出において最も重要な前処理は、YOLOv8\cite{yolov8} を用いた人物領域の除外である。
人物領域を含んだままRGBやBrightnessを算出すると、肌色や衣服の色、演奏者の動きによる輝度変化が混入し、照明そのものの変化を正確に捉えることができない。

そこで本研究では、人物検出後に膨張処理を施し、演奏者周囲も含めて背景から除外した。
そのうえで、背景画素のうち明度上位10％のみを抽出し、照明器具に最も近い画素群を対象としてRGBおよびBrightnessを算出している。


\begin{enumerate}
	\item[\textbf{(2)}]\textbf{RGB値の算出と各映像のcsvデータ}
\end{enumerate}

映像のフレーム画像を B,G,R チャンネルに分解し、背景マスクかつ上位10\%領域に該当する画素を抽出し、それぞれの平均値を計算した。

これらの値をフレーム単位でバッファに蓄積し、1秒ごとに平均を取ることで、1秒単位のRGB値の照明特徴量としてCSVに書き出した(表2,3,4)。

\begin{enumerate}
	\item[\textbf{(3)}]\textbf{Brightnessの算出と各映像のcsvデータ}
\end{enumerate}

背景照明の明るさは、グレースケール画像（または HSVのV成分）から、同じく上位10\%の明度領域を抽出して平均値を求めた。
また、RGB値と同様に、1秒ごとに平均を取ることで、1秒単位のBrightness値としてCSVに書き出した(表5,6,7)。

\begin{enumerate}
	\item[\textbf{(4)}]\textbf{点滅速度の算出と各映像のcsvデータ}
\end{enumerate}

照明の点滅速度を抽出するために、1秒ごとの平均Brightness値を時系列信号として蓄積し、
前後5秒の局所FFT を行うことで点滅に対応する周波数成分を推定し、csvに書き出した(表8,9,10)。

以上により、各映像から音響特徴量と照明特徴量のペアを1秒ごとに抽出し、学習用CSVデータとして構築した。

\subsection{照明予測モデル(仮)の学習}

4.1節で収集した学習用CSVデータを用いて、照明予測モデルの学習を行った。

\subsubsection{学習データの概要}
照明予測モデル（仮）の学習には、4.1節にて生成した音響特徴量と照明特徴量のCSVのデータを1つのデータセットとして統合し、使用した。

\subsubsection{入力変数（音響特徴量）}
照明予測モデルへの入力として使用した音響特徴量のデータセットは4.1.2節で述べた通りである。

\subsubsection{出力変数（照明特徴量）}
照明予測モデルの出力として使用した照明特徴量のデータセットは4.1.3節で述べた通りである。

\subsubsection{学習データとテストデータの分割}
構築したデータセットは、モデルの汎化性能を評価するため、全体の80\%を学習用データ、20\%をテスト用データに分割した。
分割はランダムに行い、乱数シードを固定することで実験の再現性を確保した。
\begin{enumerate}
	\item[・]学習データ：モデルの学習に使用
\end{enumerate}
\begin{enumerate}
	\item[・]テストデータ：モデルの性能評価に使用
\end{enumerate}

\subsubsection{モデルの構造と学習方法}
照明予測モデル（仮）には、回帰モデルとして Random Forest Regressor\cite{randomforestregressor} を採用した。Random Forest は複数の決定木を組み合わせたアンサンブル学習手法である。
決定木の本数は100とし、学習精度と計算コストのバランスを考慮した。

\subsubsection{学習の実行とモデル保存}
学習用データを用いて Random Forest モデルを学習し、音響特徴量と照明特徴量の対応関係を獲得した。
学習が完了したモデルは、再利用を目的として joblib を用いてファイルとして保存した。

\begin{table}[H]
\centering
\small
\caption{千本桜のRGB値のCSVデータ(一部抜粋)}
\begin{tabular}{c|ccc}
\hline
\hline
Time [s] & R & G & B \\
\hline
1.0 & 90.1 & 45.7 & 62.1 \\
2.0 & 93.9 & 60.2 & 74.1 \\
3.0 & 89.8 & 60.7 & 70.0 \\
4.0 & 84.2 & 42.5 & 53.6 \\
5.0 & 84.8 & 32.2 & 45.8 \\
6.0 & 130.6 & 16.2 & 31.1 \\
7.0 & 125.8 & 25.1 & 33.2 \\
8.0 & 124.7 & 35.9 & 38.0 \\
\hline
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\small
\caption{ソナタ第3番のRGB値のCSVデータ(一部抜粋)}
\begin{tabular}{c|ccc}
\hline
\hline
Time [s] & R & G & B \\
\hline
1.0 & 50.8 & 45.1 & 68.7 \\
2.0 & 52.3 & 45.6 & 69.7 \\
3.0 & 54.5 & 46.9 & 71.6 \\
4.0 & 53.9 & 46.2 & 70.7 \\
5.0 & 65.0 & 58.0 & 81.3 \\
6.0 & 120.8 & 119.2 & 131.2 \\
7.0 & 128.8 & 126.4 & 136.2 \\
8.0 & 139.3 & 137.1 & 145.3 \\
\hline
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\small
\caption{スプラッシュのRGB値のCSVデータ(一部抜粋)}
\begin{tabular}{c|ccc}
\hline
\hline
Time [s] & R & G & B \\
\hline
1.0 & 91.6 & 70.0 & 76.5 \\
2.0 & 54.1 & 36.1 & 50.5 \\
3.0 & 17.8 & 11.4 & 23.9 \\
4.0 & 45.1 & 36.0 & 66.9 \\
5.0 & 239.0 & 238.1 & 242.0 \\
6.0 & 189.4 & 201.3 & 208.0 \\
7.0 & 191.6 & 223.9 & 233.7 \\
8.0 & 200.0 & 203.9 & 199.8 \\  
\hline
\end{tabular}
\end{table}


\begin{table}[H]
\centering
\small
\caption{千本桜のBrightnessのCSVデータ(一部抜粋)}
\begin{tabular}{c|c}
\hline
\hline
Time [s] & Brightness  \\
\hline
1.0 & 59.8 \\
2.0 & 69.3  \\
3.0 & 87.0  \\
4.0 & 53.5  \\
5.0 & 47.7  \\
6.0 & 53.3 \\
7.0 & 56.7 \\
8.0 & 63.3 \\
\hline
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\small
\caption{ソナタ第3番のBrightnessのCSVデータ(一部抜粋)}
\begin{tabular}{c|c}
\hline
\hline
Time [s] & Brightness  \\
\hline
1.0 & 50.2 \\
2.0 & 49.8  \\
3.0 & 52.9  \\
4.0 & 51.1  \\
5.0 & 63.2  \\
6.0 & 124.0 \\
7.0 & 130.9 \\
8.0 & 140.5 \\
\hline
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\small
\caption{スプラッシュのBrightnessのCSVデータ(一部抜粋)}
\begin{tabular}{c|c}
\hline
\hline
Time [s] & Brightness  \\
\hline
1.0 & 77.5 \\
2.0 & 35.3  \\
3.0 & 19.2  \\
4.0 & 43.4  \\
5.0 & 242.3  \\
6.0 & 200.0 \\
7.0 & 215.8 \\
8.0 & 202.8 \\
\hline
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\small
\caption{千本桜の点滅速度のCSVデータ(一部抜粋)}
\begin{tabular}{c|c}
\hline
\hline
Time [s] & Blinking Speed  \\
\hline
1.0 & 0.40 \\
2.0 & 0.33  \\
3.0 & 0.29  \\
4.0 & 0.25  \\
5.0 & 0.44  \\
6.0 & 0.40 \\
7.0 & 0.40 \\
8.0 & 0.20 \\
\hline
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\small
\caption{ソナタ第3番の点滅速度のCSVデータ(一部抜粋)}
\begin{tabular}{c|c}
\hline
\hline
Time [s] & Blinking Speed  \\
\hline
1.0 & 0.40 \\
2.0 & 0.33  \\
3.0 & 0.29  \\
4.0 & 0.25  \\
5.0 & 0.22  \\
6.0 & 0.20 \\
7.0 & 0.20 \\
8.0 & 0.20 \\
\hline
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\small
\caption{スプラッシュの点滅速度のCSVデータ(一部抜粋)}
\begin{tabular}{c|c}
\hline
\hline
Time [s] & Blinking Speed  \\
\hline
1.0 & 0.40 \\
2.0 & 0.33  \\
3.0 & 0.29  \\
4.0 & 0.25  \\
5.0 & 0.22  \\
6.0 & 0.20 \\
7.0 & 0.20 \\
8.0 & 0.20 \\
\hline
\end{tabular}
\end{table}

\subsubsection{照明予測モデル(仮)の位置づけ}
本節で構築した照明予測モデル(仮)は、ライブ映像由来の実測データのみを用いて学習された点に特徴がある。
一方で、演奏種類やデータ量には限界があるため、本研究ではこのモデルを「照明予測モデル（仮）」と位置付けている。

\subsection{MIDIデータの生成と特徴量抽出}
本節では、4.2で構築した照明予測モデル（仮）を用いて、MIDI形式のヴァイオリン楽曲データから音響特徴量および照明特徴量を生成する手法について述べる。

MIDIデータは実演奏の録音と異なり、演奏指示情報のみを保持する形式である。
そのため、本研究ではMIDIデータを音声信号へ変換した上で、ライブ映像データと同一の音響特徴量を抽出し、照明予測モデル（仮）によって照明特徴量を推定するという処理フローを採用した。

\subsubsection{MIDIデータセットの概要}
本研究で使用したMIDIデータは、ヴァイオリン演奏において代表的かつ教育的価値の高い楽曲群から構成されている。
具体的には、「Paganini\cite{paganini}」、「Kayser\cite{kayser}」、「Wohlfahrt\cite{wohlfahrt}」の作品を中心としたヴァイオリンソロのMIDIデータを用いた。

Paganini の作品は、高度な演奏技巧や急激な音高変化、速いパッセージを多く含み、演奏表現の幅が非常に広い。
一方で、Kayser および Wohlfahrt の練習曲は、音階練習や基本的な運弓・音程感覚の習得を目的とした楽曲が多く、比較的安定した音高変化と明瞭なフレーズ構造を持つ。

MIDIデータは、演奏音の音高、音価、発音タイミングなどを厳密に数値情報として保持しているため、音響特徴量抽出において再現性が高く、楽曲間の比較にも適している。
特に、これらの練習曲・技巧曲は、音楽教育の現場でも広く使用されており、ヴァイオリン演奏の典型的な音域・音色変化・フレーズ構造を網羅している点で、本研究の補助的データとして有用である。
\subsubsection{MIDIデータの音声変換}
MIDIデータは直接音響特徴量を算出することができないため、まず音声信号へ変換する必要がある。
本研究では、ソフトウェアシンセサイザー FluidSynth\cite{fluidsynth}を用い、MIDIファイルをWAV形式へ変換した。

MIDIからWAVへの変換は各ファイルごとに自動的に実行される。

\subsubsection{音響特徴量の抽出}
生成されたWAVファイルに対し、Pythonライブラリ librosa を用いて音響特徴量を抽出した。
抽出する特徴量の種類および構成は、5.3節で構築した照明予測モデル（仮）の学習時に用いたデータセットと完全に一致させている。

\subsubsection{1秒単位への集約処理}
抽出された音響特徴量はフレーム単位のデータであるため、そのままでは照明制御モデルとの時間スケールが一致しない。
そこで、本研究では 1秒単位で特徴量を平均化する処理を行った。

具体的には、サンプリングレートおよびhop lengthから1秒あたりのフレーム数を算出し、該当フレーム範囲の特徴量を平均することで、1秒ごとの特徴ベクトルを生成した。

この処理により、ライブ映像データと同様に「1秒 = 1照明フレーム」という時間対応が可能となる。

\subsubsection{照明予測モデル（仮）による照明特徴量推定}
1秒単位に集約された音響特徴量は、4.2節で構築した 照明予測モデル（仮） に入力される。
本モデルは、ライブ映像データから学習した「音響特徴量と照明特徴量の対応関係」を保持しており、MIDI由来の音響特徴量に対しても照明特徴量を推定することが可能である。
推定される照明特徴量はRGB値、Brightness、点滅速度の3種類である。

\subsubsection{CSV形式での出力}
各MIDIファイルについて、以下の情報を1つのCSVデータとして出力した。
\begin{enumerate}
	\item[・]時間(秒)
\end{enumerate}
\begin{enumerate}
	\item[・]音響特徴量
\end{enumerate}
\begin{enumerate}
	\item[・]照明予測モデル(仮)による推定照明特徴量
\end{enumerate}

これにより、MIDIデータから得られた音響特徴量と対応する照明特徴量を一元的に管理できるようになった。

\subsection{MIDIデータを含めた照明予測モデルの学習}
4.3節で生成したMIDIデータ由来の特徴量データを加え、照明予測モデルを再学習する手法について述べる。
本節で構築されるモデルは、ライブ映像データに加えてMIDIデータも含めた拡張データセットを用いて学習されたモデルであり、以降「照明予測モデル(本)」と呼ぶ。

4.2節では、ライブ映像データのみを用いた学習によって、音響特徴量と実際の照明演出との対応関係を獲得できることを示した。
しかし、ライブ映像データは演奏数や演奏スタイルに限りがあり、機械学習モデルとしての汎化性能には一定の制約が存在する。
そこで本研究では、MIDIデータを用いて演奏パターンの多様性を拡張し、より汎用的な照明予測モデル(本)の構築を試みた。

\subsubsection{MIDIデータ導入の目的}
MIDIデータを学習データに加える目的は、以下の点にある。

第一に、演奏音のバリエーションを大幅に増加させることである。
MIDIデータには、音高変化、フレーズ構造、音価の違いなど、実演奏に近い音楽的情報が含まれており、ライブ映像データだけでは不足しがちな演奏パターンを補完できる。

第二に、モデルが特定の演奏映像や照明環境に過度に依存することを防ぎ、未知の演奏音に対しても安定した照明予測を行えるようにする点である。

\subsubsection{学習データセットの構成}
本研究では、4.2節で生成したMIDIデータ由来の特徴量データを、4.1節で構築したライブ映像データ由来の特徴量データに追加し、拡張された学習データセットを構成した。

MIDIデータについては、実際の照明映像を伴わないため、4.2節で学習した照明予測モデル（仮）を用いて照明パラメータを推定し、擬似的な教師データとして扱っている。

\subsubsection{特徴量構成と前処理}
入力特徴量および出力変数の構成は、4.2節で使用したものと同一であるため、本節では詳細な説明を省略する。
\begin{enumerate}
	\item[・]入力：音響特徴量
\end{enumerate}
\begin{enumerate}
	\item[・]出力：照明特徴量
\end{enumerate}

\subsubsection{学習手法}
学習手法としては、4.2節と同様に Random Forest Regressor を採用した。
モデル構造や主要なハイパーパラメータも、同一に設定している。

\subsubsection{学習データとテストデータの分割}
統合されたデータセットは、4.2節と同様に学習用データとテスト用データに分割した。
分割比率および分割方法は同一であり、実験条件の公平性を保っている。

\subsubsection{学習の実行とモデル保存}
拡張された学習データセットを用いて Random Forest モデルの学習を実行し、音響特徴量と照明特徴量の対応関係を再度獲得した。
学習が完了したモデルは、 joblib を用いてファイルとして保存し、リアルタイム照明制御システムに直接組み込んだ。

\subsubsection{照明予測モデル(本)の位置づけ}
本節で学習された照明予測モデル(本)は、ライブ映像由来の実測データと、MIDIデータによって補完された多様な演奏パターンの両方を内包している。

このモデルを、本研究における最終的な照明予測モデルと位置付け、システムに組み込む。

\subsection{照明予測モデルの評価}
本節では、4.4節で構築した MIDIデータを含めた照明予測モデル(本)の性能評価について述べる。
評価には、機械学習における回帰問題で一般的に用いられる決定係数（R²スコア）を採用した。

\subsubsection{評価指標：決定係数 (R²スコア)}
本研究では、照明予測モデル(本)の性能を定量的に評価するため、決定係数 R²（Coefficient of Determination） (図16)を用いた。
R²スコアは、モデルの予測値が実測値の分散をどの程度説明できているかを示す指標であり、以下の式で定義される。

\begin{figure}[H]
	\centering
	\includegraphics[width=\hsize]{r2-formula.jpg}
	\caption{決定係数 (R²) の定義式}
	\label{fig:r2-formula}
\end{figure}

R²スコアは 1 に近いほど予測精度が高く、0 は平均値予測と同等、負の値は平均予測より劣ることを示す。

\subsubsection{評価方法}
評価には、学習時に使用していないテストデータを用いた。
テストデータには、ライブ映像由来データを中心に含めることで、実際の照明演出との整合性を重視した評価を行った。

評価は、RGB値、Brightness、点滅速度といった複数の照明特徴量を同時に含む多出力回帰問題として実施し、全出力を総合したR²スコアを算出した。

\subsubsection{評価結果}
最終照明予測モデル(本)に対する評価の結果、R²スコアは 0.85 を示した(図17)。

\begin{figure}[H]
	\centering
	\includegraphics[width=\hsize]{r2.png}
	\caption{決定係数 (R²) の結果}
	\label{fig:r2}
\end{figure}


この結果は、音響特徴量から照明特徴量への変換において、モデルが実測照明データの分散の約85\%を説明できていることを意味する。
すなわち、照明の色、明るさ、点滅といった複合的な要素を、演奏音のみから高精度に推定できていることが確認された。

\subsection{音響特徴量と照明特徴量の対応関係}
本節では、最終的に構築した照明予測モデル(本)が、音響特徴量と照明特徴量の間にどのような対応関係を学習しているかを分析する。
前節まででは、R²スコアを用いた定量的評価によりモデル全体の性能を示したが、本節ではさらに踏み込み、「どの音響的要素が、どの照明要素に強く影響しているか」を明らかにすることを目的とする。

音と照明の関係は本質的に主観的であり、一意に定義されるものではない。
しかし、機械学習モデルが内部的に利用している特徴量の重要度を解析することで、モデルがどのような音響的判断基準に基づいて照明を制御しているかを客観的に考察することが可能となる。

\subsubsection{解析手法の概要}
音響特徴量と照明特徴量の対応関係を解析するため、本研究では Random Forest における特徴量重要度（feature importance） を用いた。

Random Forest では、各決定木において分岐に使用された特徴量がどれだけ予測誤差の低減に寄与したかを基に、特徴量重要度が算出される。
この値は、モデルが予測を行う際にどの特徴量を重視しているかを示す指標である。

本研究では、照明特徴量ごとに個別の Random Forest 回帰モデルを学習し、それぞれの出力に対して音響特徴量の重要度を算出した。
これにより、照明の色・明るさ・点滅といった各要素が、どの音響的特徴と強く結びついているかを独立に分析できる。

\subsubsection{使用データ}
解析には、4.1節および4.3節で生成されたCSVデータを使用した。
これらのCSVファイルには、1秒単位で集約された音響特徴量と対応する照明特徴量が含まれている。

\subsubsection{照明特徴量別の解析方法}
「RGB値」「Brightness」「点滅速度」の各照明特徴量に対して、それぞれ独立に解析を行った。

各照明特徴量を目的変数とし、音響特徴量を説明変数として Random Forest 回帰モデルを学習した。
学習後、特徴量重要度を算出し、寄与度の高い上位5特徴量を抽出した。

結果の可視化には円グラフを用い、各特徴量が予測にどの程度寄与しているかを直感的に把握できるようにした。

\subsubsection{音響特徴量と照明色（R成分）の対応関係}
図18は、照明色の R 成分（Average R）を目的変数として学習した Random Forest 回帰モデルにおける、音響特徴量の重要度上位5項目を円グラフで示したものである。
本図は、モデルが赤色成分の強度を予測する際に、どの音響特徴量をどの程度重視しているかを可視化した結果である。

本解析の結果、$\mathrm{RMS}$ が最も高い重要度を示し、次いで $\mathrm{Tonnetz\_6}$、$\mathrm{MFCC\_5}$、$\mathrm{MFCC\_19}$、$\mathrm{Spectral Contrast}$ の順となった。

RMS（Root Mean Square）は音響信号のエネルギー量、すなわち音量を表す特徴量である。
本結果において RMS が最も高い寄与を示したことは、音量の大きさが照明の赤成分の強度に強く反映されていることを示している。

2番目に高い重要度を示した $\mathrm{Tonnetz\_6}$ は、音楽の和声構造や調性感を表す特徴量である。
これが R 成分の予測に大きく寄与していることから、単なる音量だけでなく、和声的・音楽的な性質が照明色に反映されていることが示唆される。

また、$\mathrm{MFCC\_5}$ および $\mathrm{MFCC\_19}$ といった MFCC 系特徴量も比較的高い重要度を示している。
MFCCは音色の特徴を表す指標であり、これらの結果は、音色の違いが照明色の変化に影響していることを意味している。

特に中低次および高次の MFCC 成分が同時に重要となっている点から、モデルは音の明るさだけでなく、倍音構造の微妙な違いも考慮して照明色を決定していると考えられる。

Spectral Contrast は、周波数帯域ごとのピークと谷の差を表す特徴量であり、音の鋭さや迫力を反映する指標である。
本結果では重要度は比較的低いものの、上位5特徴量に含まれている。

これは、音の輪郭が明瞭であるかどうかが、赤色成分の強調に一定の影響を与えていることを示している。

\begin{figure}[H]
	\centering
	\includegraphics[width=\hsize]{rlightstyle.png}
	\caption{照明色 (R成分) に対する音響特徴量}
	\label{fig:rlightstyle}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\hsize]{glightstyle.png}
	\caption{照明色 (G成分)に対する音響特徴量}
	\label{fig:glightstyle}
\end{figure}

\subsubsection{音響特徴量と照明色（G成分）の対応関係}
図19は、照明色の G 成分（Average G）を目的変数として学習した Random Forest 回帰モデルにおける、音響特徴量の重要度上位5項目を示した結果である。
本図は、音響信号のどの要素が緑色成分の強度に影響を与えているかを可視化したものである。

解析の結果、RMS が全体の約86.8\% と極めて高い重要度を示し、他の特徴量（$\mathrm{Tonnetz\_6}$、$\mathrm{Chroma\_7}$、$\mathrm{MFCC\_5}$、$\mathrm{Spectral Contrast}$）はいずれも1〜7\%程度に留まった。

G成分において RMS が圧倒的な重要度を示したことは、音量が緑色照明の強度をほぼ支配的に決定していることを意味している。

この結果は、G成分が他の色成分に比べて、音楽的な細かなニュアンスよりも音圧の変化を直接反映する成分として扱われていることを示している。

RMS に次いで $\mathrm{Tonnetz\_6}$（約6.5\%）が高かった。
この結果は、緑色成分においても、音量以外に和声的要素が一定の影響を及ぼしていることを示唆している。

また、$\mathrm{Chroma\_7}$ や $\mathrm{MFCC\_5}$ はそれぞれ音高クラス分布および音色に関連する特徴量であり、これらが上位に含まれている点は、緑成分が完全に単純な音量依存ではないことを示している。
ただし、これらの寄与率は小さく、あくまで補助的な要素であると解釈できる。

R成分の解析結果では、RMS に加えて Tonnetz や MFCC が比較的高い割合を占めていたのに対し、G成分では RMS の寄与が著しく大きくなっている。
この違いは、照明色ごとに音響特徴量との対応関係が異なることを示している。

\subsubsection{音響特徴量と照明色（B成分）の対応関係}
図20は、照明色の B 成分（Average B）を目的変数として学習した Random Forest 回帰モデルにおける、音響特徴量の重要度上位5項目を示した結果である。
本図は、音響信号のどの要素が青色照明の強度に影響を与えているかを示している。

解析の結果、RMS が約70.2\% と最も高い重要度を示した。
次いで  $\mathrm{MFCC\_19}$（約14.9\%）が比較的高い寄与を示し、 $\mathrm{Chroma\_7}$、$\mathrm{Tonnetz\_6}$、$\mathrm{MFCC\_5}$ はそれぞれ約5\%前後であった。
\begin{figure}[H]
	\centering
	\includegraphics[width=\hsize]{blightstyle.png}
	\caption{照明色 (B成分)に対する音響特徴量}
	\label{fig:blightstyle}
\end{figure}
B成分においても RMS が最も支配的な特徴量であることから、青色照明も音量変化に大きく依存していることが分かる。
音量が大きい場面では青色成分が強調され、音量が小さい場面では抑制される傾向をモデルが学習していると考えられる。

ただし、G成分と比較すると RMS の割合はやや低下しており、B成分では音量以外の要素も一定程度考慮されていることが示唆される。

$\mathrm{MFCC\_19}$ が約15\%と比較的高い重要度を示した点は、B成分が音色や高次スペクトル成分に影響されやすいことを示している。

$\mathrm{Chroma\_7}$ および $\mathrm{Tonnetz\_6}$ が上位に含まれていることから、B成分は音量や音色だけでなく、音楽的構造（音高分布や和声的関係）とも関連していることが分かる。

特に Tonnetz が青色成分に影響している点は、静的・緊張感のある和声や、落ち着いた調性が青系照明として表現されやすい可能性を示唆している。

これらの結果から、RGB 各成分は一様に音量に反応するのではなく、それぞれ異なる音響的役割を担っていることが明らかとなった。

\subsubsection{音響特徴量と照明の明るさの対応関係}
図21は、照明の明るさ（Background Brightness）を目的変数として学習した Random Forest 回帰モデルにおける、音響特徴量の重要度上位5項目を示した結果である。
本図は、音響信号のどの要素が照明全体の明るさに影響を与えているかを明らかにするものである。
解析の結果、RMS が約77.8\% と最も高い重要度を示した。次いで $\mathrm{Tonnetz\_6}$（約10.4\%）、$\mathrm{MFCC\_19}$（約5.3\%）、$\mathrm{MFCC\_5}$（約3.7\%）、$\mathrm{Spectral Contrast}$（約2.8\%）が続いた。

\begin{figure}[H]
	\centering
	\includegraphics[width=\hsize]{brilightstyle.png}
	\caption{照明の明るさに対する音響特徴量}
	\label{fig:brightnessstyle}
\end{figure}

RMS が約8割近い寄与率を示したことから、照明の明るさは主に音量に基づいて決定されていることが明らかとなった。

本モデルでは、演奏音が強くなるにつれて照明が明るくなり、弱くなるにつれて暗くなるという対応関係が学習されている。

RMS に次いで $\mathrm{Tonnetz\_6}$（約10.4\%）の重要度が高かった。
この結果は、照明の明るさが単純な音量変化だけでなく、音楽的な構造要素にも影響を受けていることを示している。

また、$\mathrm{MFCC\_19}$ および $\mathrm{MFCC\_5}$ が上位に含まれていることから、照明の明るさは音量だけでなく、音の質感や倍音構成にも影響されていることが分かる。

RGB 各成分と照明の明るさの関係を比較すると、明るさは RGB 成分以上に RMS への依存度が高いことが分かる。
これは、明るさが照明全体の基盤的なパラメータであり、色成分よりも直接的に音量変化を反映する役割を担っているためである。

一方で、Tonnetz や MFCC が一定の寄与を示していることから、明るさも完全に単純な制御ではなく、音楽的文脈を考慮した調整が行われていると解釈できる。

\subsubsection{音響特徴量と照明の点滅速度の対応関係}
図22は、照明の点滅速度（Blinking Speed）を目的変数として学習した Random Forest 回帰モデルにおける、音響特徴量の重要度上位5項目を示した結果である。
本図は、音響信号のどの要素が照明の時間的変化、すなわち点滅の速さに影響を与えているかを可視化したものである。
\begin{figure}[H]
	\centering
	\includegraphics[width=\hsize]{blinklightstyle.png}
	\caption{照明の点滅速度に対する音響特徴量}
	\label{fig:blinkstyle}
\end{figure}

解析の結果、$\mathrm{Chroma\_7}$ が約30.6\% と最も高い重要度を示し、次いで $\mathrm{MFCC\_5}$（約21.9\%）、$\mathrm{Chroma\_1}$（約19.7\%）、$\mathrm{MFCC\_1}$（約15.6\%）、RMS（約12.\%）という順となった。

点滅速度において最も大きな影響を与えているのは Chroma 系特徴量であり、特に $\mathrm{Chroma}_{7}$ および $\mathrm{Chroma}_{1}$ が全体の約50\%を占めている。
この結果は、照明の点滅が音量の大小よりも、音楽の和声的・音高的構造に強く依存していることを示している。

Chroma の変化が大きい場面では、照明の点滅が速くなり、音楽的な動きや緊張感を視覚的に強調する演出が行われていると考えられる。

$\mathrm{MFCC\_5}$ および $\mathrm{MFCC\_1}$ も比較的高い重要度を示しており、点滅速度が音色の変化にも影響を受けていることが分かる。

RMS は点滅速度においても一定の影響を持つものの、その重要度は約12.1\%に留まっている。
これは、照明の明るさとは異なり、点滅という時間的変化の制御においては、音量そのものよりも音楽的構造や音高変化が重視されていることを示唆している。

RGB 各成分および照明の明るさでは RMS の重要度が支配的であったのに対し、点滅速度では Chroma や MFCC といった音楽的特徴量が主導的な役割を果たしている。
この違いは、照明パラメータごとに参照される音響情報の種類が異なることを明確に示している。


\section{実演}
本章では、本研究で構築したリアルタイム照明制御システムを用いて実施した実演について述べる。
本研究では、システムの動作確認および表現の検証を目的として、異なる楽曲および異なる場所において複数回の実演を行った。
実演は、「大学教室における検証的実演」、「新町商店街における公開実演」の順で実施している。

\subsection{実演の全体構成と目的}
本研究における実演は、以下の二つの目的を持って段階的に実施した。
\begin{enumerate}
	\item[(1)]システムが安定して動作するかを確認する検証的実演
\end{enumerate}

\begin{enumerate}
	\item[(2)]実際の観客を想定した環境における表現力の確認
\end{enumerate}

\subsection{第1回実演：福知山公立大学(検証的実演)}

\subsubsection{実演環境}
最初の実演は、福知山公立大学の3201教室において実施した

教室内は外光や環境音の影響が比較的少なく、安定した条件で実験を行うことが可能である。
そのため、音響入力、運弓動作取得、照明制御が正しく連動しているかを確認する環境として適している。

室内には演奏者を中央に配置し、その周囲四方向にDMX対応照明機器を設置した。
照明は床方向から天井に向けて照射される構成とし、四方から演奏者を照らすような光環境を作り出した。
システムは以下の機器構成で運用した。
また、各機材の配置図(上面図)は図23に示す。

図23におけるそれぞれの配線はケーブルを用いたものを実線で示し、無線通信は点線で示している。

\begin{enumerate}
	\item[・]ヴァイオリン(演奏者：筆者)
	\item[・]コンデンサマイク(音声入力)
	\item[・]Windows 11 PC
	\item[・]スマートフォン(ZIG-SIM用)
	\item[・]USB-DMXインタフェース
	\item[・]DMX対応照明機器(4台)
\end{enumerate}

\begin{figure}[H]
	\centering
	\includegraphics[width=\hsize]{setting.png}
	\caption{実演機材配置図(上面図)}
	\label{fig:haichi}
\end{figure}

\subsubsection{実演曲目}

実演に使用した楽曲は、『君に乗せて\cite{kimi}』(作曲：久石譲)である。
本曲はアニメ『天空の城ラピュタ』の主題歌として広く知られており、旋律が明瞭でテンポ変化が比較的少ない楽曲である。
ヴァイオリンソロアレンジ版を用い、演奏時間は約2分30秒である。
演奏はAメロからサビまでをカバーしている。
図24に実演で使用した楽譜を示す。

\begin{figure}[H]
	\centering
	\includegraphics[width=\hsize]{kimiscore.png}
	\caption{実演曲目『君に乗せて』楽譜}
	\label{fig:kimi}
\end{figure}

\subsubsection{実演内容と観察結果}
この実演では、来場者を想定せず、研究者自身による検証を目的として演奏を行った(図25,26)。
図の通り、演奏音の入力に応じて照明の色および明るさが変化すること、運弓方向に応じて照明の上下動作が発生することを確認した。

\begin{figure}[H]
	\centering
	\includegraphics[width=\hsize]{kimi2.png}
	\caption{『君に乗せて』実演中の照明変化の様子(Aメロ～Bメロ)}
	\label{fig:kimi2}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\hsize]{kimi3.png}
	\caption{『君に乗せて』実演中の照明変化の様子(サビ)}
	\label{fig:kimi3}
\end{figure}

\subsection{第2回実演：新町商店街（公開実演）}

\subsubsection{実演環境}
次の実演は、京都府福知山市の新町商店街にある「Tsunaga Room\cite{tsunagaroom}」にて実施した。
この実演は、一般来場者を含む環境で行われた公開実演である。
また、使用機材の配置や照射方法は5.2.2節と同様である。

\subsubsection{実演曲目}

実演に使用した楽曲は、『紅蓮華\cite{utasonglisa}』（作曲：草野華余子）である。
本曲はアニメ『鬼滅の刃』のオープニングテーマとして広く知られており、力強いリズムと感情豊かなメロディが特徴である。
ヴァイオリンソロアレンジ版を用い、演奏時間は約1分30秒である。
演奏はイントロから1番のサビまでをカバーしている。
図27に実演で使用した楽譜を示す。

\subsubsection{実演の進行方法}
第2回実演では、来場者に対して本研究の趣旨および実演内容について簡単な説明を行った上で、ヴァイオリン演奏と照明演出の実演を実施した。

実演開始前に、演奏者（筆者）から来場者に対して、以下の内容を口頭で説明した。

まず、本実演ではヴァイオリン演奏の音そのものが照明を制御していることを説明した。
具体的には、演奏音の音量や音色、響き方といった要素がリアルタイムに解析され、それに基づいて照明の色や明るさ、点滅が自動的に変化する仕組みであることを伝えた。
また、照明は事前に演出をプログラムしているものではなく、演奏内容に応じて毎回異なる変化が生じることを説明した。

次に、音響情報に加えて、演奏者の運弓動作が照明の動きに反映されていることを説明した。演奏者の腕の動きをスマートフォンで取得し、弓を上げる動作と下げる動作に応じて照明が上下に動く仕組みであることを示した。この説明により、来場者が演奏中の身体動作と照明の動きを対応づけて観察できるよう配慮した。

説明後、楽曲『紅蓮華』の実演を開始した。

\begin{figure}[H]
	\centering
	\includegraphics[width=\hsize]{score.png}
	\caption{実演曲目『紅蓮華』楽譜}
	\label{fig:gurennge1}
\end{figure}


\subsubsection{実演中の照明変化の様子}
図28～図31に、実演中の照明変化の様子を示す。これらの画像は、前述した実演環境にて、ヴァイオリン演奏曲『紅蓮華』を演奏した際の照明挙動を記録したものである。

演奏開始直後のイントロ部分（図28）では、照明は主に青色系の低輝度な光で構成されている。
これは、音響特徴量のうちRMS値が低く、音量が抑えられている区間であるためであり、照明のBrightnessが低く制御されていることを示している。また、色成分についても急激な変化は見られず、楽曲導入部として静的かつ落ち着いた視覚表現が生成されている。

演奏が進行していくにつれて（図29）、照明の色調は赤色を含む暖色系へと変化し、同時に明るさが増加している様子が確認できる。
この変化は、音量の増加および音色の変化に伴い、機械学習モデルがRGB値およびBrightnessを動的に更新している結果である。
特に、赤系の照明が演奏者の身体およびヴァイオリンに強調的に照射されており、楽曲の緊張感やエネルギーを視覚的に表現している。
さらに、サビ部分に近づくにつれて（図 30,31）、照明は高輝度状態となり、色成分も紫、黄色といった複数の色が時間的に変化している。

\begin{figure}[H]
	\centering
	\includegraphics[width=0.3\textwidth]{intro.png}
	\caption{『紅蓮華』実演中の照明変化の様子（イントロ部分）}
	\label{fig:intro}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.3\textwidth]{abc.png}
	\caption{『紅蓮華』実演中の照明変化の様子（Aメロ～Cメロ）}
	\label{fig:chorus}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.3\textwidth]{savi.png}
	\caption{『紅蓮華』実演中の照明変化の様子（サビ部分）}
	\label{fig:savi}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.3\textwidth]{savi2.png}
	\caption{『紅蓮華』実演中の照明変化の様子（サビ部分）}
	\label{fig:savi2}
\end{figure}

\section{考察}
本章では、実演結果を踏まえ、本研究の意義および課題について考察する。特に、本研究が音楽演奏と照明演出の関係にどのような新しい可能性を示したのか、また実演を通して明らかになった技術的・表現的な課題について整理する。

\subsection{本研究および実演の意義}
\subsubsection{演奏者主体の照明制御という意義}
本研究の最も大きな意義は、演奏者自身の演奏行為によって照明が自律的に制御されるシステムを構築した点にある。

本研究では、音響特徴量および演奏動作という二種類の情報を用いることで、演奏者の表現そのものが照明演出へと変換される構造を実現した。
特に、筆者自身がヴァイオリン演奏を行いながら実演を行ったことにより、「演奏者が照明を操作している」という感覚を伴った演出が可能となった点は、本研究の重要な成果である。

\subsubsection{音楽表現の視覚的拡張としての意義}
実演を通して、照明の色・明るさ・動きが演奏内容に応じて変化することで、音楽表現が視覚的に拡張されることが確認された。

特に実演では、楽曲『紅蓮華』の盛り上がりに応じて照明が変化し、音楽のエネルギーが視覚的にも強調される効果が見られた。

\subsection{実演を通して明らかになった課題}

\subsubsection{照明表現の一貫性と解釈の問題}
照明の色や動きは機械学習モデルによって生成されるため、演奏内容が似ていても、照明表現が必ずしも一貫した意味を持つとは限らない。例えば、同じフレーズであっても、微細な音響特徴の違いにより照明の色が変化する場合があった。

これは、照明表現が「意図された意味」を必ずしも持たず、観客や演奏者の解釈に委ねられる部分が大きいことを示している。

\subsubsection{演奏動作取得の精度と安定性}
ZIG-SIMを用いた運弓方向推定は概ね有効であったが、演奏中の細かな動作や急激な動きに対して、誤判定が生じる可能性も確認された。特に、弓の動きが小さい場合や、角速度が閾値付近にある場合には、上下動作が不安定になることがあった。

この課題に対しては、センサ情報の平滑化や、複数軸情報を用いた判定など、より安定した動作推定手法の導入が必要である。

\subsection{今後への示唆}
以上の考察から、本研究は演奏者主体の照明制御という新しい表現手法の可能性を示す一方で、照明表現の意味付け、動作推定精度などといった点に改善の余地があることが明らかとなった。


\section{結び}
本研究では、ヴァイオリン演奏に基づいて照明をリアルタイムに制御するインタラクティブ照明演出システムの構築を目的とし、音響特徴量および演奏動作情報を用いた照明制御手法を提案・実装・実演を通して検証した。

まず研究背景として、従来の舞台照明が事前に設計された演出や、音量に単純に反応する仕組みに依存している点に着目し、演奏者の表現そのものが直接照明に反映される仕組みの必要性を示した。
特に本研究では、筆者自身がヴァイオリン演奏を行うことができるという特性を活かし、「演奏しながら照明を動かす」という発想を出発点として研究を進めた。

提案手法では、ヴァイオリン演奏音から抽出した多次元の音響特徴量を入力として、照明の色（RGB）、明るさ、点滅といった複数の照明パラメータを同時に予測する機械学習モデルを構築した。
また、ZIG-SIMを用いて演奏者の運弓動作を取得し、照明の物理的な上下動作に反映させることで、音響情報だけでは表現しきれない身体的表現を照明演出に取り込んだ。

照明予測モデルの構築においては、ライブ映像およびMIDI音源を用いたデータ生成手法を採用し、Random Forest回帰モデルによって音響特徴量と照明特徴量の関係を学習した。
これにより、実演においても安定して照明パラメータを生成できるモデルを構築することができた。

実演では、新町商店街（福知山市）の屋内会場において楽曲『紅蓮華』を演奏し、より実環境に近い条件下で照明演出の表現力およびリアルタイム性を検証した。
これらの実演を通して、演奏音および演奏動作に応じて照明が自律的に変化し、演奏と照明が同期する演出が実現できることを確認した。

以上より、本研究は、音楽演奏と照明演出をリアルタイムに結びつける一つの実践的手法を提示し、演奏者の表現を視覚的に拡張する可能性を示した点に意義があると結論づけられる。
\section*{謝辞}
本論文を執筆するにあたり、多くの方々からご指導・ご支援を賜りました。
ここに深く感謝の意を表します。
まず、本研究を進めるにあたり、研究の方向性から終始丁寧なご指導と貴重なご助言をくださった橋田光代准教授に心より御礼申し上げます。
また、研究室の4年生の皆様には、日頃より温かく接していただき、研究の相談にも快く応じていただきました。
皆様との和やかな会話は、研究生活の大きな支えとなりました。
心より感謝いたします。
最後に、本研究に関わってくださったすべての方々に、厚く御礼申し上げます。

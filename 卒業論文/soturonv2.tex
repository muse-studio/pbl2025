% !TEX root = _main.tex
% ========================================
% 卒業論文　本文
% ========================================

\section{はじめに}
舞台演出やライブパフォーマンスにおいて、照明は音楽表現を視覚的に拡張し、観客の没入感を高める重要な要素である。照明の色や明暗、動きは演奏の意図や感情を強調し、音響のみでは伝わりにくい表現を補完する役割を果たす。しかし、従来の舞台照明は照明オペレーターによる手動操作に大きく依存しており、演奏内容に即応した柔軟な制御が難しいという課題がある。特に、小規模な演奏環境や個人演奏においては、専任の照明オペレーターを配置することが困難であり、演奏者自身の表現意図を照明に反映させる手段が限られている。

このような背景から、演奏そのものを入力として照明を自動制御する仕組みが求められている。音楽演奏には音響的特徴だけでなく、身体動作に基づく表現も含まれており、これらを照明制御に統合することで、より一体感のある演出が実現できると考えられる。

そこで本研究では、ヴァイオリン演奏に着目し、音響信号と身体動作を組み合わせたリアルタイム照明制御システムの構築を目的とする。具体的には、マイクから入力されたヴァイオリン音響信号から、スペクトル重心、スペクトル帯域幅、MFCC、ZCR などの音響特徴量を抽出し、機械学習モデルを用いて照明の RGB 値、明るさ、点滅速度といった照明特徴量を予測する。また、スマートフォンのモーションセンシングアプリ ZIG-SIM\cite{zigsim} を用いて、演奏者のヴァイオリン演奏における弓の上下動作を角速度として取得し、照明の上下動作に直接反映させる。

本研究の着想は、筆者自身の「自分のヴァイオリン演奏だけで照明を直接動かしてみたい」という個人的関心に基づいている。ヴァイオリンを対象とした理由として、筆者自身が演奏経験を有しており、演奏者の立場からシステムの反応性や表現性を評価できる点が挙げられる。

以上より本研究では、ヴァイオリン演奏音から抽出した音響特徴量に基づいて照明の色、明るさ、点滅速度を制御すると同時に、演奏者の運弓動作を照明の上下動作として直接反映することで、音楽表現と身体表現が一体となった照明演出の実現を目指す。

\section{関連研究}
音楽ライブやコンサートにおける照明演出は、観客の没入感や一体感を高める重要な要素であり、従来は照明デザイナーやオペレータの経験や感性に基づいて設計されてきた。しかし、小規模公演や即興性の高い演奏では十分な準備時間や人員を確保することが難しく、音楽情報処理技術を用いた照明演出の自動化が有効な手段として注目されている。
\subsection{音響信号に基づく照明演出の自動化}

音響信号に基づく照明制御に関する研究として、月東らは楽曲音源からリズムの強調箇所を検出し、それを照明演出へ反映する手法を提案している\cite{1}。
この研究では、楽曲をドラム、ベース、ギター、ボーカルの各パートに音源分離し、それぞれのオンセットエンベロープを算出した上で、複数パートで同時に強いオンセットが観測されるタイミングを「キメ」と定義している。検出されたキメを基点として、照明のフラッシュや明滅といった瞬間的な演出を付与し、主観評価実験によりリズムの一体感や迫力の向上を確認している。

この手法は高い時間解像度でリズム的アクセントを捉えられる点に特徴がある一方で、照明制御はキメの有無に依存した離散的なものに留まっており、演奏中の表現変化を連続的に反映することは難しい。また、演奏者の身体的動作は入力として扱われていない。

\subsection{楽曲の印象・意味内容に基づく照明生成}
楽曲全体の印象や意味内容を照明演出に反映する研究も多く報告されている。神野らは、歌詞および曲調の印象に基づいて照明演出を自動生成するシステムを提案している\cite{2}。

この研究では、歌詞を形態素解析した上で分散表現ベクトルに変換し、言語イメージスケールを用いて照明色候補を選択する。さらに、楽曲の長短調やテンポに基づいて色の優先度や配置を決定し、BPMや歌詞のネガティブ・ポジティブ度を用いて明度を調整することで、照明の色、配置、明るさを総合的に決定している。この手法により、照明デザイナーの感性に依存しない一貫性のある演出が実現されている。

一方で、照明変化は小節やフレーズ単位といった比較的長い時間スケールで行われるため、演奏中の瞬間的な表現変化や演奏者の身体動作を直接反映することは想定されていない。

\subsection{深層学習を用いた照明演出生成}
近年では、照明演出を生成タスクとして捉え、深層学習モデルを用いて照明パラメータを自動生成する研究も行われている。月東らの研究内でも言及されている Zhao らの手法では、楽曲特徴量系列を入力として照明の色や明るさの時系列データを生成する生成モデルが提案されている\cite{5}。

このようなアプローチは、表現力や多様性の高い照明演出を実現できる可能性を示しているが、学習データへの依存度が高く、生成過程の解釈が困難であるという課題がある。また、演奏者の身体動作は入力として扱われていない。

\subsection{演奏者操作によるリアルタイム照明制御}
演奏者自身が照明演出に直接関与する研究として、瀧口らはドラム音やMIDIフットコントローラーを用いて演奏中に照明を操作可能なシステムを提案している\cite{4}。
この研究では、バスドラム音の音量が閾値を超えた際に照明を点灯させる手法や、足元のコントローラー操作によって照明の色や輝度、ストロボ効果を変更する仕組みが実装されている。

この手法により、即興演奏に柔軟に対応できる照明演出が実現されている一方で、演奏者が照明操作を意識する必要があり、演奏負荷が増加する可能性がある。また、演奏動作そのものを自動的に取得・利用する仕組みは含まれていない。

\subsection{身体動作と照明演出の連動}
演奏者の身体動作と照明演出を連動させる研究として、浅田らはオンラインライブ空間において、演奏者の身体情報を用いて照明や振動装置を制御するシステムを提案している\cite{5}。
この研究では、ギター演奏者の腕の角度に応じてムービングライトの向きを変化させるなど、身体動作を照明の物理的な動きに対応付けることで、演奏者と観客の一体感を高めている。

しかし、音響特徴量と身体動作を統合的に扱う設計にはなっておらず、音楽表現と照明表現の対応関係は限定的である。

\subsection{本研究の位置づけ}
以上の関連研究から、音響特徴量に基づく照明制御、楽曲印象に基づく照明生成、演奏者操作型および身体動作連動型の照明演出はそれぞれ発展してきたものの、音響情報と演奏者の身体動作を同時に統合し、連続的かつリアルタイムに照明を制御する研究は十分に行われていないことが分かる。

そこで本研究では、ヴァイオリン演奏音の特徴量と演奏者の運弓動作に応じてステージ上の照明がリアルタイムに変化する照明制御システムを提案する。

\section{リアルタイム照明制御システムの実装}
本章では、本研究で提案したヴァイオリン演奏に基づくリアルタイム照明制御手法を実現するために構築したシステム構成(図1)について詳述する。
本システムは、ヴァイオリン演奏に伴う音響特徴量および演奏動作情報を入力とし、機械学習モデルによって推定された照明特徴量をDMX信号として出力することで、演奏に同期した照明演出を自動生成するものである。

\subsection{システム設計の基本方針}
本システムの設計にあたり、以下の方針を重視した。
\begin{enumerate}
	\item[\textbf{(1)}]\textbf{演奏中に人手操作を必要としないこと}
\end{enumerate}

本システムでは、演奏者が演奏中に照明操作を意識したり、追加の入力デバイスを操作したりする必要がない自律的な構成を目指した。これにより、演奏者は音楽表現そのものに集中でき、照明演出が演奏の妨げとなることを防いでいる。
\begin{enumerate}
	\item[\textbf{(2)}]\textbf{遅延を極力抑え、リアルタイム性を確保すること}
\end{enumerate}

ヴァイオリン演奏の微細な表現変化を照明に即座に反映させるため、システム全体の処理遅延を可能な限り低減する設計とした。
\begin{enumerate}
	\item[\textbf{(3)}]\textbf{実演が可能な安定した動作を実現すること}
\end{enumerate}

本研究では実際の演奏会形式での使用を想定しているため、長時間の動作においても処理が停止せず、安定して照明制御が行われることを設計要件とした。
そのため、計算負荷の大きい処理を最小限に抑える構成を採用している。

\begin{figure}[H]
	\centering
	\includegraphics[width=\hsize]{../fig/system-structure.png}
	\caption{リアルタイム照明制御システム処理フロー図}
	\label{fig:system-structure}
\end{figure}


\subsection{ハードウェア構成}
本システムは以下のハードウェアで構成される(表1)。

\begin{table}[H]
\centering
\small
\caption{ハードウェア構成}
\begin{tabular}{c}
\hline
\hline
PC(Windows 11)　\\
コンデンサマイク(音声入力)  \\
スマートフォン(ZIG-SIM用) \\
USB-DMXインタフェース(図2)  \\
DMX対応照明機器4台(図3)  \\
\hline
\end{tabular}
\end{table}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.3\textwidth]{../fig/usbdmx.png}
	\caption{USB-DMXインタフェース}
	\label{fig:usbdmx}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.3\textwidth]{../fig/light.png}
	\caption{DMX対応照明機器}
	\label{fig:light}
\end{figure}


マイクは演奏音をリアルタイムで取得するために使用し、スマートフォンはヴァイオリン演奏時の運弓動作を取得するために使用する。
照明機器は地面にステージ四方に配置され、床から天井に向けて演奏者を中心に照射する構成とした(図4)。

\begin{figure}[H]
	\centering
	\includegraphics[width=\hsize]{../fig/setting.png}
	\caption{機材配置図(上面図)}
	\label{fig:haichi}
\end{figure}

それぞれの配線はケーブルを用いたものを実線で示し、無線通信は点線で示している。

\subsection{ソフトウェア構成}
本システムはPython\cite{python}で実装され、すべての処理を単一プログラム上で統合している。
使用した主なライブラリを表2に示す。

\begin{table}[H]
\centering
\small
\caption{使用ライブラリと役割}
\begin{tabular}{c|c}
\hline
\hline
ライブラリ名 & 役割  \\
\hline
sounddevice\cite{sounddevice} & 音声ストリーム取得 \\
librosa\cite{librosa} & 音響特徴量抽出  \\
numpy\cite{numpy} & 数値処理、データ管理 \\
scikit-learn\cite{scikitlearn} & 機械学習モデル推論  \\
socket\cite{socket} & UDP通信  \\
ftd2xx\cite{ftd2xx} & DMX信号出力 \\
\hline
\end{tabular}
\end{table}

\subsection{音声入力処理}
音声入力は sounddevice.InputStream を用いて行う。
サンプリング周波数は22,050 Hz、モノラル入力とし、一定数のサンプルが蓄積されるとコールバック関数内でリングバッファに格納される。

リングバッファを採用することで、音声の欠落を防ぎつつ、安定したフレーム分割が可能となっている。
バッファ内のデータが22,050サンプルに達した時点で、1秒分の音声フレームとして切り出される。

\subsection{音響特徴量抽出}
各音声フレームに対して、librosaライブラリを用いて12種類の音響特徴量を算出する。
本研究で使用した特徴量それぞれの算出方法は以下の通りである。
\subsubsection{ゼロ交差率(ZCR)}

ゼロ交差率（ZCR）は、時間領域信号が正から負、または負から正へ符号反転する回数を表す特徴量であり、音の粗さやノイズ成分の多さを示す指標である。
フレーム長を N、時間領域信号をx[n] とすると、ZCR は次式で定義される。

\begin{figure}[H]
    \centering
    \includegraphics[width=\hsize]{../fig/zcr.png}
    \caption{ゼロ交差率 (ZCR) の定義式}
    \label{fig:zcr}
\end{figure}

ここで$\mathrm{sgn}(\cdot)$は符号関数である。
本研究では、librosa の librosa.feature.zero\_crossing\_rate() を用いて ZCR を算出し、各フレームにおける平均値を特徴量として採用した。

\subsubsection{スペクトル重心(Spectral Centroid)}

スペクトル重心は、周波数スペクトルにおけるエネルギー分布の重心位置を示す特徴量であり、音の明るさや鋭さと関連がある。
短時間フーリエ変換（STFT）によって得られた振幅スペクトルを $|X(k)|$、周波数ビンを$k$とすると、スペクトル重心 $C$ は次式で定義される。

\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\hsize]{../fig/spectralcentroid.png}
	\caption{スペクトル重心 (Spectral Centroid) の定義式}
	\label{fig:spectralcentroid}
\end{figure}

本研究では、librosa の librosa.feature.spectral\_centroid()を用いてスペクトル重心を算出し、各フレームにおける平均値を特徴量とした。

\subsubsection{スペクトル帯域幅(SpectralBandwidth)}
スペクトル帯域幅は、スペクトル重心を中心とした周波数成分の広がりを表す特徴量であり、音色の拡散度合いを示す指標である。
次数 p=2 の場合、スペクトル帯域幅 B は次式で表される。

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\hsize]{../fig/spectralbandwidth.png}
    \caption{スペクトル帯域幅 (Spectral Bandwidth) の定義式}
    \label{fig:spectralbandwidth}
\end{figure}

本研究では、librosa の librosa.feature.spectral\_bandwidth()を用いて算出した値の平均を特徴量として用いた。

\subsubsection{スペクトルコントラスト(SpectralContrast)}
スペクトルコントラストは、周波数帯域ごとのピーク成分と谷成分の差を表す特徴量であり、倍音構造の明瞭さや音色のコントラストを表現する。
librosaのlibrosa.feature.spectral\_contrast()を用いて複数帯域に分割したスペクトルコントラストを算出し、その平均値を特徴量として使用した。

\subsubsection{スペクトルフラットネス(SpectralFlatness)}

スペクトルフラットネスは、スペクトルの平坦さを示す特徴量であり、音が純音的かノイズ的かを判別する指標である。
振幅スペクトルを$|X(k)|$とすると、スペクトルフラットネス F は次式で定義される。

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\hsize]{../fig/spectralflatness.png}
    \caption{スペクトルフラットネス (Spectral Flatness) の定義式}
    \label{fig:spectralflatness}
\end{figure}

本研究では、librosa の librosa.feature.spectral\_flatness()を用いて算出した。

\subsubsection{スペクトルロールオフ（SpectralRolloff）}

スペクトルロールオフは、全スペクトルエネルギーの一定割合が含まれる周波数を示す特徴量である。
本研究では、全エネルギーの 85\% を含む周波数をロールオフ周波数として定義し、librosa のlibrosa.feature.spectral\_rolloff()を用いて算出した。

\subsubsection{音量}

音量（RMS）は、音声信号の振幅エネルギーを表す特徴量である。
時間領域信号 x[n] に対して、RMS は次式で定義される。

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\hsize]{../fig/rms.png}
    \caption{音量（RMS）の定義式}
    \label{fig:rms}
\end{figure}

本研究では、librosa の librosa.feature.rms()を用いて算出した。

\subsubsection{メル周波数ケプストラム係数（MFCC）}

MFCC は、人間の聴覚特性を考慮したメル周波数尺度に基づく特徴量であり、音色を表現する代表的な指標である。
本研究では、音声信号に対してメルフィルタバンクを適用し、対数パワースペクトルに離散コサイン変換（DCT）を施すことで MFCC を算出する。
librosa の librosa.feature.mfcc() を用いて 20 次元の MFCC を算出し、各次元の平均値を特徴量として採用した。

さらに、時間的変化を捉えるため、MFCC の一次差分および二次差分を以下の式で定義し、librosa.feature.delta() を用いて算出した。

\begin{figure}[H]
    \centering
    \includegraphics[width=\hsize]{../fig/mfccdelta.png}
    \caption{MFCC の一次差分および二次差分の定義式}
    \label{fig:mfccdelta}
\end{figure}

\subsubsection{クロマ特徴量（Chroma）}

クロマ特徴量は、12 音階ごとのエネルギー分布を表す特徴量であり、音高や和声的特徴を反映する。
本研究では、librosa の librosa.feature.chroma\_stft()を用いてクロマ特徴量を算出し、その平均値を特徴量として使用した。

\subsubsection{トーナルネットワーク特徴量（Tonnetz）}
トーナルネットワーク特徴量（Tonnetz）は、調性や和声構造を表現する特徴量である。
調波成分を抽出した音声信号に対して、librosa の librosa.feature.tonnetz()を用いて6 次元の Tonnetz 特徴量を算出し、各次元の平均値を特徴量として採用した。

\subsection{演奏動作取得}
本システムでは、演奏者の弓の上下運動(上げ弓・下げ弓)(図11\cite{v}・図12\cite{v})を取得するために、スマートフォンをZIG-SIMセンサとして利用している。
スマートフォンにはジャイロセンサが搭載されており、X軸方向の角速度を取得することで弓の前後運動を検出する。

\begin{figure}[H]
    \centering
    \includegraphics[width=\hsize]{../fig/up.png}
    \caption{上げ弓}
    \label{fig:up}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\hsize]{../fig/down.png}
    \caption{下げ弓}
    \label{fig:down}
\end{figure}

\subsubsection{ハードウェア配置}
スマートフォンは演奏者の右腕に装着し、弓を持つ手首付近に位置させる。
これにより、弓の上下運動が腕の回転として反映されるため、ジャイロセンサで容易に検出可能となる。

\subsubsection{データ通信方式}
右腕に装着したスマートフォンから取得したジャイロデータはUDP通信を用いてPCに送信する。
UDPはTCPに比べて通信遅延が少なく、リアルタイム性を重視した設計である。

\subsubsection{運弓方向判定アルゴリズム}
PCに送信されたx軸のジャイロデータをもとに、弓方向の判定を行う。
弓方向の判定にあたっては、**角速度の値に基づく判定**と、**角速度変化量に基づく判定**を併用している。

\begin{enumerate}
	\item[\textbf{(1)}]\textbf{角速度の値による判定}
\end{enumerate}

閾値を0.01に設定し、ジャイロデータ ＞ 0.01 なら上げ弓、ジャイロデータ ＜ -0.01 なら下げ弓と判定している。

\begin{enumerate}
	\item[\textbf{(2)}]\textbf{角速度閾値 0.01 の設定理由}
\end{enumerate}

この値は理論的に決定したものではなく、実際の演奏データを用いた試行錯誤によって決定した経験的な値である。具体的には、以下の手順で検討を行った。

\begin{enumerate}
	\item[(2-1)]弓を完全に静止させた状態で数十秒間データを取得
\end{enumerate}

\begin{enumerate}
	\item[(2-2)]弓を非常にゆっくり動かした場合の角速度を計測
\end{enumerate}

\begin{enumerate}
	\item[(2-3)]通常の演奏速度における角速度分布を確認
\end{enumerate}
その結果、
\begin{enumerate}
	\item[・]静止時の角速度はおおむね ±0.005 以下
\end{enumerate}

\begin{enumerate}
	\item[・]意図的な弓の動きは ±0.01 を明確に超える
\end{enumerate}
という傾向が確認された。

したがって、0.01 という値は「ノイズと実際の演奏動作を分離できる最小限の境界値」として妥当であると判断した。

\begin{enumerate}
	\item[\textbf{(3)}]\textbf{角速変化量による判定}
\end{enumerate}

角速度変化量とは、「今の角速度が、直前と比べてどれだけ変わったか」を表す値である。
本研究では、連続する2時刻のジャイロX軸角速度の差を用いて、次のように定義している。

角速度変化量 ＝ 今回の角速度 − 前回の角速度

この値が大きい場合、「弓が急に動き始めた」「弓の方向が切り替わった」ことを意味する。
本研究では、次の3つのケースに分類して弓方向を判定している。

\begin{enumerate}
	\item[(3-1)]各速度変化量が正に大きく変化した場合→上げ弓
\end{enumerate}

\begin{enumerate}
	\item[(3-2)]各速度変化量が負に大きく変化した場合→下げ弓
\end{enumerate}

この2つを併用することで、微妙な演奏タッチや小さな振動が含まれる状況においても誤判定を避け、弓の動作をより正確に捉えることが可能となる。

このように、本システムではジャイロセンサによる演奏動作取得をリアルタイムで行い、音響特徴量と組み合わせて照明制御に反映している。

\subsection{機械学習モデルによる照明特徴量推定}
抽出された音響特徴量を用いて、機械学習モデルにより照明特徴量を推定する。
モデルは\textbf{「RGB(色)」}、\textbf{「Brightness(明るさ)」}、\textbf{「点滅速度」}、の3つの照明特徴量を出力する。
推定値は0〜1の正規化値として出力され、DMX出力時に0〜255の整数値へスケーリングされる。
また、機械学習モデルについては次章で詳述する。

\subsection{DMX信号生成と照明制御}
本節では、機械学習モデルおよび演奏動作推定によって得られた照明特徴量を、実際の照明機器に反映させるためのDMX信号生成および制御方法について詳細に述べる。
DMX512は舞台照明分野において広く用いられている通信規格であり、本研究ではこの標準規格を用いることで、実際のステージ環境に近い条件での実演を可能にしている。

\subsubsection{DMX512プロトコルの概要}
DMX512は、最大512チャンネルの制御データを1フレームとして送信する一方向通信プロトコルである。
各チャンネルは0〜255の8bit値を持ち、照明機器の色、明るさ、点滅速度を制御することができる。

\subsubsection{DMXハードウェアインタフェース}
DMX信号の出力には、USB接続のDMXインタフェースを使用した。
本研究ではFTDI社製チップを搭載したデバイスを用い、Pythonからftd2xxライブラリを介して制御を行っている。

\subsubsection{照明特徴量からDMX値への変換}
照明特徴量（RGB値, Brightness, 点滅速度）は、DMXチャンネルに対応する値へ変換される。

これらは連続値として出力されるため、DMXチャンネルに割り当てる際には0〜255の整数値へ正規化・クリッピング処理を行う。

具体的には、各値に対して以下の処理を適用する。

\begin{enumerate}
	\item[(1)]正規化：各特徴量の最小値・最大値を基に0〜1の範囲にスケーリング
	\item[(2)]スケーリング：0〜1の値を255倍して0〜255の範囲に変換
	\item[(3)]クリッピング：小数点以下を切り捨て、整数値に変換
\end{enumerate}

\subsubsection{複数照明機器への同時制御}

本システムでは、4台の照明機器を同時に制御する構成とした。
各照明機器は異なるDMXアドレスに割り当てられており、同一の照明特徴量を用いて各機器へ同時に信号を送信する。

DMX配列は513バイトで構成され、照明機器ごとにチャンネル割り当てを行っている(表3)。

\subsubsection{演奏動作情報との統合制御}
音響特徴量から推定された照明特徴量とは別に、ZIG-SIMによって取得した運弓方向情報をDMX制御に直接反映させている。

具体的には、運弓方向に応じて上下動作のDMXチャンネル（チャンネル2）の値を変更し、照明の上下動作を演奏者の弓の動きに同期させている。

\begin{table}[H]
\centering
\small
\caption{チャンネルの割り当て}
\begin{tabular}{c|c}
\hline
\hline
チャンネル & 役割  \\
\hline
CH1 & 未使用 \\
CH2 & 上下動作  \\
CH3 & 点滅速度 \\
CH4 & R成分  \\
CH5 & G成分  \\
CH6 & B成分 \\
CH7 & 明るさ \\
\hline
\end{tabular}
\end{table}

\begin{enumerate}
	\item[(1)]上げ弓：DMX値を120に設定（照明上昇）(図13参照)
	\item[(2)]下げ弓：DMX値を0に設定（照明下降）(図13参照)
\end{enumerate}
とすることで、演奏者の身体動作と照明の物理的動きを同期させている。
この設計により、音だけでなく演奏動作そのものが照明演出に影響を与える構造となっている。

\begin{figure}[H]
	\centering
	\includegraphics[width=0.3\textwidth]{../fig/lightupdown.png}
	\caption{照明上下動作}
	\label{fig:lightupdown}
\end{figure}

\subsubsection{リアルタイム出力制御と更新周期}
DMX 信号の送信は約 10ms 周期で実行される。
これは、人間が遅延として知覚しにくい範囲であり、演奏と照明がほぼ同時に変化しているように感じられる。

\section{照明予測モデル}
本章では、3.7節で述べたヴァイオリン演奏音から照明特徴量を予測する機械学習モデルの構築方法について詳述する。

本研究では、音響特徴量を入力とし、それらの照明特徴量を予測するモデルを用いて照明の制御を行っている。
本研究で機械学習モデルを用いたのは、先行研究には音響特徴量から照明特徴量を直接予測し、リアルタイムの自動照明制御として機能させる学習ベースの枠組みが十分に見られず、新しいアプローチとなると考えたためである。
最終的に構築されたモデルは、リアルタイムの演奏音を入力として照明を自動制御する「照明予測モデル」として機能する。
図14に、照明予測モデル構築のためのデータ生成フローを示す。この図に基づき、各ステップを詳細に説明する。

\begin{figure}[H]
	\centering
	\includegraphics[width=\hsize]{../fig/dataflow.png}
	\caption{照明予測モデル構築のためのデータ生成フロー}
	\label{fig:dataflow}
\end{figure}

\subsection{データベースの構築}
まずは、モデルを学習させるためのデータベースの構築を行う。

\subsubsection{映像データの収集}
照明予測モデルの学習には、音響特徴量とそれに対応する照明特徴量のペアが必要である。
そのためのデータソースとして、実際のヴァイオリン演奏におけるライブ映像データを用意した。
これらは実際の照明演出を伴う演奏映像であり、音楽と照明の対応関係を直接的に取得できる点で有用である。
本研究では、入手可能な映像素材が限られているという制約のもと、音質および照明演出が比較的明瞭に記録されており、学習データとして十分な情報量を有すると判断された3種類の映像を選定した。
各映像の詳細は以下の通りである。

ライブ映像データの1つ目は、ヴァイオリン演奏曲「千本桜\cite{senbonzakura}」の演奏を記録した映像(図15)である\cite{stradivariussenbonzakura}。

\begin{figure}[H]
	\centering
	\includegraphics[width=\hsize]{../fig/senbonzakura.png}
	\caption{*千本桜* 演奏映像}
	\label{fig:senbonzakura}
\end{figure}

2つ目のライブ映像データは、ヴァイオリンソロによるクラシック楽曲「ソナタ第三番\cite{recochokusonata}」を演奏した映像(図16)である\cite{stradivariussonata}。

\begin{figure}[H]
	\centering
	\includegraphics[width=\hsize]{../fig/sonata.png}
	\caption{*Sonata* 演奏映像}
	\label{fig:sonata}
\end{figure}

3つ目のライブ映像データは、ヴァイオリンソロによる楽曲「スプラッシュ\cite{recochokusplash}」を演奏した映像(図17)である\cite{stradivariussplash}。

\begin{figure}[H]
	\centering
	\includegraphics[width=\hsize]{../fig/splash.png}
	\caption{*スプラッシュ* 演奏映像}
	\label{fig:splash}
\end{figure}

これらの映像に対して、音響特徴量と照明特徴量を抽出するための処理を行い、照明予測モデルの学習データを生成する。

\subsubsection{音響特徴量の抽出}
\begin{enumerate}
	\item[\textbf{(1)}]\textbf{ヴァイオリン音源の分離}
\end{enumerate}

ライブ映像には、会場の反響音、観客ノイズ、伴奏音など、ヴァイオリン以外の音が含まれる。
音響特徴量は不要な音が混入したまま学習するとモデルの精度劣化を招く。
そこで音源分離モデルDemucs\cite{mucs}を用い、音声からヴァイオリン成分を抽出した。

\begin{table*}[t]
\centering
\small
\caption{各映像の音響特徴量のCSVデータ(一部抜粋)}

\begin{tabular}{c|cccccccccc}
\hline
\multicolumn{11}{c}{千本桜} \\
\hline
\hline
Time [s] & Centroid & Contrast & Bandwidth & Rolloff & ZCR & RMS &
MFCC\_1 & Delta\_MFCC\_1 & Chroma\_1 & Tonnetz\_1 \\
\hline
1.0 & 8894 & 14.5 & 6149 & 16606 & 0.28 & 0.001 & -948 & -4.54 & 0.67 & 0.03 \\
2.0 & 6722 & 17.9 & 3928 & 11385 & 0.23 & 0.005 & -661 & 4.44 & 0.76 & 0.12 \\
3.0 & 5257 & 23.2 & 3046 & 8144.0 & 0.18 & 0.033 & -385 & 0.52 & 0.57 & 0.20 \\
4.0 & 4459 & 21.7 & 2653 & 6389.1 & 0.17 & 0.023 & -398 & 2.95 & 0.75 & 0.26 \\
5.0 & 3987 & 19.5 & 2593 & 5936.9 & 0.12 & 0.028 & -358 & -1.11 & 0.28 & 0.32 \\
6.0 & 3678 & 18.3 & 3031 & 7210.9 & 0.16 & 0.050 & -347 & -0.45 & 0.37 & 0.20 \\
7.0 & 3521 & 17.8 & 2742 & 7049.2 & 0.17 & 0.039 & -410 & -0.38 & 0.30 & 0.27 \\
8.0 & 3400 & 16.7 & 2896 & 7186.3 & 0.14 & 0.008 & -471 & -2.29 & 0.43 & 0.18 \\
\hline
\multicolumn{11}{c}{ソナタ第三番} \\
\hline
\hline
Time [s] & Centroid & Contrast & Bandwidth & Rolloff & ZCR & RMS &
MFCC\_1 & Delta\_MFCC\_1 & Chroma\_1 & Tonnetz\_1 \\
\hline
1.0 & 1947 & 9.04 & 1705 & 3958.3 & 0.03 & 0.001 & -1068 & 3.25 & 0.04 & -0.05 \\
2.0 & 2898 & 20.1 & 3562 & 4256.3 & 0.07 & 0.000 & -809.0 & 1.43 & 0.12 & -0.20 \\
3.0 & 1695 & 19.9 & 2284 & 2615.9 & 0.03 & 0.001 & -675.9 & 1.79 & 0.35 & -0.07 \\
4.0 & 1560 & 20.1 & 1951 & 2630.2 & 0.03 & 0.002 & -618.0 & -0.26 & 0.43 & 0.27 \\
5.0 & 1728 & 19.1 & 1909 & 2767.1 & 0.05 & 0.002 & -606.8 & -0.07 & 0.56 & -0.04 \\
6.0 & 1887 & 19.5 & 1940 & 2967.1 & 0.05 & 0.002 & -604.9 & -0.06 & 0.28 & 0.02 \\
7.0 & 2113 & 19.0 & 2080 & 3132.7 & 0.06 & 0.002 & -585.3 & 0.05 & 0.22 & 0.03 \\
8.0 & 2475 & 19.9 & 2305 & 3629.7 & 0.07 & 0.002 & -602.9 & -0.41 & 0.08 & -0.01 \\
\hline
\multicolumn{11}{c}{スプラッシュ} \\
\hline
\hline
Time [s] & Centroid & Contrast & Bandwidth & Rolloff & ZCR & RMS &
MFCC\_1 & Delta\_MFCC\_1 & Chroma\_1 & Tonnetz\_1 \\
\hline
1.0 & 1002 & 0.00 & 579.2 & 1750.0 & 0.00 & 0.000 & -1131 & 0.00 & 0.08 & -0.01 \\
2.0 & 3582 & 8.98 & 3553 & 8076.6 & 0.00 & 0.008 & -1055 & 1.65 & 0.27 & 0.07 \\
3.0 & 1603 & 2.24 & 1542 & 3564.3 & 0.00 & 0.001 & -1109 & -0.29 & 0.16 & -0.02 \\
4.0 & 1564 & 15.0 & 2032 & 2707.4 & 0.03 & 0.032 & -774.5 & 4.52 & 0.23 & 0.14 \\
5.0 & 1798 & 18.4 & 2226 & 2553.0 & 0.05 & 0.054 & -641.0 & 1.15 & 0.37 & 0.31 \\
6.0 & 1893 & 20.1 & 2182 & 2354.2 & 0.05 & 0.055 & -667.7 & -0.62 & 0.39 & 0.27 \\
7.0 & 2060 & 20.1 & 2190 & 2775.3 & 0.05 & 0.057 & -634.1 & -0.16 & 0.30 & 0.33 \\
8.0 & 759.7 & 19.8 & 2239 & 3193.1 & 0.05 & 0.062 & -621.3 & -1.14 & 0.25 & 0.17 \\
\hline
\end{tabular}

\end{table*}

\begin{enumerate}
	\item[\textbf{(2)}]\textbf{ヴァイオリン特徴量の抽出}
\end{enumerate}

各映像から抽出したヴァイオリン音声を音声ファイルとして保存し、それらに対して1秒ごとに音響特徴量を算出した。
本節ではその具体的な処理内容を述べる。

まず、音声ファイルを Pythonのlibrosaライブラリ を用いて読み込む。
この際、サンプリングレートは 44.1 kHz、チャンネル数は モノラル に統一する。
読み込んだ波形データおよびサンプリングレートから音声全体の長さ（秒）を算出し、その長さに応じて 0 秒から終端まで 1 秒刻みの時間インデックスを生成する。
各 1 秒セグメントごとに extractAdvancedFeatures 関数を用いて3.5節で述べた音響特徴量を算出する。
算出されたすべての音響特徴量は一つの辞書にまとめられ、最終的に「1 行を 1 秒セグメント、1 列を 1 種類の音響特徴量」とする特徴量テーブルを生成する。

この特徴量テーブルは CSV 形式で保存され、学習用 CSV データとして構築される。
表4に、各ライブ映像から抽出された音響特徴量のCSVデータの一部を示す。

\subsubsection{照明特徴量の抽出}
続いて、各映像からRGB値 (色)、Brightness(明るさ)、点滅速度の 3 種類の照明特徴量を1秒ごとに抽出し、学習用CSVデータとして構築した。
本節では、照明特徴量の抽出手法、CSVデータ構造について詳述する。

\begin{enumerate}
	\item[\textbf{(1)}]\textbf{人体検出による照明領域の特定}
\end{enumerate}

照明特徴量の抽出において最も重要な前処理は、YOLOv8\cite{yolov8} を用いた人物領域の除外である。
人物領域を含んだままRGBやBrightnessを算出すると、肌色や衣服の色、演奏者の動きによる輝度変化が混入し、照明そのものの変化を正確に捉えることができない。

そこで本研究では、人物検出後に膨張処理を施し、演奏者周囲も含めて背景から除外した。
そのうえで、背景画素のうち明度上位10％のみを抽出し、照明器具に最も近い画素群を対象としてRGBおよびBrightnessを算出している。


\begin{enumerate}
	\item[\textbf{(2)}]\textbf{RGB値の算出と各映像のcsvデータ}
\end{enumerate}

映像のフレーム画像を B,G,R チャンネルに分解し、背景マスクかつ上位10\%領域に該当する画素を抽出し、それぞれの平均値を計算した。

これらの値をフレーム単位でバッファに蓄積し、1秒ごとに平均を取ることで、1秒単位のRGB値の照明特徴量としてCSVに書き出した(表5,6,7)。

\begin{enumerate}
	\item[\textbf{(3)}]\textbf{Brightnessの算出と各映像のcsvデータ}
\end{enumerate}

背景照明の明るさは、グレースケール画像（または HSVのV成分）から、同じく上位10\%の明度領域を抽出して平均値を求めた。
また、RGB値と同様に、1秒ごとに平均を取ることで、1秒単位のBrightness値としてCSVに書き出した(表8,9,10)。

\begin{enumerate}
	\item[\textbf{(4)}]\textbf{点滅速度の算出と各映像のcsvデータ}
\end{enumerate}

照明の点滅速度を抽出するために、1秒ごとの平均Brightness値を時系列信号として蓄積し、
前後5秒の局所FFT を行うことで点滅に対応する周波数成分を推定し、csvに書き出した(表11,12,13)。

以上により、各映像から音響特徴量と照明特徴量のペアを1秒ごとに抽出し、学習用CSVデータとして構築した。

\begin{table}[H]
\centering
\small
\caption{千本桜のRGB値のCSVデータ(一部抜粋)}
\begin{tabular}{c|ccc}
\hline
\hline
Time [s] & R & G & B \\
\hline
1.0 & 90.1 & 45.7 & 62.1 \\
2.0 & 93.9 & 60.2 & 74.1 \\
3.0 & 89.8 & 60.7 & 70.0 \\
4.0 & 84.2 & 42.5 & 53.6 \\
5.0 & 84.8 & 32.2 & 45.8 \\
6.0 & 130.6 & 16.2 & 31.1 \\
7.0 & 125.8 & 25.1 & 33.2 \\
8.0 & 124.7 & 35.9 & 38.0 \\
\hline
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\small
\caption{ソナタ第3番のRGB値のCSVデータ(一部抜粋)}
\begin{tabular}{c|ccc}
\hline
\hline
Time [s] & R & G & B \\
\hline
1.0 & 50.8 & 45.1 & 68.7 \\
2.0 & 52.3 & 45.6 & 69.7 \\
3.0 & 54.5 & 46.9 & 71.6 \\
4.0 & 53.9 & 46.2 & 70.7 \\
5.0 & 65.0 & 58.0 & 81.3 \\
6.0 & 120.8 & 119.2 & 131.2 \\
7.0 & 128.8 & 126.4 & 136.2 \\
8.0 & 139.3 & 137.1 & 145.3 \\
\hline
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\small
\caption{スプラッシュのRGB値のCSVデータ(一部抜粋)}
\begin{tabular}{c|ccc}
\hline
\hline
Time [s] & R & G & B \\
\hline
1.0 & 91.6 & 70.0 & 76.5 \\
2.0 & 54.1 & 36.1 & 50.5 \\
3.0 & 17.8 & 11.4 & 23.9 \\
4.0 & 45.1 & 36.0 & 66.9 \\
5.0 & 239.0 & 238.1 & 242.0 \\
6.0 & 189.4 & 201.3 & 208.0 \\
7.0 & 191.6 & 223.9 & 233.7 \\
8.0 & 200.0 & 203.9 & 199.8 \\  
\hline
\end{tabular}
\end{table}


\begin{table}[H]
\centering
\small
\caption{千本桜のBrightnessのCSVデータ(一部抜粋)}
\begin{tabular}{c|c}
\hline
\hline
Time [s] & Brightness  \\
\hline
1.0 & 59.8 \\
2.0 & 69.3  \\
3.0 & 87.0  \\
4.0 & 53.5  \\
5.0 & 47.7  \\
6.0 & 53.3 \\
7.0 & 56.7 \\
8.0 & 63.3 \\
\hline
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\small
\caption{ソナタ第3番のBrightnessのCSVデータ(一部抜粋)}
\begin{tabular}{c|c}
\hline
\hline
Time [s] & Brightness  \\
\hline
1.0 & 50.2 \\
2.0 & 49.8  \\
3.0 & 52.9  \\
4.0 & 51.1  \\
5.0 & 63.2  \\
6.0 & 124.0 \\
7.0 & 130.9 \\
8.0 & 140.5 \\
\hline
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\small
\caption{スプラッシュのBrightnessのCSVデータ(一部抜粋)}
\begin{tabular}{c|c}
\hline
\hline
Time [s] & Brightness  \\
\hline
1.0 & 77.5 \\
2.0 & 35.3  \\
3.0 & 19.2  \\
4.0 & 43.4  \\
5.0 & 242.3  \\
6.0 & 200.0 \\
7.0 & 215.8 \\
8.0 & 202.8 \\
\hline
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\small
\caption{千本桜の点滅速度のCSVデータ(一部抜粋)}
\begin{tabular}{c|c}
\hline
\hline
Time [s] & Blinking Speed  \\
\hline
1.0 & 0.40 \\
2.0 & 0.33  \\
3.0 & 0.29  \\
4.0 & 0.25  \\
5.0 & 0.44  \\
6.0 & 0.40 \\
7.0 & 0.40 \\
8.0 & 0.20 \\
\hline
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\small
\caption{ソナタ第3番の点滅速度のCSVデータ(一部抜粋)}
\begin{tabular}{c|c}
\hline
\hline
Time [s] & Blinking Speed  \\
\hline
1.0 & 0.40 \\
2.0 & 0.33  \\
3.0 & 0.29  \\
4.0 & 0.25  \\
5.0 & 0.22  \\
6.0 & 0.20 \\
7.0 & 0.20 \\
8.0 & 0.20 \\
\hline
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\small
\caption{スプラッシュの点滅速度のCSVデータ(一部抜粋)}
\begin{tabular}{c|c}
\hline
\hline
Time [s] & Blinking Speed  \\
\hline
1.0 & 0.40 \\
2.0 & 0.33  \\
3.0 & 0.29  \\
4.0 & 0.25  \\
5.0 & 0.22  \\
6.0 & 0.20 \\
7.0 & 0.20 \\
8.0 & 0.20 \\
\hline
\end{tabular}
\end{table}

\subsection{照明予測モデル(仮)の学習}

4.1節で収集した学習用CSVデータを用いて、照明予測モデルの学習を行った。

\subsubsection{学習データの概要}
照明予測モデル（仮）の学習には、4.1節にて生成した音響特徴量と照明特徴量のCSVのデータを1つのデータセットとして統合し、使用した。

\subsubsection{入力変数（音響特徴量）}
照明予測モデルへの入力として使用した音響特徴量のデータセットは4.1.2節で述べた通りである。

\subsubsection{出力変数（照明特徴量）}
照明予測モデルの出力として使用した照明特徴量のデータセットは4.1.3節で述べた通りである。

\subsubsection{学習データとテストデータの分割}
構築したデータセットは、モデルの汎化性能を評価するため、全体の80\%を学習用データ、20\%をテスト用データに分割した。
分割はランダムに行い、乱数シードを固定することで実験の再現性を確保した。
\begin{enumerate}
	\item[(1)]学習データ：モデルの学習に使用
\end{enumerate}
\begin{enumerate}
	\item[(2)]テストデータ：モデルの性能評価に使用
\end{enumerate}

\subsubsection{モデルの構造と学習方法}
照明予測モデル（仮）には、回帰モデルとして Random Forest Regressor\cite{randomforestregressor} を採用した。Random Forest は複数の決定木を組み合わせたアンサンブル学習手法である。
決定木の本数は100とし、学習精度と計算コストのバランスを考慮した。

\subsubsection{学習の実行とモデル保存}
学習用データを用いて Random Forest モデルを学習し、音響特徴量と照明特徴量の対応関係を獲得した。
学習が完了したモデルは、再利用を目的として joblib を用いてファイルとして保存した。

\subsubsection{照明予測モデル(仮)の位置づけ}
本節で構築した照明予測モデル(仮)は、ライブ映像由来の実測データのみを用いて学習された点に特徴がある。
一方で、演奏種類やデータ量には限界があるため、本研究ではこのモデルを「照明予測モデル（仮）」と位置付けている。

\subsection{MIDIデータの生成と特徴量抽出}
本節では、4.2で構築した照明予測モデル（仮）を用いて、MIDI形式のヴァイオリン楽曲データから音響特徴量および照明特徴量を生成する手法について述べる。

MIDIデータは実演奏の録音と異なり、演奏指示情報のみを保持する形式である。
そのため、本研究ではMIDIデータを音声信号へ変換した上で、ライブ映像データと同一の音響特徴量を抽出し、照明予測モデル（仮）によって照明特徴量を推定するという処理フローを採用した。

\subsubsection{MIDIデータセットの概要}
本研究で使用したMIDIデータは、ヴァイオリン演奏において代表的かつ教育的価値の高い楽曲群から構成されている。
具体的には、「Paganini\cite{paganini}」、「Kayser\cite{kayser}」、「Wohlfahrt\cite{wohlfahrt}」の作品を中心としたヴァイオリンソロのMIDIデータを用いた。

Paganini の作品は、高度な演奏技巧や急激な音高変化、速いパッセージを多く含み、演奏表現の幅が非常に広い。
一方で、Kayser および Wohlfahrt の練習曲は、音階練習や基本的な運弓・音程感覚の習得を目的とした楽曲が多く、比較的安定した音高変化と明瞭なフレーズ構造を持つ。

MIDIデータは、演奏音の音高、音価、発音タイミングなどを厳密に数値情報として保持しているため、音響特徴量抽出において再現性が高く、楽曲間の比較にも適している。
特に、これらの練習曲・技巧曲は、音楽教育の現場でも広く使用されており、ヴァイオリン演奏の典型的な音域・音色変化・フレーズ構造を網羅している点で、本研究の補助的データとして有用である。
\subsubsection{MIDIデータの音声変換}
MIDIデータは直接音響特徴量を算出することができないため、まず音声信号へ変換する必要がある。
本研究では、ソフトウェアシンセサイザー FluidSynth\cite{fluidsynth}を用い、MIDIファイルをWAV形式へ変換した。

MIDIからWAVへの変換は各ファイルごとに自動的に実行される。

\subsubsection{音響特徴量の抽出}
生成されたWAVファイルに対し、Pythonライブラリ librosa を用いて音響特徴量を抽出した。
抽出する特徴量の種類および構成は、4.2節で構築した照明予測モデル（仮）の学習時に用いたデータセットと完全に一致させている。

\subsubsection{1秒単位への集約処理}
抽出された音響特徴量はフレーム単位のデータであるため、そのままでは照明制御モデルとの時間スケールが一致しない。
そこで、本研究では 1秒単位で特徴量を平均化する処理を行った。

具体的には、サンプリングレートおよびhop lengthから1秒あたりのフレーム数を算出し、該当フレーム範囲の特徴量を平均することで、1秒ごとの特徴ベクトルを生成した。

この処理により、ライブ映像データと同様に「1秒 = 1照明フレーム」という時間対応が可能となる。

\subsubsection{照明予測モデル（仮）による照明特徴量推定}
1秒単位に集約された音響特徴量は、4.2節で構築した 照明予測モデル（仮） に入力される。
本モデルは、ライブ映像データから学習した「音響特徴量と照明特徴量の対応関係」を保持しており、MIDI由来の音響特徴量に対しても照明特徴量を推定することが可能である。
推定される照明特徴量はRGB値、Brightness、点滅速度の3種類である。

\subsubsection{CSV形式での出力}
各MIDIファイルについて、以下の情報を1つのCSVデータとして出力した。
\begin{enumerate}
	\item[(1)]時間(秒)
\end{enumerate}
\begin{enumerate}
	\item[(2)]音響特徴量
\end{enumerate}
\begin{enumerate}
	\item[(3)]照明予測モデル(仮)による推定照明特徴量
\end{enumerate}

これにより、MIDIデータから得られた音響特徴量と対応する照明特徴量を一元的に管理できるようになった。

\subsection{MIDIデータを含めた照明予測モデルの学習}
4.3節で生成したMIDIデータ由来の特徴量データを加え、照明予測モデルを再学習する手法について述べる。
本節で構築されるモデルは、ライブ映像データに加えてMIDIデータも含めた拡張データセットを用いて学習されたモデルであり、以降「照明予測モデル(本)」と呼ぶ。

4.2節では、ライブ映像データのみを用いた学習によって、音響特徴量と実際の照明演出との対応関係を獲得できることを示した。
しかし、ライブ映像データは演奏数や演奏スタイルに限りがあり、機械学習モデルとしての汎化性能には一定の制約が存在する。
そこで本研究では、MIDIデータを用いて演奏パターンの多様性を拡張し、より汎用的な照明予測モデル(本)の構築を試みた。

\subsubsection{MIDIデータ導入の目的}
MIDIデータを学習データに加える目的は、以下の点にある。

第一に、演奏音のバリエーションを大幅に増加させることである。
MIDIデータには、音高変化、フレーズ構造、音価の違いなど、実演奏に近い音楽的情報が含まれており、ライブ映像データだけでは不足しがちな演奏パターンを補完できる。

第二に、モデルが特定の演奏映像や照明環境に過度に依存することを防ぎ、未知の演奏音に対しても安定した照明予測を行えるようにする点である。

\subsubsection{学習データセットの構成}
本研究では、4.2節で生成したMIDIデータ由来の特徴量データを、4.1節で構築したライブ映像データ由来の特徴量データに追加し、拡張された学習データセットを構成した。

MIDIデータについては、実際の照明映像を伴わないため、4.2節で学習した照明予測モデル（仮）を用いて照明パラメータを推定し、擬似的な教師データとして扱っている。

\subsubsection{特徴量構成と前処理}
入力特徴量および出力変数の構成は、4.2節で使用したものと同一であるため、本節では詳細な説明を省略する。
\begin{enumerate}
	\item[(1)]入力：音響特徴量
\end{enumerate}
\begin{enumerate}
	\item[(2)]出力：照明特徴量
\end{enumerate}

\subsubsection{学習手法}
学習手法としては、4.2節と同様に Random Forest Regressor を採用した。
モデル構造や主要なハイパーパラメータも、同一に設定している。

\subsubsection{学習データとテストデータの分割}
統合されたデータセットは、4.2節と同様に学習用データとテスト用データに分割した。
分割比率および分割方法は同一であり、実験条件の公平性を保っている。

\subsubsection{学習の実行とモデル保存}
拡張された学習データセットを用いて Random Forest モデルの学習を実行し、音響特徴量と照明特徴量の対応関係を再度獲得した。
学習が完了したモデルは、 joblib を用いてファイルとして保存し、リアルタイム照明制御システムに直接組み込んだ。

\subsubsection{照明予測モデル(本)の位置づけ}
本節で学習された照明予測モデル(本)は、ライブ映像由来の実測データと、MIDIデータによって補完された多様な演奏パターンの両方を内包している。

このモデルを、本研究における最終的な照明予測モデルと位置付け、システムに組み込む。

\subsection{照明予測モデルの評価}
本節では、4.4節で構築した MIDIデータを含めた照明予測モデル(本)の性能評価について述べる。
評価には、機械学習における回帰問題で一般的に用いられる決定係数（R²スコア）を採用した。

\subsubsection{評価指標：決定係数 (R²スコア)}
本研究では、照明予測モデル(本)の性能を定量的に評価するため、決定係数 R²（Coefficient of Determination） (図18)を用いた。
R²スコアは、モデルの予測値が実測値の分散をどの程度説明できているかを示す指標であり、以下の式で定義される。

\begin{figure}[H]
	\centering
	\includegraphics[width=\hsize]{../fig/r2-formula.jpg}
	\caption{決定係数 (R²) の定義式}
	\label{fig:r2-formula}
\end{figure}

R²スコアは 1 に近いほど予測精度が高く、0 は平均値予測と同等、負の値は平均予測より劣ることを示す。

\subsubsection{評価方法}
評価には、学習時に使用していないテストデータを用いた。
テストデータには、ライブ映像由来データを中心に含めることで、実際の照明演出との整合性を重視した評価を行った。

評価は、RGB値、Brightness、点滅速度といった複数の照明特徴量を同時に含む多出力回帰問題として実施し、全出力を総合したR²スコアを算出した。

\subsubsection{評価結果}
最終照明予測モデル(本)に対する評価の結果、R²スコアは 0.85 を示した(図19)。

\begin{figure}[H]
	\centering
	\includegraphics[width=\hsize]{../fig/r2.png}
	\caption{決定係数 (R²) の結果}
	\label{fig:r2}
\end{figure}


この結果は、音響特徴量から照明特徴量への変換において、モデルが実測照明データの分散の約85\%を説明できていることを意味する。
すなわち、照明の色、明るさ、点滅といった複合的な要素を、演奏音のみから高精度に推定できていることが確認された。

\subsection{音響特徴量と照明特徴量の対応関係}
本節では、最終的に構築した照明予測モデル(本)が、音響特徴量と照明特徴量の間にどのような対応関係を学習しているかを分析する。
前節まででは、R²スコアを用いた定量的評価によりモデル全体の性能を示したが、本節ではさらに踏み込み、「どの音響的要素が、どの照明要素に強く影響しているか」を明らかにすることを目的とする。

音と照明の関係は本質的に主観的であり、一意に定義されるものではない。
しかし、機械学習モデルが内部的に利用している特徴量の重要度を解析することで、モデルがどのような音響的判断基準に基づいて照明を制御しているかを客観的に考察することが可能となる。

\subsubsection{解析手法の概要}
音響特徴量と照明特徴量の対応関係を解析するため、本研究では Random Forest における特徴量重要度（feature importance） を用いた。

Random Forest では、各決定木において分岐に使用された特徴量がどれだけ予測誤差の低減に寄与したかを基に、特徴量重要度が算出される。
この値は、モデルが予測を行う際にどの特徴量を重視しているかを示す指標である。

本研究では、照明特徴量ごとに個別の Random Forest 回帰モデルを学習し、それぞれの出力に対して音響特徴量の重要度を算出した。
これにより、照明の色・明るさ・点滅といった各要素が、どの音響的特徴と強く結びついているかを独立に分析できる。

\subsubsection{使用データ}
解析には、4.1節および4.3節で生成されたCSVデータを使用した。
これらのCSVファイルには、1秒単位で集約された音響特徴量と対応する照明特徴量が含まれている。

\subsubsection{照明特徴量別の解析方法}
「RGB値」「Brightness」「点滅速度」の各照明特徴量に対して、それぞれ独立に解析を行った。

各照明特徴量を目的変数とし、音響特徴量を説明変数として Random Forest 回帰モデルを学習した。
学習後、特徴量重要度を算出し、寄与度の高い上位5特徴量を抽出した。

結果の可視化には円グラフを用い、各特徴量が予測にどの程度寄与しているかを直感的に把握できるようにした。

\subsubsection{音響特徴量と照明色（R成分）の対応関係}
図20は、照明色の R 成分（Average R）を目的変数として学習した Random Forest 回帰モデルにおける、音響特徴量の重要度上位5項目を円グラフで示したものである。
本図は、モデルが赤色成分の強度を予測する際に、どの音響特徴量をどの程度重視しているかを可視化した結果である。

本解析の結果、$\mathrm{RMS}$ が最も高い重要度を示し、次いで $\mathrm{Tonnetz\_6}$、$\mathrm{MFCC\_5}$、$\mathrm{MFCC\_19}$、$\mathrm{Spectral Contrast}$ の順となった。

\begin{figure}[H]
	\centering
	\includegraphics[width=\hsize]{../fig/rlightstyle.png}
	\caption{照明色 (R成分) に対する音響特徴量}
	\label{fig:rlightstyle}
\end{figure}

RMS（Root Mean Square）は音響信号のエネルギー量、すなわち音量を表す特徴量である。
本結果において RMS が最も高い寄与を示したことは、音量の大きさが照明の赤成分の強度に強く反映されていることを示している。

2番目に高い重要度を示した $\mathrm{Tonnetz\_6}$ は、音楽の和声構造や調性感を表す特徴量である。
これが R 成分の予測に大きく寄与していることから、単なる音量だけでなく、和声的・音楽的な性質が照明色に反映されていることが示唆される。

また、$\mathrm{MFCC\_5}$ および $\mathrm{MFCC\_19}$ といった MFCC 系特徴量も比較的高い重要度を示している。
MFCCは音色の特徴を表す指標であり、これらの結果は、音色の違いが照明色の変化に影響していることを意味している。

特に中低次および高次の MFCC 成分が同時に重要となっている点から、モデルは音の明るさだけでなく、倍音構造の微妙な違いも考慮して照明色を決定していると考えられる。

Spectral Contrast は、周波数帯域ごとのピークと谷の差を表す特徴量であり、音の鋭さや迫力を反映する指標である。
本結果では重要度は比較的低いものの、上位5特徴量に含まれている。

これは、音の輪郭が明瞭であるかどうかが、赤色成分の強調に一定の影響を与えていることを示している。

\subsubsection{音響特徴量と照明色（G成分）の対応関係}
図21は、照明色の G 成分（Average G）を目的変数として学習した Random Forest 回帰モデルにおける、音響特徴量の重要度上位5項目を示した結果である。
本図は、音響信号のどの要素が緑色成分の強度に影響を与えているかを可視化したものである。

解析の結果、RMS が全体の約86.8\% と極めて高い重要度を示し、他の特徴量（$\mathrm{Tonnetz\_6}$、$\mathrm{Chroma\_7}$、$\mathrm{MFCC\_5}$、$\mathrm{Spectral Contrast}$）はいずれも1〜7\%程度に留まった。

\begin{figure}[H]
	\centering
	\includegraphics[width=\hsize]{../fig/glightstyle.png}
	\caption{照明色 (G成分)に対する音響特徴量}
	\label{fig:glightstyle}
\end{figure}

G成分において RMS が圧倒的な重要度を示したことは、音量が緑色照明の強度をほぼ支配的に決定していることを意味している。

この結果は、G成分が他の色成分に比べて、音楽的な細かなニュアンスよりも音圧の変化を直接反映する成分として扱われていることを示している。

RMS に次いで $\mathrm{Tonnetz\_6}$（約6.5\%）が高かった。
この結果は、緑色成分においても、音量以外に和声的要素が一定の影響を及ぼしていることを示唆している。

また、$\mathrm{Chroma\_7}$ や $\mathrm{MFCC\_5}$ はそれぞれ音高クラス分布および音色に関連する特徴量であり、これらが上位に含まれている点は、緑成分が完全に単純な音量依存ではないことを示している。
ただし、これらの寄与率は小さく、あくまで補助的な要素であると解釈できる。

R成分の解析結果では、RMS に加えて Tonnetz や MFCC が比較的高い割合を占めていたのに対し、G成分では RMS の寄与が著しく大きくなっている。
この違いは、照明色ごとに音響特徴量との対応関係が異なることを示している。

\subsubsection{音響特徴量と照明色（B成分）の対応関係}
図22は、照明色の B 成分（Average B）を目的変数として学習した Random Forest 回帰モデルにおける、音響特徴量の重要度上位5項目を示した結果である。
本図は、音響信号のどの要素が青色照明の強度に影響を与えているかを示している。

解析の結果、RMS が約70.2\% と最も高い重要度を示した。
次いで  $\mathrm{MFCC\_19}$（約14.9\%）が比較的高い寄与を示し、 $\mathrm{Chroma\_7}$、$\mathrm{Tonnetz\_6}$、$\mathrm{MFCC\_5}$ はそれぞれ約5\%前後であった。

\begin{figure}[H]
	\centering
	\includegraphics[width=\hsize]{../fig/blightstyle.png}
	\caption{照明色 (B成分)に対する音響特徴量}
	\label{fig:blightstyle}
\end{figure}

B成分においても RMS が最も支配的な特徴量であることから、青色照明も音量変化に大きく依存していることが分かる。
音量が大きい場面では青色成分が強調され、音量が小さい場面では抑制される傾向をモデルが学習していると考えられる。

ただし、G成分と比較すると RMS の割合はやや低下しており、B成分では音量以外の要素も一定程度考慮されていることが示唆される。

$\mathrm{MFCC\_19}$ が約15\%と比較的高い重要度を示した点は、B成分が音色や高次スペクトル成分に影響されやすいことを示している。

$\mathrm{Chroma\_7}$ および $\mathrm{Tonnetz\_6}$ が上位に含まれていることから、B成分は音量や音色だけでなく、音楽的構造（音高分布や和声的関係）とも関連していることが分かる。

特に Tonnetz が青色成分に影響している点は、静的・緊張感のある和声や、落ち着いた調性が青系照明として表現されやすい可能性を示唆している。

これらの結果から、RGB 各成分は一様に音量に反応するのではなく、それぞれ異なる音響的役割を担っていることが明らかとなった。

\subsubsection{音響特徴量と照明の明るさの対応関係}
図23は、照明の明るさ（Background Brightness）を目的変数として学習した Random Forest 回帰モデルにおける、音響特徴量の重要度上位5項目を示した結果である。
本図は、音響信号のどの要素が照明全体の明るさに影響を与えているかを明らかにするものである。

解析の結果、RMS が約77.8\% と最も高い重要度を示した。次いで $\mathrm{Tonnetz\_6}$（約10.4\%）、$\mathrm{MFCC\_19}$（約5.3\%）、$\mathrm{MFCC\_5}$（約3.7\%）、$\mathrm{Spectral Contrast}$（約2.8\%）が続いた。

\begin{figure}[H]
	\centering
	\includegraphics[width=\hsize]{../fig/brilightstyle.png}
	\caption{照明の明るさに対する音響特徴量}
	\label{fig:brightnessstyle}
\end{figure}

RMS が約8割近い寄与率を示したことから、照明の明るさは主に音量に基づいて決定されていることが明らかとなった。

本モデルでは、演奏音が強くなるにつれて照明が明るくなり、弱くなるにつれて暗くなるという対応関係が学習されている。

RMS に次いで $\mathrm{Tonnetz\_6}$（約10.4\%）の重要度が高かった。
この結果は、照明の明るさが単純な音量変化だけでなく、音楽的な構造要素にも影響を受けていることを示している。

また、$\mathrm{MFCC\_19}$ および $\mathrm{MFCC\_5}$ が上位に含まれていることから、照明の明るさは音量だけでなく、音の質感や倍音構成にも影響されていることが分かる。

RGB 各成分と照明の明るさの関係を比較すると、明るさは RGB 成分以上に RMS への依存度が高いことが分かる。
これは、明るさが照明全体の基盤的なパラメータであり、色成分よりも直接的に音量変化を反映する役割を担っているためである。

一方で、Tonnetz や MFCC が一定の寄与を示していることから、明るさも完全に単純な制御ではなく、音楽的文脈を考慮した調整が行われていると解釈できる。

\subsubsection{音響特徴量と照明の点滅速度の対応関係}
図24は、照明の点滅速度（Blinking Speed）を目的変数として学習した Random Forest 回帰モデルにおける、音響特徴量の重要度上位5項目を示した結果である。
本図は、音響信号のどの要素が照明の時間的変化、すなわち点滅の速さに影響を与えているかを可視化したものである。

解析の結果、$\mathrm{Chroma\_7}$ が約30.6\% と最も高い重要度を示し、次いで $\mathrm{MFCC\_5}$（約21.9\%）、$\mathrm{Chroma\_1}$（約19.7\%）、$\mathrm{MFCC\_1}$（約15.6\%）、RMS（約12.\%）という順となった。

\begin{figure}[H]
	\centering
	\includegraphics[width=\hsize]{../fig/blinklightstyle.png}
	\caption{照明の点滅速度に対する音響特徴量}
	\label{fig:blinkstyle}
\end{figure}

点滅速度において最も大きな影響を与えているのは Chroma 系特徴量であり、特に $\mathrm{Chroma}_{7}$ および $\mathrm{Chroma}_{1}$ が全体の約50\%を占めている。
この結果は、照明の点滅が音量の大小よりも、音楽の和声的・音高的構造に強く依存していることを示している。

Chroma の変化が大きい場面では、照明の点滅が速くなり、音楽的な動きや緊張感を視覚的に強調する演出が行われていると考えられる。

$\mathrm{MFCC\_5}$ および $\mathrm{MFCC\_1}$ も比較的高い重要度を示しており、点滅速度が音色の変化にも影響を受けていることが分かる。

RMS は点滅速度においても一定の影響を持つものの、その重要度は約12.1\%に留まっている。
これは、照明の明るさとは異なり、点滅という時間的変化の制御においては、音量そのものよりも音楽的構造や音高変化が重視されていることを示唆している。

RGB 各成分および照明の明るさでは RMS の重要度が支配的であったのに対し、点滅速度では Chroma や MFCC といった音楽的特徴量が主導的な役割を果たしている。
この違いは、照明パラメータごとに参照される音響情報の種類が異なることを明確に示している。


\section{実演}
本章では、本研究で構築したリアルタイム照明制御システムを用いて実施した実演について述べる。
本研究では、システムの動作確認および表現の検証を目的として、異なる楽曲および異なる場所において複数回の実演を行った。
実演は、「大学教室における検証的実演」、「新町商店街における公開実演」の順で実施している。

\subsection{実演の全体構成と目的}
本研究における実演は、以下の二つの目的を持って段階的に実施した。
\begin{enumerate}
	\item[(1)]システムが安定して動作するかを確認する検証的実演
\end{enumerate}

\begin{enumerate}
	\item[(2)]実際の観客を想定した環境における表現力の確認
\end{enumerate}

\subsection{第1回実演：君に乗せて(検証的実演)}

\subsubsection{実演環境}
最初の実演は、福知山公立大学の3201教室において実施した。
教室内は外光や環境音の影響が比較的少なく、安定した条件で実験を行うことが可能である。
そのため、音響入力、運弓動作取得、照明制御が正しく連動しているかを確認する環境として適している。

室内には演奏者を中央に配置し、その周囲四方向にDMX対応照明機器を設置した。
照明は床方向から天井に向けて照射される構成とし、四方から演奏者を照らすような光環境を作り出した。
システムは以下の機器構成で運用した(表14)。

\begin{table}[H]
\centering
\small
\caption{実演機材構成}
\begin{tabular}{c}
\hline
\hline
ヴァイオリン(演奏者：筆者)　\\
PC(Windows 11)　\\
コンデンサマイク(音声入力)  \\
スマートフォン(ZIG-SIM用) \\
USB-DMXインタフェース  \\
DMX対応照明機器4台  \\
\hline
\end{tabular}
\end{table}

また、実演における各機材の配置図(上面図)は図4の通りである。

\subsubsection{実演曲目}

実演に使用した楽曲は、『君に乗せて\cite{kimi}』(作曲：久石譲)である。
本曲はアニメ『天空の城ラピュタ』の主題歌として広く知られており、旋律が明瞭でテンポ変化が比較的少ない楽曲である。
ヴァイオリンソロアレンジ版を用い、演奏時間は約2分30秒である。
演奏はAメロからサビまでをカバーしている。
図25に実演で使用した楽譜を示す。

\subsubsection{実演内容と観察結果}
この実演では、来場者を想定せず、研究者自身による検証を目的として演奏を行った(図26,27)。
図26,27の通り、演奏音の入力に応じて照明の色、明るさおよび点滅速度が変化すること、運弓方向に応じて照明の上下動作が発生することを確認した。

\begin{figure}[H]
	\centering
	\includegraphics[width=\hsize]{../fig/kimiscore.png}
	\caption{実演曲目『君に乗せて』楽譜}
	\label{fig:kimi}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\hsize]{../fig/kimi2.png}
	\caption{『君に乗せて』実演中の照明変化の様子(Aメロ～Bメロ)}
	\label{fig:kimi2}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\hsize]{../fig/kimi3.png}
	\caption{『君に乗せて』実演中の照明変化の様子(サビ)}
	\label{fig:kimi3}
\end{figure}

\subsection{第2回実演：紅蓮華（公開実演）}

\subsubsection{実演環境}
次の実演は、京都府福知山市の新町商店街にある「Tsunaga Room\cite{tsunagaroom}」にて実施した。
この実演は、一般来場者を含む環境で行われた公開実演である。
また、使用機材の配置や照射方法は5.2.1節と同様である。

\subsubsection{実演曲目}

実演に使用した楽曲は、『紅蓮華\cite{utasonglisa}』（作曲：草野華余子）である。
本曲はアニメ『鬼滅の刃』のオープニングテーマとして広く知られており、力強いリズムと感情豊かなメロディが特徴である。
ヴァイオリンソロアレンジ版を用い、演奏時間は約1分30秒である。
演奏はイントロから1番のサビまでをカバーしている。
図28に実演で使用した楽譜を示す。

\begin{figure}[H]
	\centering
	\includegraphics[width=\hsize]{../fig/score.png}
	\caption{実演曲目『紅蓮華』楽譜}
	\label{fig:gurennge1}
\end{figure}

\subsubsection{実演の進行方法}
第2回実演では、来場者に対して本研究の趣旨および実演内容について簡単な説明を行った上で、ヴァイオリン演奏と照明演出の実演を実施した。

実演開始前に、演奏者（筆者）から来場者に対して、以下の内容を口頭で説明した。

まず、本実演ではヴァイオリン演奏の音そのものが照明を制御していることを説明した。
具体的には、演奏音の音量や音色、響き方といった要素がリアルタイムに解析され、それに基づいて照明の色や明るさ、点滅が自動的に変化する仕組みであることを伝えた。
また、照明は事前に演出をプログラムしているものではなく、演奏内容に応じて毎回異なる変化が生じることを説明した。

次に、音響情報に加えて、演奏者の運弓動作が照明の動きに反映されていることを説明した。演奏者の腕の動きをスマートフォンで取得し、弓を上げる動作と下げる動作に応じて照明が上下に動く仕組みであることを示した。この説明により、来場者が演奏中の身体動作と照明の動きを対応づけて観察できるよう配慮した。

説明後、楽曲『紅蓮華』の実演を開始した。

\subsubsection{実演中の照明変化の様子}
図29～図32に、実演中の照明変化の様子を示す。これらの画像は、前述した実演環境にて、ヴァイオリン演奏曲『紅蓮華』を演奏した際の照明挙動を記録したものである。

\begin{enumerate}
	\item[\textbf{(1)}]\textbf{導入部（イントロ）における低音量区間の照明制御}
\end{enumerate}

演奏開始直後のイントロ部分（図29）では、演奏音のRMS値が全体を通して低い水準で推移しており、音量変動も小さい区間である。
このため、機械学習モデルの出力として得られるBrightness値は低く抑えられ、照明は全体的に暗い状態が維持されている。

色成分については、RGBのうちB成分が相対的に高く、RおよびG成分は低い値に制御されている。
その結果、照明は青色系を基調とした色調となり、急激な色相変化は発生していない。
このような制御により、楽曲の導入部に適した静的かつ安定した視覚環境が形成されている。

\begin{enumerate}
	\item[\textbf{(2)}]\textbf{中盤部における音量増加に伴う色相・輝度の遷移}
\end{enumerate}

演奏が進行し、中盤部分（図30）に入ると、RMS値の平均および最大値が徐々に増加し、それに伴ってBrightnessも段階的に上昇していることが確認できる。

また、音色の変化に対応してRGB成分にも変化が生じており、特にR成分の増加が顕著である。その結果、照明の色調は青系から赤色を含む暖色系へと移行している。これにより、演奏者の身体およびヴァイオリンが赤系の光によって強調され、楽曲中盤における緊張感やエネルギーの高まりが視覚的に表現されている。

\begin{enumerate}
	\item[\textbf{(3)}]\textbf{サビ部分における高音量・高変動区間の動的照明制御}
\end{enumerate}

さらに、楽曲がサビ部分に近づくにつれて（図31、図32）、RMS値は高い値を維持するとともに、短時間内での変動幅も大きくなっている。このような音響的特徴を受けて、モデルはBrightnessを高水準に設定され、照明は高輝度状態で制御されている。

加えて、音響特徴量の変動に応じてRGB成分が時間的に大きく変化しており、紫色や黄色など複数の色がフレーム単位で切り替わる挙動が確認できる。これにより、照明は静的な演出から動的な演出へと移行し、楽曲の盛り上がりに同期した視覚的な変化が強調されている。その結果、演奏音と照明演出が高い時間的整合性をもって連動し、没入感の高い演奏空間が実現されている。

\begin{figure}[H]
	\centering
	\includegraphics[width=0.3\textwidth]{../fig/intro.png}
	\caption{『紅蓮華』実演中の照明変化の様子（イントロ部分）}
	\label{fig:intro}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.3\textwidth]{../fig/abc.png}
	\caption{『紅蓮華』実演中の照明変化の様子（Aメロ～Cメロ）}
	\label{fig:chorus}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.3\textwidth]{../fig/savi.png}
	\caption{『紅蓮華』実演中の照明変化の様子（サビ部分）}
	\label{fig:savi}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.3\textwidth]{../fig/savi2.png}
	\caption{『紅蓮華』実演中の照明変化の様子（サビ部分）}
	\label{fig:savi2}
\end{figure}

\section{考察}

前章では、音響特徴量に基づく照明制御の挙動を、楽曲構造ごとに示した。本節では、それらの結果を基に、本システムの制御特性を設計意図との対応関係および実演環境における実用性の観点から詳細に考察する。

\subsection{意図通りに制御できた点}
本実験結果から、音響特徴量に基づく照明制御が、楽曲構造および演奏表現と整合性をもって機能していることが確認できた。

まず、演奏導入部においては、RMS値が低い区間でBrightnessが低く制御され、青色系を基調とした落ち着いた照明表現が生成されていた。これは、音量が小さい区間では視覚的刺激を抑制するという意図と一致しており、楽曲の導入部として適切な雰囲気を形成できている。

次に、中盤部では、音量の増加に伴ってBrightnessが段階的に上昇し、同時にRGB成分のうちR成分が強調されることで、暖色系の照明へと遷移していた。
このような色調および輝度の連続的変化は、音響特徴量の変化を滑らかに視覚表現へ反映するという本システムの設計方針を満たしている。

さらに本システムの特徴として、運弓動作に応じて照明位置が上下方向に変化する挙動もしっかり確認された。
実演では、上げ弓となる区間で照明が上方へ移動し、下げ弓となるの区間では下方に留まる傾向が確認された。

サビ部分では、音量および音響特徴量の時間的変動が大きくなるのに対応して、高輝度かつ動的な色変化が発生していた。複数色が時間的に切り替わる照明演出は、楽曲の盛り上がりやエネルギー感を強調する効果があり、演奏音と照明演出がリアルタイムに同期していることを示している。以上より、音響情報に基づく照明制御は、意図した方向性で機能していると評価できる。

\subsection{実演において明らかとなった課題}

一方で、実演結果からはいくつかの課題も確認された。
\subsubsection{照明制御における課題}
第一に、サビ部分など音響特徴量の変動が急激な区間では、RGB成分が短時間で大きく変化するため、照明の色変化がやや過度に感じられる場面が見られた。
これは、音響特徴量の瞬時値を直接照明制御に反映していることが要因であり、時間方向の平滑化処理が不十分である可能性が示唆される。
また、この影響により照明の点滅速度が聴感上の自然さと一致せず、過度に速い、あるいは不規則に感じられる場合があった。

第二に、照明制御が主に音量（RMS値）に強く依存しているため、音量変化が小さいが音楽的に重要なフレーズ（抑揚や表情付け）に対して、照明変化が十分に反映されない場合があった。この点は、音量以外の音響特徴量（スペクトル重心や倍音構造など）の寄与が限定的であることに起因していると考えられる。

第三に、演奏者や楽器への照射は強調されているものの、空間全体としての照明バランスや観客視点での視認性については、さらなる検討の余地がある。

\subsubsection{演奏動作取得における課題}
ZIG-SIMを用いた運弓方向推定は概ね有効であったが、演奏中の細かな動作や急激な動きに対して、誤判定が生じる可能性も確認された。特に、弓の動きが小さい場合や、角速度が閾値付近にある場合には、上下動作が不安定になることがあった。

この課題に対しては、センサ情報の平滑化や、複数軸情報を用いた判定など、より安定した動作推定手法の導入が必要である。

\subsection{今後への示唆}
以上の考察から、本研究は演奏者主体の照明制御という新しい表現手法の可能性を示す一方で、複数特徴量を統合的に扱う制御手法の導入、動作推定精度などといった点に改善の余地があることが明らかとなった。

\subsection{本研究における実演システム構築の意義}
本研究においてヴァイオリン演奏に合わせて照明が自動制御されるシステムを構築し、実際に実演まで行ったことの意義を、研究的観点、実践的観点、の二つの側面から明確に位置付ける。

\subsubsection{研究的観点からの意義}
研究的観点において、本研究の意義は、音響特徴量に基づく照明制御を単なるシミュレーションやオフライン解析に留めず、リアルタイムかつ実演環境で検証した点にある。

これにより、音響特徴量の変動が照明挙動にどのような影響を与えるのかを、実演という制約条件下で観察することが可能となった。

\subsubsection{実践的観点からの意義}
実践的観点では、本研究は演奏者が事前に照明操作を意識することなく、演奏に集中できる環境を実現した点に意義がある。照明制御を自動化することで、演奏中に照明オペレータを必要とせず、単独演奏や小規模な演奏環境においても視覚演出を付加できる可能性を示した。

\section{結び}
本研究では、ヴァイオリン演奏に基づいて照明をリアルタイムに制御するインタラクティブ照明演出システムの構築を目的とし、音響特徴量および演奏動作情報を用いた照明制御手法を提案・実装・実演を通して検証した。

まず研究背景として、従来の舞台照明が事前に設計された演出や、音量に単純に反応する仕組みに依存している点に着目し、演奏者の表現そのものが直接照明に反映される仕組みの必要性を示した。
特に本研究では、筆者自身がヴァイオリン演奏を行うことができるという特性を活かし、「演奏しながら照明を動かす」という発想を出発点として研究を進めた。

提案手法では、ヴァイオリン演奏音から抽出した多次元の音響特徴量を入力として、照明の色（RGB）、明るさ、点滅といった複数の照明パラメータを同時に予測する機械学習モデルを構築した。
また、ZIG-SIMを用いて演奏者の運弓動作を取得し、照明の物理的な上下動作に反映させることで、音響情報だけでは表現しきれない身体的表現を照明演出に取り込んだ。

照明予測モデルの構築においては、ライブ映像およびMIDI音源を用いたデータ生成手法を採用し、Random Forest回帰モデルによって音響特徴量と照明特徴量の関係を学習した。
これにより、実演においても安定して照明パラメータを生成できるモデルを構築することができた。

実演では、新町商店街（福知山市）の屋内会場において楽曲『紅蓮華』を演奏し、より実環境に近い条件下で照明演出の表現力およびリアルタイム性を検証した。
これらの実演を通して、演奏音および演奏動作に応じて照明が自律的に変化し、演奏と照明が同期する演出が実現できることを確認した。

以上より、本研究は、音楽演奏と照明演出をリアルタイムに結びつける一つの実践的手法を提示し、演奏者の表現を視覚的に拡張する可能性を示した点に意義があると結論づけられる。


\textbf{謝辞} 本論文を執筆するにあたり、多くの方々からご指導・ご支援を賜りました。
ここに深く感謝の意を表します。
まず、本研究を進めるにあたり、研究の方向性から終始丁寧なご指導と貴重なご助言をくださった橋田光代准教授に心より御礼申し上げます。
また、研究室の4年生の皆様には、日頃より温かく接していただき、研究の相談にも快く応じていただきました。
皆様との和やかな会話は、研究生活の大きな支えとなりました。
心より感謝いたします。
最後に、本研究に関わってくださったすべての方々に、厚く御礼申し上げます。

% !TEX root = _main.tex
% ========================================
% 卒業論文　本文
% ========================================

\section{はじめに}

舞台演出やライブパフォーマンスにおいて、照明は音楽表現を視覚的に拡張し、観客の没入感を高める重要な要素である。
照明の色や明暗、動きは演奏の意図や感情を強調し、音響のみでは伝わりにくい表現を補完する役割を果たす。
しかし、従来の舞台照明は照明オペレーターによる手動操作に大きく依存しており、演奏内容に即応した柔軟な制御が難しいという課題がある。
特に、小規模な演奏環境や個人演奏においては、専任の照明オペレーターを配置することが困難であり、演奏者自身の表現意図を照明に反映させる手段が限られている。

このような背景から、演奏そのものを入力として照明を自動制御する仕組みが求められている。
音楽演奏には音響的特徴だけでなく、身体動作に基づく表現も含まれており、これらを照明制御に統合することで、より一体感のある演出が実現できると考えられる。

そこで本研究では、ヴァイオリン演奏に着目し、音響信号と身体動作を組み合わせたリアルタイム照明制御システムの構築を目的とする。
具体的には、マイクから入力されたヴァイオリン音響信号から、スペクトル重心、スペクトル帯域幅、MFCC、ZCR などの音響特徴量を抽出し、機械学習モデルを用いて照明の RGB 値、明るさ、点滅速度といった照明特徴量を予測する。
また、スマートフォンのモーションセンシングアプリ ZIG-SIM\cite{1-10incZIGProject} を用いて、演奏者のヴァイオリン演奏における弓の上下動作を角速度として取得し、照明の上下動作に直接反映させる。

本研究の着想は、筆者自身の「自分のヴァイオリン演奏だけで照明を直接動かしてみたい」という個人的関心に基づいている。
ヴァイオリンを対象とした理由として、筆者自身が演奏経験を有しており、演奏者の立場からシステムの反応性や表現性を評価できる点が挙げられる。

以上より本研究では、ヴァイオリン演奏音から抽出した音響特徴量に基づいて照明の色、明るさ、点滅速度を制御すると同時に、演奏者の運弓動作を照明の上下動作として直接反映することで、音楽表現と身体表現が一体となった照明演出の実現を目指す。

また、本論文の構成は以下のとおりである。

第\ref{sec:Related Research}章では関連研究を整理し、本研究の位置づけを明確にする。
第\ref{sec:system}章では、ヴァイオリン演奏に基づくリアルタイム照明制御システムの実装について述べる。
第\ref{sec:model}章では、音響特徴量から照明特徴量を推定する照明予測モデルの構築および評価を行う。
第\ref{sec:play}章では実演内容と結果を示し、第\ref{sec:think}章で考察を行う。
最後に第\ref{sec:result}章で本研究を総括する。


\section{関連研究} \label{sec:Related Research}

音楽ライブやコンサートにおける照明演出は、観客の没入感や一体感を高める重要な要素であり、従来は照明デザイナーやオペレータの経験や感性に基づいて設計されてきた。
しかし、小規模公演や即興性の高い演奏では十分な準備時間や人員を確保することが難しく、音楽情報処理技術を用いた照明演出の自動化が有効な手段として注目されている。

\subsection{音響信号に基づく照明演出の自動化}

音響信号に基づく照明制御に関する研究として、月東らは楽曲音源からリズムの強調箇所を検出し、それを照明演出へ反映する手法を提案している\cite{1}。
この研究では、楽曲をドラム、ベース、ギター、ボーカルの各パートに音源分離し、それぞれのオンセットエンベロープを算出した上で、複数パートで同時に強いオンセットが観測されるタイミングを「キメ」と定義している。
検出されたキメを基点として、照明のフラッシュや明滅といった瞬間的な演出を付与し、主観評価実験によりリズムの一体感や迫力の向上を確認している。

この手法は高い時間解像度でリズム的アクセントを捉えられる点に特徴がある一方で、照明制御はキメの有無に依存した離散的なものに留まっており、演奏中の表現変化を連続的に反映することは難しい。
また、演奏者の身体的動作は入力として扱われていない。

\subsection{楽曲の印象・意味内容に基づく照明生成}

楽曲全体の印象や意味内容を照明演出に反映する研究も多く報告されている。
神野らは、歌詞および曲調の印象に基づいて照明演出を自動生成するシステムを提案している\cite{2}。

この研究では、歌詞を形態素解析した上で分散表現ベクトルに変換し、言語イメージスケールを用いて照明色候補を選択する。
さらに、楽曲の長短調やテンポに基づいて色の優先度や配置を決定し、BPMや歌詞のネガティブ・ポジティブ度を用いて明度を調整することで、照明の色、配置、明るさを総合的に決定している。
この手法により、照明デザイナーの感性に依存しない一貫性のある演出が実現されている。

一方で、照明変化は小節やフレーズ単位といった比較的長い時間スケールで行われるため、演奏中の瞬間的な表現変化や演奏者の身体動作を直接反映することは想定されていない。

\subsection{深層学習を用いた照明演出生成}

近年では、照明演出を生成タスクとして捉え、深層学習モデルを用いて照明パラメータを自動生成する研究も行われている。
月東らの研究内でも言及されている Zhao らの手法では、楽曲特徴量系列を入力として照明の色や明るさの時系列データを生成する生成モデルが提案されている\cite{5}。

このようなアプローチは、表現力や多様性の高い照明演出を実現できる可能性を示しているが、学習データへの依存度が高く、生成過程の解釈が困難であるという課題がある。
また、演奏者の身体動作は入力として扱われていない。

\subsection{演奏者操作によるリアルタイム照明制御}

演奏者自身が照明演出に直接関与する研究として、瀧口らはドラム音やMIDIフットコントローラーを用いて演奏中に照明を操作可能なシステムを提案している\cite{3}。
この研究では、バスドラム音の音量が閾値を超えた際に照明を点灯させる手法や、足元のコントローラー操作によって照明の色や輝度、ストロボ効果を変更する仕組みが実装されている。

この手法により、即興演奏に柔軟に対応できる照明演出が実現されている一方で、演奏者が照明操作を意識する必要があり、演奏負荷が増加する可能性がある。
また、演奏動作そのものを自動的に取得・利用する仕組みは含まれていない。

\subsection{身体動作と照明演出の連動}

演奏者の身体動作と照明演出を連動させる研究として、浅田らはオンラインライブ空間において、演奏者の身体情報を用いて照明や振動装置を制御するシステムを提案している\cite{4}。
この研究では、ギター演奏者の腕の角度に応じてムービングライトの向きを変化させるなど、身体動作を照明の物理的な動きに対応付けることで、演奏者と観客の一体感を高めている。

しかし、音響特徴量と身体動作を統合的に扱う設計にはなっておらず、音楽表現と照明表現の対応関係は限定的である。

\subsection{本研究の位置づけ}

以上の関連研究から、音響特徴量に基づく照明制御、楽曲印象に基づく照明生成、演奏者操作型および身体動作連動型の照明演出はそれぞれ発展してきたものの、音響情報と演奏者の身体動作を同時に統合し、連続的かつリアルタイムに照明を制御する研究は十分に行われていないことが分かる。

これを踏まえ、本研究ではヴァイオリン演奏音の特徴量と演奏者の運弓動作に応じてステージ上の照明がリアルタイムに変化する照明制御システムを提案する。

\subsection{プロトタイプの実装と課題}

システムの実装に先立ち、システムの方向性を検討するため、簡易的なプロトタイプシステムの実装を行った。
このプロトタイプは、ヴァイオリンを演奏しながら、足元に配置した 3 つのフットペダルを用いて演奏者の周りに配置している照明機材の RGB 成分の切り替えを制御し、かつ音量に応じて照明の光量を制御するシステムである。

このプロトタイプを実装した理由は、演奏者自身が実際にヴァイオリンを演奏しながら照明を動かすという体験を試行的に行い、演奏と照明演出が同時に存在する状況を身体感覚として把握するためである。

そして、このプロトタイプを用いた試行を通して、いくつかの課題が明らかとなった。

第1に、照明の色変化がペダル操作に対応した離散的な切り替えに限定されているため、演奏中の音色の変化などといった連続的な表現を十分に反映されていないことに違和感を感じた点である。

第2に、演奏中に照明操作を行う必要があるため、演奏と操作の両立が難しく、演奏者に身体的・認知的負荷が生じる点である。

第3に、演奏と照明の間に起因する遅延により、演奏と照明の同期が主観的にずれて感じられる場面が見られた。

これらの課題を踏まえ、本システムでは以下の設計方針を定めた。

\begin{enumerate}
	\item[\textbf{(1)}]\textbf{演奏表現を連続的に反映可能な照明制御を実現すること}
\end{enumerate}

本研究では、音響特徴量と照明特徴量との対応関係を学習する照明予測モデルをシステムに導入した。

これにより、単純な閾値処理やルールベースによる制御では実現が難しい、演奏表現に応じた滑らかな照明変化を自動的に生成できると考えられる。

また、演奏者が照明操作を意識することなく、演奏内容そのものが照明に反映される構成を実現できる点も、照明予測モデルを採用した理由の1つである。

\begin{enumerate}
	\item[\textbf{(2)}]\textbf{演奏中に人手操作を必要としないこと}
\end{enumerate}

本システムでは、演奏者が演奏中に照明操作を意識したり、追加の入力デバイスを操作したりする必要がない自律的な構成を目指した。

これは、プロトタイプにおいて、演奏と照明操作を同時に行うことが大きな負荷となることが確認されたためである。

本研究では、演奏音および演奏動作そのものを入力として照明を自動制御する設計とした。

\begin{enumerate}
	\item[\textbf{(3)}]\textbf{遅延を極力抑え、リアルタイム性を確保すること}
\end{enumerate}

本システムでは、ヴァイオリン演奏の微細な表現変化を照明に即座に反映させるため、システム全体の処理遅延を可能な限り低減する設計とした。
プロトタイプでは、演奏と照明の同期にずれが生じる場面が確認された。

この課題を踏まえ、本研究では人間の操作を介さずに制御を行うとともに、演奏と照明演出の間の時間のずれを約 10ms 以内に収めるように設定した。
一般に、人間は 20～30ms 未満の視覚的遅延を明確には知覚しにくいとされており\cite{MenXieShenTiGanJuetoShiJueQingBaonizuregaShengziruMeiRuHuanJingniokeruDiChiYannaYingXiangnoyuzahenoYingXiang2019}、10ms 程度の更新周期であれば、演奏と照明がほぼ同時に変化しているように感じられると考えられる。
この知見に基づき、演奏と照明の一体感を損なわない更新周期として 10ms を採用した。

\section{リアルタイム照明制御システム} \label{sec:system}

本章では、本研究で提案したヴァイオリン演奏に基づくリアルタイム照明制御手法を実現するために構築したシステム構成(\figref{fig:system-structure})について詳述する。
本システムは、ヴァイオリン演奏に伴う音響特徴量および演奏動作情報を入力とし、機械学習モデルによって推定された照明特徴量をDMX信号として出力することで、演奏に同期した照明演出を自動生成するものである。

\begin{figure}[tb]
	\centering
	\includegraphics[width=\hsize]{../fig/system-structure.png}
	\caption{リアルタイム照明制御システム処理フロー図}
	\label{fig:system-structure}
\end{figure}

\subsection{ハードウェア構成}

本システムは\tabref{tab:hardwea}に記載されているハードウェアで構成される。

\begin{table}[tb]
\centering
\small
\caption{ハードウェア構成}
\label{tab:hardwea}
\begin{tabular}{c}
\hline
\hline
PC(Windows 11)　\\
コンデンサマイク(演奏音入力)  \\
スマートフォン(演奏動作取得) \\
USB-DMXインタフェース(\figref{fig:dmx-light})  \\
DMX対応照明機器4台(\figref{fig:dmx-light})  \\
\hline
\end{tabular}
\end{table}

\begin{figure}[tb]
	\centering
	\includegraphics[width=\hsize]{../fig/dmx-light.png}
	\caption{USB-DMXインタフェース(左)とDMX対応照明機器(右)}
	\label{fig:dmx-light}
\end{figure}

マイクは演奏音をリアルタイムで取得するために使用し、スマートフォンはヴァイオリン演奏時の運弓動作を取得するために使用する。
照明機器は地面にステージ四方に配置され、床から天井に向けて演奏者を中心に照射する構成とした(\figref{fig:setting})。
\figref{fig:setting}中のそれぞれの配線はケーブルを用いたものを実線で示し、無線通信は点線で示している。

\begin{figure}[tb]
	\centering
	\includegraphics[width=\hsize]{../fig/setting.png}
	\caption{機材配置図(上面図)}
	\label{fig:setting}
\end{figure}

\subsection{ソフトウェア構成}

本システムはPython\cite{WelcomePythonorg2025}で実装され、すべての処理を単一プログラム上で統合している。
使用した主なライブラリを\tabref{tab:library}に示す。

\begin{table}[tb]
\centering
\small
\caption{使用ライブラリと役割}
\label{tab:library}
\begin{tabular}{c|c}
\hline
\hline
ライブラリ名 & 役割  \\
\hline
sounddevice\cite{SounddevicePlayRecord} & 演奏音取得 \\
librosa\cite{mcfeeLibrosaAudioMusic2015} & 音響特徴量抽出  \\
numpy\cite{NumPy} & 数値処理、データ管理 \\
scikit-learn\cite{ScikitlearnMachineLearning} & 機械学習モデル推論  \\
socket\cite{SocketDiShuiZhunnetutowakuintahuesu} & UDP通信  \\
ftd2xx\cite{Ftd2xxPythonInterface} & DMX信号出力 \\
\hline
\end{tabular}
\end{table}

\subsection{演奏音入力および音響特徴量抽出} \label{sec:sound}

まず、ヴァイオリンの演奏音は Python の sounddevice ライブラリを用いて取得する。
音声入力には InputStream を使用し、サンプリング周波数を 22,050 Hz、モノラル入力として設定した。
演奏音は少量のサンプルごとに順次取得され、そのたびにコールバック関数が呼び出される。

取得された音声データはリングバッファに順番に格納される。
この方式により、音声データの欠落を防ぎつつ、連続した音声を安定して扱うことができる。
バッファ内に 22,050 サンプルが蓄積されると、それを 1 秒分の音声データとしてまとめ、以降の処理に用いる。

次に、切り出された 1 秒分の音声フレームに対して、librosa ライブラリを用いて音響特徴量の抽出を行う。
本研究では、ヴァイオリン演奏音に含まれる特徴を数値として表現するため、10種類の音響特徴量を算出している。

\begin{enumerate}
	\item[\textbf{(1)}]\textbf{ゼロ交差率}
\end{enumerate}

ゼロ交差率は、音の波形が上下に切り替わる回数を表す特徴量である。
高速に変化する音やノイズ的な音では値が大きくなり、滑らかで安定した音では小さくなる。

例えば、摩擦音のような「シーッ」という音では値が高く、低音で持続的な音では低くなる傾向がある。

\begin{enumerate}
	\item[\textbf{(2)}]\textbf{スペクトル重心(Spectral Centroid)}
\end{enumerate}

スペクトル重心は、音の「明るさ」を表す特徴量である。
高音成分が多く含まれる音ほど値が大きくなり、低音成分が中心の音では値が小さくなる。

例えば、高い音域で演奏されたヴァイオリンの音ではスペクトル重心は大きくなり、低音域中心の楽器音では小さくなる。
この特徴量により、音の鋭さや軽やかさを捉えることができる。

\begin{enumerate}
	\item[\textbf{(3)}]\textbf{スペクトル帯域幅(SpectralBandwidth)}
\end{enumerate}

スペクトル帯域幅は、音に含まれる周波数成分の「広がり」を表す特徴量である。
さまざまな高さの音が同時に含まれている場合は値が大きくなり、単一の音高が明確な場合は値が小さくなる。

和音やノイズ成分を多く含む音では帯域幅が広くなり、単音でクリアな音では狭くなる傾向がある。

\begin{enumerate}
	\item[\textbf{(4)}]\textbf{スペクトルコントラスト(SpectralContrast)}
\end{enumerate}

スペクトルコントラストは、音の中の「強い成分」と「弱い成分」の差を表す特徴量である。
値が大きい場合はメリハリのある音であり、アクセントがはっきりしていることを示す。
一方、値が小さい場合は全体的に均一で平坦な音である。

この特徴量により、演奏の抑揚や音の輪郭の明瞭さを捉えることができる。

\begin{enumerate}
	\item[\textbf{(5)}]\textbf{スペクトルフラットネス(SpectralFlatness)}
\end{enumerate}

スペクトルフラットネスは、音がどの程度ノイズ的であるかを示す特徴量である。
値が大きい場合はホワイトノイズのようにザラザラした音であり、値が小さい場合は楽器音のように滑らかで音高が明確な音であることを示す。

ヴァイオリン演奏においては、弓の擦れ音が強い場面などで変化が現れる。

\begin{enumerate}
	\item[\textbf{(6)}]\textbf{スペクトルロールオフ（SpectralRolloff）}
\end{enumerate}

スペクトルロールオフは、音に含まれる高音成分がどのあたりまで存在しているかを示す特徴量である。

値が大きい場合は高音成分が多く含まれており、値が小さい場合は低音成分が中心の音であることを意味する。

この特徴量は、音の尖り具合や鋭さを把握するために用いられる。

\begin{enumerate}
	\item[\textbf{(7)}]\textbf{音量(Root Mean Square)}
\end{enumerate}

Root Mean Squareは、音の「大きさ」、すなわち音量を表す特徴量である。
値が大きいほど強い音であり、値が小さいほど弱く静かな音であることを示す。

\begin{enumerate}
	\item[\textbf{(8)}]\textbf{MFCC（メル周波数ケプストラム係数）}
\end{enumerate}

MFCC は、人間の聴覚特性に近い形で音色を数値化した特徴量である。
「人の声っぽい」「金属的」「柔らかい」といった音の質感を捉えることに優れており、複数の数値の組み合わせによって構成されている。

一言で表すと、MFCC は音の「質感」を表す特徴量である。

\begin{enumerate}
	\item[\textbf{(9)}]\textbf{クロマ特徴量（Chroma）}
\end{enumerate}

クロマ特徴量は、12 音階（ド・レ・ミなど）それぞれの音の強さを表す特徴量である。
どの音階がどの程度鳴っているかを示すため、和音や調性感の把握に用いられる。

\begin{enumerate}
	\item[\textbf{(10)}]\textbf{トーナルネットワーク特徴量（Tonnetz）}
\end{enumerate}

Tonnetz は、音階同士の関係性を表す特徴量であり、和音構造や調性の傾向を捉えることができる。
この特徴量により、明るい調（メジャー）か暗い調（マイナー）かといった音楽的な雰囲気を数値的に表現することが可能となる。

\subsection{演奏動作取得}

本システムでは、演奏者の弓の上下運動(上げ弓・下げ弓)(\figref{fig:up}・\figref{fig:down})を取得するために、スマートフォンをZIG-SIMセンサとして利用している。
スマートフォンにはジャイロセンサが搭載されており、X軸方向の角速度を取得することで弓の前後運動を検出する。

スマートフォンは演奏者の右腕に装着し、弓を持つ手首付近(\figref{fig:up}・\figref{fig:down}の青丸部分)に位置させる。
これにより、弓の上下運動が腕の回転として反映されるため、ジャイロセンサで容易に検出可能となる。

右手首に装着したスマートフォンから取得したジャイロデータはUDP通信を用いてPCに送信する。
UDPはTCPに比べて通信遅延が少なく、リアルタイム性を重視した設計である。

\begin{figure}[tb]
    \centering
    \includegraphics[width=\hsize]{../fig/up.png}
    \caption{上げ弓}
    \label{fig:up}
\end{figure}

\begin{figure}[tb]
    \centering
    \includegraphics[width=\hsize]{../fig/down.png}
    \caption{下げ弓}
    \label{fig:down}
\end{figure}

PCに送信されたx軸のジャイロデータをもとに、弓方向の判定を行う。
弓方向の判定にあたっては、\textbf{「角速度の値に基づく判定」}と、\textbf{「角速度変化量に基づく判定」}を併用している。

\subsubsection{各速度の値による運弓方向判定}

閾値を0.01に設定し、ジャイロデータ ＞ 0.01 なら上げ弓、ジャイロデータ ＜ -0.01 なら下げ弓と判定している。

この値は理論的に決定したものではなく、実際の筆者の演奏動作を用いた試行錯誤によって決定した経験的な値である。
具体的には、以下の手順で検討を行った。

\begin{enumerate}
	\item[(2-1)]弓を完全に静止させた状態で数十秒間データを取得
\end{enumerate}

\begin{enumerate}
	\item[(2-2)]弓を非常にゆっくり動かした場合の角速度を計測
\end{enumerate}

\begin{enumerate}
	\item[(2-3)]通常の演奏速度における角速度分布を確認
\end{enumerate}
その結果、
\begin{enumerate}
	\item[・]静止時の角速度はおおむね ±0.005 以下
\end{enumerate}

\begin{enumerate}
	\item[・]意図的な弓の動きは ±0.01 を明確に超える
\end{enumerate}
という傾向が確認された。

したがって、0.01 という値は「ノイズと実際の演奏動作を分離できる最小限の境界値」として妥当であると判断した。

\subsubsection{各速度変化量による運弓方向判定}

角速度変化量とは、「今の角速度が、直前と比べてどれだけ変わったか」を表す値である。
本研究では、連続する2時刻のジャイロX軸角速度の差を用いて、次のように定義している。

\textbf{角速度変化量 ＝ 今回の角速度 − 前回の角速度}

この値が大きい場合、「弓が急に動き始めた」「弓の方向が切り替わった」ことを意味する。
本研究では、次の2つのケースに分類して弓方向を判定している。

\begin{enumerate}
	\item[(3-1)]各速度変化量が正に大きく変化した場合→上げ弓
\end{enumerate}

\begin{enumerate}
	\item[(3-2)]各速度変化量が負に大きく変化した場合→下げ弓
\end{enumerate}

本研究における角速度変化量の判定には、閾値を 0.09 と設定した。
この閾値は、演奏時に取得したジャイロセンサの時系列データを用い、複数の候補値（0.05～0.15）について弓方向判定を行い、実際の演奏動作と比較することで決定した。
筆者自身が一定のテンポおよび強弱で演奏を行い、得られた角速度変化量が弓方向の切り替わり時にのみ大きく変化し、かつ微小な手振れやノイズによる誤検出が最小となる値を評価した。
その結果、0.09 が弓方向の変化を最も安定して検出できる閾値であると判断し、最終的に採用した。

この2つを併用することで、微妙な演奏タッチや小さな振動が含まれる状況においても誤判定を避け、弓の動作をより正確に捉えることが可能となる。
なお、閾値設定の被験者は1名であり、個人差を考慮していないため、一般化には課題が残る。

このように、本システムではジャイロセンサによる演奏動作取得をリアルタイムで行い、音響特徴量と組み合わせて照明制御に反映している。

\subsection{機械学習モデルによる照明特徴量推定} \label{sec:light}

抽出された音響特徴量を用いて、機械学習モデルにより照明特徴量を推定する。
モデルは\textbf{「RGB(色)」}、\textbf{「Brightness(明るさ)」}、\textbf{「点滅速度」}、の3つの照明特徴量を出力する。
推定値は0〜1の正規化値として出力され、DMX出力時に0〜255の整数値へスケーリングされる。
また、機械学習モデルについては次章で詳述する。

\subsection{DMX信号生成と照明制御}

本節では、機械学習モデルおよび演奏動作推定によって得られた照明特徴量を、実際の照明機器に反映させるためのDMX信号生成および制御方法について詳細に述べる。
DMX512は舞台照明分野において広く用いられている通信規格であり、本研究ではこの標準規格を用いることで、実際のステージ環境に近い条件での実演を可能にしている。

\subsubsection{DMX512プロトコルの概要}

DMX512は、最大512チャンネルの制御データを1フレームとして送信する一方向通信プロトコルである。
各チャンネルは0〜255の8bit値を持ち、照明機器の色、明るさ、点滅速度を制御することができる。

また、DMX信号の出力には、USB接続のDMXインタフェースを使用した。
本研究ではFTDI社製チップを搭載したデバイスを用い、Pythonからftd2xxライブラリを介して制御を行っている。

\subsubsection{照明特徴量からDMX値への変換}

照明特徴量（RGB値, Brightness, 点滅速度）は、DMXチャンネルに対応する値へ変換される。

これらは連続値として出力されるため、DMXチャンネルに割り当てる際には0〜255の整数値へ正規化・クリッピング処理を行う。

具体的には、各値に対して以下の処理を適用する。

\begin{enumerate}
	\item[(1)]正規化：各特徴量の最小値・最大値を基に0〜1の範囲にスケーリング
	\item[(2)]スケーリング：0〜1の値を255倍して0〜255の範囲に変換
	\item[(3)]クリッピング：小数点以下を切り捨て、整数値に変換
\end{enumerate}

また、本システムでは、4台の照明機器を同時に制御する構成とした。
各照明機器は異なるDMXアドレスに割り当てられており、同一の照明特徴量を用いて各機器へ同時に信号を送信する。

DMX配列は513バイトで構成され、照明機器ごとにチャンネル割り当てを行っている(\tabref{tab:channel})。

\begin{table}[tb]
\centering
\small
\caption{チャンネルの割り当て}
\label{tab:channel}
\begin{tabular}{c|c}
\hline
\hline
チャンネル & 役割  \\
\hline
CH1 & 未使用 \\
CH2 & 上下動作  \\
CH3 & 点滅速度 \\
CH4 & R成分  \\
CH5 & G成分  \\
CH6 & B成分 \\
CH7 & 明るさ \\
\hline
\end{tabular}
\end{table}

\subsubsection{演奏動作情報のDMX値への変換}

音響特徴量から推定された照明特徴量とは別に、ZIG-SIMによって取得した運弓方向情報をDMX制御に直接反映させている。

具体的には、運弓方向に応じて上下動作のDMXチャンネル（チャンネル2）の値を変更し、照明の上下動作を演奏者の弓の動きに同期させている。

\begin{enumerate}
	\item[(1)]上げ弓：DMX値を120に設定（照明約60度上昇）(\figref{fig:lightupdown}参照)
	\item[(2)]下げ弓：DMX値を0に設定（照明約60度下降）(\figref{fig:lightupdown}参照)
\end{enumerate}
とすることで、演奏者の身体動作と照明の物理的動きを同期させている。
この設計により、音だけでなく演奏動作そのものが照明演出に影響を与える構造となっている。

\begin{figure}[tb]
	\centering
	\includegraphics[width=0.25\textwidth]{../fig/lightupdown.png}
	\caption{照明上下動作}
	\label{fig:lightupdown}
\end{figure}

\subsection{音響特徴量と照明特徴量の対応関係}

これまでに述べたように、本システムではヴァイオリン演奏音から抽出した音響特徴量を入力として、照明の色、明るさ、点滅速度といった照明特徴量を制御している。
本節では、システム内で扱われる音響特徴量と照明特徴量の間にどのような対応関係が見られるのかを整理し、分析する。

音響特徴量と照明特徴量の対応関係を解析するため、本研究では Random Forest における特徴量重要度（feature importance） を用いた。

Random Forest では、各決定木において分岐に使用された特徴量がどれだけ予測誤差の低減に寄与したかを基に、特徴量重要度が算出される。
この値は、モデルが予測を行う際にどの特徴量を重視しているかを示す指標である。

本研究では、照明特徴量ごとに個別の Random Forest 回帰モデルを学習し、それぞれの出力に対して音響特徴量の重要度を算出し、寄与度の高い上位5特徴量を抽出した。
これにより、照明の色・明るさ・点滅といった各要素が、どの音響的特徴と強く結びついているかを独立に分析できる。

結果の可視化には円グラフを用い、各特徴量が予測にどの程度寄与しているかを直感的に把握できるようにした。

\subsubsection{音響特徴量と照明色（R成分）の対応関係}

解析の結果、$\mathrm{RMS}$ が最も高い重要度を示し、次いで $\mathrm{Tonnetz\_6}$、$\mathrm{MFCC\_5}$、$\mathrm{MFCC\_19}$、$\mathrm{Spectral Contrast}$ の順となった(\figref{fig:rlightstyle})。

R成分の予測においてはRMSが最も高い重要度を示した。このことから、赤色成分の強度は音量の大きさに強く影響されていることが分かる。

一方で、 $\mathrm{Tonnetz\_6}$ や$\mathrm{MFCC\_5}$ 、 $\mathrm{MFCC\_19}$ といった特徴量も比較的高い重要度を示しており、R成分は音量だけでなく、音楽的構造や音色的要素も反映して決定されていることが示唆される。

また、Spectral Contrast も上位に含まれていることから、音の輪郭の明瞭さが赤色成分の強調に一定の影響を与えていると考えられる。

\subsubsection{音響特徴量と照明色（G成分）の対応関係}

解析の結果、RMS が全体の約86.8\% と極めて高い重要度を示し、他の特徴量（$\mathrm{Tonnetz\_6}$、$\mathrm{Chroma\_7}$、$\mathrm{MFCC\_5}$、$\mathrm{Spectral Contrast}$）はいずれも1〜7\%程度に留まった(\figref{fig:glightstyle})。

\begin{figure}[tb]
	\centering
	\includegraphics[width=\hsize]{../fig/rlightstyle.png}
	\caption{照明色 (R成分) に対する音響特徴量}
	\label{fig:rlightstyle}
\end{figure}

\begin{figure}[tb]
	\centering
	\includegraphics[width=\hsize]{../fig/glightstyle.png}
	\caption{照明色 (G成分)に対する音響特徴量}
	\label{fig:glightstyle}
\end{figure}

G成分において RMS が圧倒的な重要度を示したことから、緑色成分の強度は他の要因に比べて音量にほぼ支配的に決定されていることが明らかとなった。

この結果は、G成分が他の色成分に比べて、音楽的な細かなニュアンスよりも音圧の変化を直接反映する成分として扱われていることを示している。

RMS に次いで $\mathrm{Tonnetz\_6}$や$\mathrm{Chroma\_7}$ 、 $\mathrm{MFCC\_5}$ が上位に含まれているものの、それらの寄与率は比較的小さく、G成分は主として音圧変化を直接反映する成分として機能していると解釈できる。

R成分の解析結果では、RMS に加えて Tonnetz や MFCC が比較的高い割合を占めていたのに対し、G成分では RMS の寄与が著しく大きくなっている。
この違いは、照明色ごとに音響特徴量との対応関係が異なることを示している。

\subsubsection{音響特徴量と照明色（B成分）の対応関係}

解析の結果、RMS が約70.2\% と最も高い重要度を示した。
次いで  $\mathrm{MFCC\_19}$（約14.9\%）が比較的高い寄与を示し、 $\mathrm{Chroma\_7}$、$\mathrm{Tonnetz\_6}$、$\mathrm{MFCC\_5}$ はそれぞれ約5\%前後であった(\figref{fig:blightstyle})。

\begin{figure}[tb]
	\centering
	\includegraphics[width=\hsize]{../fig/blightstyle.png}
	\caption{照明色 (B成分)に対する音響特徴量}
	\label{fig:blightstyle}
\end{figure}

B成分においても RMS が最も支配的な特徴量であることから、青色照明も音量変化に大きく依存していることが分かる。
音量が大きい場面では青色成分が強調され、音量が小さい場面では抑制される傾向をモデルが学習していると考えられる。

ただし、G成分と比較すると RMS の割合はやや低下しており、B成分では音量以外の要素も一定程度考慮されていることが示唆される。

$\mathrm{MFCC\_19}$ が約15\%と比較的高い重要度を示した点は、B成分が音色や高次スペクトル成分に影響されやすいことを示している。

$\mathrm{Chroma\_7}$ および $\mathrm{Tonnetz\_6}$ が上位に含まれていることから、B成分は音量や音色だけでなく、音楽的構造（音高分布や和声的関係）とも関連していることが分かる。

これらの結果から、RGB 各成分は一様に音量に反応するのではなく、それぞれ異なる音響的役割を担っていることが明らかとなった。

\subsubsection{音響特徴量と照明の明るさの対応関係}

解析の結果、RMS が約77.8\% と最も高い重要度を示した。次いで $\mathrm{Tonnetz\_6}$（約10.4\%）、$\mathrm{MFCC\_19}$（約5.3\%）、$\mathrm{MFCC\_5}$（約3.7\%）、$\mathrm{Spectral Contrast}$（約2.8\%）が続いた(\figref{fig:brightnessstyle})。

RMS が約8割近い寄与率を示したことから、照明の明るさは主に音量に基づいて決定されていることが明らかとなった。

本モデルでは、演奏音が強くなるにつれて照明が明るくなり、弱くなるにつれて暗くなるという対応関係が学習されている。

\begin{figure}[tb]
	\centering
	\includegraphics[width=\hsize]{../fig/brilightstyle.png}
	\caption{照明の明るさに対する音響特徴量}
	\label{fig:brightnessstyle}
\end{figure}

RMS に次いで $\mathrm{Tonnetz\_6}$の重要度が高かった。
この結果は、照明の明るさが単純な音量変化だけでなく、音楽的な構造要素にも影響を受けていることを示している。

また、$\mathrm{MFCC\_19}$ および $\mathrm{MFCC\_5}$ が上位に含まれていることから、照明の明るさは音量だけでなく、音の質感や倍音構成にも影響されていることが分かる。

RGB 各成分と照明の明るさの関係を比較すると、明るさは RGB 成分以上に RMS への依存度が高いことが分かる。
これは、明るさが照明全体の基盤的なパラメータであり、色成分よりも直接的に音量変化を反映する役割を担っているためである。

一方で、Tonnetz や MFCC が一定の寄与を示していることから、明るさも完全に単純な制御ではなく、音楽的文脈を考慮した調整が行われていると解釈できる。

\subsubsection{音響特徴量と照明の点滅速度の対応関係} \label{sec:blink}

解析の結果、$\mathrm{Chroma\_7}$ が約30.6\% と最も高い重要度を示し、次いで $\mathrm{MFCC\_5}$（約21.9\%）、$\mathrm{Chroma\_1}$（約19.7\%）、$\mathrm{MFCC\_1}$（約15.6\%）、RMS（約12.\%）という順となった(\figref{fig:blinkstyle})。

\begin{figure}[tb]
	\centering
	\includegraphics[width=\hsize]{../fig/blinklightstyle.png}
	\caption{照明の点滅速度に対する音響特徴量}
	\label{fig:blinkstyle}
\end{figure}

点滅速度において最も大きな影響を与えているのは Chroma 系特徴量であり、特に $\mathrm{Chroma\_7}$ および $\mathrm{Chroma\_1}$ が全体の約50\%を占めている。
この結果は、照明の点滅が音量の大小よりも、音楽の和声的・音高的構造に強く依存していることを示している。

Chroma の変化が大きい場面では、照明の点滅が速くなり、音楽的な動きや緊張感を視覚的に強調する演出が行われていると考えられる。

$\mathrm{MFCC\_5}$ および $\mathrm{MFCC\_1}$ も比較的高い重要度を示しており、点滅速度が音色の変化にも影響を受けていることが分かる。

RMS は点滅速度においても一定の影響を持つものの、その重要度は約12.1\%に留まっている。
これは、照明の明るさとは異なり、点滅という時間的変化の制御においては、音量そのものよりも音楽的構造や音高変化が重視されていることを示唆している。

RGB 各成分および照明の明るさでは RMS の重要度が支配的であったのに対し、点滅速度では Chroma や MFCC といった音楽的特徴量が主導的な役割を果たしている。
この違いは、照明パラメータごとに参照される音響情報の種類が異なることを明確に示している。


\section{照明予測モデル} \label{sec:model}

本章では、\ref{sec:light}節で述べたヴァイオリン演奏音から照明特徴量を予測する機械学習モデルの構築方法について詳述する。

本研究では、音響特徴量を入力とし、それらの照明特徴量を予測するモデルを用いて照明の制御を行っている。
最終的に構築されたモデルは、リアルタイムの演奏音を入力として照明を自動制御する「照明予測モデル」として機能する。
照明予測モデルの構築は以下の生成フローから成り立っている。

\begin{enumerate}
	\item[\textbf{(1)}]\textbf{データベースの構築(照明予測モデルβ)}
	\item[\textbf{(2)}]\textbf{照明予測モデルβの学習}
	\item[\textbf{(3)}]\textbf{データベースの構築(照明予測モデル)}
	\item[\textbf{(4)}]\textbf{照明予測モデルの学習}
\end{enumerate}

また、\figref{fig:dataflow}に、照明予測モデル構築のためのデータ生成フローを示す。
この図に基づき、各ステップを詳細に説明する。

\begin{figure}[tb]
	\centering
	\includegraphics[width=\hsize]{../fig/dataflow.png}
	\caption{照明予測モデル構築のためのデータ生成フロー}
	\label{fig:dataflow}
\end{figure}

\subsection{データベースの構築(照明予測モデルβ)} \label{sec:database}

本研究では、最終的にリアルタイム照明制御に用いる照明予測モデルを構築する前段階として、照明予測モデルβを学習する。

照明予測モデルβは、実際のヴァイオリン演奏におけるライブ映像から取得した実測データのみを用いて学習され、音楽表現と照明表現との基礎的な対応関係を獲得する役割を担う。
しかし、本研究で学習に用いるライブ演奏映像は 3 種類と限られており、このデータのみでは十分な学習データ量を確保することが難しい。

そこで、本研究では、照明予測モデルβを最終的な照明予測モデルの学習に用いるデータベース構築に利用した。
具体的には、約600種類のMIDIデータから照明特徴量を推定するための中間的なデータとした。

本節では、この照明予測モデルβを学習させるために構築したデータベースについて述べる。

\subsubsection{映像データの収集}

まずは、データソースとして、実際のヴァイオリン演奏におけるライブ映像データを用意した。
これらは実際の照明演出を伴う演奏映像であり、音楽と照明の対応関係を直接的に取得できる点で有用である。
本研究では、入手可能な映像素材が限られているという制約のもと、音質および照明演出が比較的明瞭に記録されており、学習データとして十分な情報量を有すると判断された3種類の映像を選定した。
各映像の詳細は以下の通りである。

ライブ映像データの1つ目は、ヴァイオリン演奏曲「千本桜\cite{QianBenYingWhiteFlameFeatChuYinmikunoGeCi}」の演奏を記録した映像(\figref{fig:senbonzakura})である\cite{ayakoishikawatvSutoradeibariusuJinXiaonoQianBenYinghakiretukireShiChuanLingZiAYAKOISHIKAWA2021}。

\begin{figure}[tb]
	\centering
	\includegraphics[width=\hsize]{../fig/senbonzakura.png}
	\caption{**千本桜**の演奏映像}
	\label{fig:senbonzakura}
\end{figure}

2つ目のライブ映像データは、ヴァイオリンソロによるクラシック楽曲「ヴァイオリンソナタ第三番"バラード"\cite{VuaiorinsonataDi3FanBetovuenBaiKeShiDian}」を演奏した映像(\figref{fig:sonata})である\cite{aiolinofficialLIVEHikaritoViolinSolo2019}。

\begin{figure}[tb]
	\centering
	\includegraphics[width=\hsize]{../fig/sonata.png}
	\caption{**ソナタ第三番"バラード"**の演奏映像}
	\label{fig:sonata}
\end{figure}

3つ目のライブ映像データは、ヴァイオリンソロによる楽曲「スプラッシュ\cite{SHOGOSPLASH}」を演奏した映像(\figref{fig:splash})である\cite{violinistshogoViolinistSHOGOSorosutezi2017}。

\begin{figure}[tb]
	\centering
	\includegraphics[width=\hsize]{../fig/splash.png}
	\caption{**スプラッシュ**の演奏映像}
	\label{fig:splash}
\end{figure}

これらの映像に対して、音響特徴量と照明特徴量を抽出するための処理を行い、照明予測モデルの学習データを生成する。

\subsubsection{音響特徴量の抽出} \label{sec:soundmodel}

\begin{enumerate}
	\item[\textbf{(1)}]\textbf{ヴァイオリン音源の分離}
\end{enumerate}

ライブ映像には、会場の反響音、観客ノイズ、伴奏音など、ヴァイオリン以外の音が含まれる。
音響特徴量は不要な音が混入したまま学習するとモデルの精度劣化を招く。
そこで音源分離モデルDemucs\cite{FacebookresearchDemucs2026}を用い、音声からヴァイオリン成分を抽出した。

\begin{enumerate}
	\item[\textbf{(2)}]\textbf{ヴァイオリン特徴量の抽出}
\end{enumerate}

各映像から抽出したヴァイオリン音声を音声ファイルとして保存し、それらに対して1秒ごとに音響特徴量を算出した。
本節ではその具体的な処理内容を述べる。

まず、音声ファイルを Pythonのlibrosaライブラリ を用いて読み込む。
この際、サンプリングレートは 44.1 kHz、チャンネル数は モノラル に統一する。
読み込んだ波形データおよびサンプリングレートから音声全体の長さ（秒）を算出し、その長さに応じて 0 秒から終端まで 1 秒刻みの時間インデックスを生成する。
各 1 秒セグメントごとに extractAdvancedFeatures 関数を用いて\ref{sec:sound}節で述べた音響特徴量と同様の特徴量を算出する。
算出されたすべての音響特徴量は一つの辞書にまとめられ、最終的に「1 行を 1 秒セグメント、1 列を 1 種類の音響特徴量」とする特徴量テーブルを生成する。

この特徴量テーブルは CSV 形式で保存され、学習用 CSV データとして構築される。

\subsubsection{照明特徴量の抽出} \label{sec:lightmodel}

続いて、各映像からRGB値 (色)、Brightness(明るさ)、点滅速度の 3 種類の照明特徴量を1秒ごとに抽出し、学習用CSVデータとして構築した。
本節では、照明特徴量の抽出手法、CSVデータ構造について詳述する。

\begin{enumerate}
	\item[\textbf{(1)}]\textbf{人体検出による照明領域の特定}
\end{enumerate}

照明特徴量の抽出における映像への前処理は、YOLOv8\cite{YOLOv8StateoftheArtComputer} を用いた人物領域の除外である。
人物領域を含んだままRGBやBrightnessを算出すると、肌色や衣服の色、演奏者の動きによる輝度変化が混入し、照明そのものの変化を正確に捉えることができない。

そこで本研究では、人物検出後に膨張処理を施し、演奏者周囲も含めて背景から除外した。
そのうえで、背景画素のうち明度上位10％のみを抽出し、照明器具に最も近い画素群を対象としてRGBおよびBrightnessを算出している。

\begin{enumerate}
	\item[\textbf{(2)}]\textbf{RGB値の算出と各映像のcsvデータ}
\end{enumerate}

映像のフレーム画像を B,G,R チャンネルに分解し、背景マスクかつ上位10\%領域に該当する画素を抽出し、それぞれの平均値を計算した。
これらの値をフレーム単位でバッファに蓄積し、1秒ごとに平均を取ることで、1秒単位のRGB値の照明特徴量としてCSVに書き出した。

\begin{table*}[t]
\centering
\small
\caption{各映像の音響特徴量と照明特徴量のCSVデータ(一部抜粋)}
\label{tab:csv}
\begin{tabular}{c|cccccccccc}
\hline
\multicolumn{11}{c}{千本桜} \\
\hline
\hline
Time [s] & Centroid & Contrast & RMS &　MFCC\_1 & Chroma\_1 & R & G & B
 & Brightness & Blink Speed  \\
\hline
1.0 & 8894 & 14.5 & 0.001 & -948.6 & 0.67 & 90.1 & 45.7 & 62.1 & 59.8 & 0.40 \\
2.0 & 6722 & 17.9 & 0.005 & -661.4 & 0.76 & 93.9 & 60.2 & 74.1 & 69.3 & 0.33 \\
3.0 & 5257 & 23.2 & 0.033 & -385.2 & 0.57 & 89.8 & 60.7 & 70.0 & 87.0 & 0.29 \\
4.0 & 4459 & 21.7 & 0.023 & -398.0 & 0.75 & 84.2 & 42.5 & 53.6 & 53.5 & 0.25 \\
5.0 & 3987 & 19.5 & 0.028 & -358.8 & 0.28 & 84.8 & 32.2 & 45.8 & 47.7 & 0.44 \\
6.0 & 3678 & 18.3 & 0.050 & -347.5 & 0.37 & 130.6 & 16.2 & 31.1 & 53.3 & 0.40 \\
7.0 & 3521 & 17.8 & 0.039 & -410.6 & 0.30 & 125.8 & 126.4 & 136.2 & 56.7 & 0.40 \\
8.0 & 3400 & 16.7 & 0.008 & -471.5 & 0.43 & 124.7 & 35.9 & 38.0 & 63.3 & 0.20 \\
\hline
\multicolumn{11}{c}{ソナタ第三番} \\
\hline
\hline
Time [s] & Centroid & Contrast & RMS &　MFCC\_1 & Chroma\_1 & R & G & B
 & Brightness & Blink Speed  \\
\hline
1.0 & 1947 & 9.04 & 0.001 & -1068.0 & 0.04 & 50.8 & 45.1 & 68.7 & 50.2 & 0.40 \\
2.0 & 2898 & 20.1 & 0.000 & -809.0 & 0.12 & 52.3 & 45.6 & 69.7 & 49.8 & 0.33 \\
3.0 & 1695 & 19.9 & 0.001 & -675.9 & 0.35 & 54.5 & 46.9 & 71.6 & 52.9 & 0.29 \\
4.0 & 1560 & 20.1 & 0.002 & -618.0 & 0.43 & 53.9 & 46.2 & 70.7 & 51.1 & 0.25 \\
5.0 & 1728 & 19.1 & 0.002 & -606.8 & 0.56 & 65.0 & 58.0 & 81.3 & 63.2 & 0.22\\
6.0 & 1887 & 19.5 & 0.002 & -604.9 & 0.28 & 120.8 & 119.2 & 131.2 & 124.0 & 0.20 \\
7.0 & 2113 & 19.0 & 0.002 & -585.3 & 0.22 & 128.8 & 126.4 & 136.2 & 130.9 & 0.20 \\
8.0 & 2475 & 19.9 & 0.002 & -602.9 & 0.08 & 139.3 & 137.1 & 145.3 & 140.5 & 0.20 \\
\hline
\multicolumn{11}{c}{スプラッシュ} \\
\hline
\hline
Time [s] & Centroid & Contrast & RMS &　MFCC\_1 & Chroma\_1 & R & G & B
 & Brightness & Blink Speed  \\
\hline
1.0 & 1002 & 0.00 & 0.000 & -1131.0 & 0.08 & 91.6 & 70.0 & 76.5 & 77.5 & 0.40 \\
2.0 & 3582 & 8.98 & 0.008 & -1055.0 & 0.27 & 54.1 & 36.1 & 50.5 & 35.3 & 0.33 \\
3.0 & 1603 & 2.24 & 0.001 & -1109.0 & 0.16 & 17.8 & 11.4 & 23.9 & 19.2 & 0.29 \\
4.0 & 1564 & 15.0 & 0.032 & -774.5 & 0.23 & 45.1 & 36.0 & 66.9 & 43.4 & 0.25 \\
5.0 & 1798 & 18.4 & 0.054 & -641.0 & 0.37 & 239.0 & 238.1 & 242.0 & 242.3 & 0.22 \\
6.0 & 1893 & 20.1 & 0.055 & -667.7 & 0.30 & 189.4 & 201.3 & 208.0 & 200.0 & 0.20 \\
7.0 & 2060 & 20.1 & 0.057 & -634.1 & 0.25 & 191.6 & 223.9 & 233.7 & 215.8 & 0.20 \\
8.0 & 759.7 & 19.8 & 0.062 & -621.3 & 0.25 & 200.0 & 203.9 & 199.8 & 202.8 & 0.20 \\
\hline
\end{tabular}
\end{table*}

\begin{enumerate}
	\item[\textbf{(3)}]\textbf{Brightnessの算出と各映像のcsvデータ}
\end{enumerate}

背景照明の明るさは、グレースケール画像（または HSVのV成分）から、同じく上位10\%の明度領域を抽出して平均値を求めた。
また、RGB値と同様に、1秒ごとに平均を取ることで、1秒単位のBrightness値としてCSVに書き出した。

\begin{enumerate}
	\item[\textbf{(4)}]\textbf{点滅速度の算出と各映像のcsvデータ}
\end{enumerate}

照明の点滅速度を抽出するために、1秒ごとの平均Brightness値を時系列信号として蓄積し、
前後5秒の局所FFT を行うことで点滅に対応する周波数成分を推定し、csvに書き出した。

以上により、各映像から音響特徴量と照明特徴量のペアを1秒ごとに抽出し、学習用CSVデータとして構築した。
また、これらを1つのデータセットとして統合し、照明予測モデルの学習に使用した(\tabref{tab:csv})。

\subsection{照明予測モデルβの学習} \label{sec:modelthink}

\ref{sec:database}節で収集した学習用CSVデータを用いて、照明予測モデルの学習を行った。

\subsubsection{入力変数と出力変数}

照明予測モデルβへの入力として使用した音響特徴量のデータセットは\ref{sec:soundmodel}節で述べた通りである。

照明予測モデルβの出力として使用した照明特徴量のデータセットは\ref{sec:lightmodel}節で述べた通りである。

\subsubsection{モデルの構造と学習の実行}

照明予測モデルβには、回帰モデルとして Random Forest Regressor\cite{RandomForestRegressor} を採用した。
Random Forest は複数の決定木を組み合わせたアンサンブル学習手法である。
決定木の本数は100とし、学習精度と計算コストのバランスを考慮した。

データセットを用いて Random Forest モデルを学習し、音響特徴量と照明特徴量の対応関係を獲得した。
学習が完了したモデルは、再利用を目的として joblib を用いてファイルとして保存した。

\subsection{データベースの構築(照明予測モデル)} \label{sec:midi}

本節では、照明予測モデルを学習するためのデータベースの構築手法について詳述する。

本研究では、約600種類のMIDIデータを音声信号へ変換した上で、音響特徴量を抽出し、照明予測モデルβによって照明特徴量を推定するという処理フローを採用した。

これにより、MIDIデータから得られた音響特徴量と対応する照明特徴量のデータベースを構築することができる。

\subsubsection{MIDIデータ導入の目的}

MIDIデータを学習データとする目的は、以下の点にある。

第1に、演奏音のバリエーションを大幅に増加させることである。
MIDIデータには、音高変化、フレーズ構造、音価の違いなど、実演奏に近い音楽的情報が含まれており、ライブ映像データだけでは不足しがちな演奏パターンを補完できる。

第2に、モデルが特定の演奏映像や照明環境に過度に依存することを防ぎ、未知の演奏音に対しても安定した照明予測を行えるようにする点である。

\subsubsection{MIDIデータセットの概要}

本研究で使用したMIDIデータは、ヴァイオリン演奏において代表的かつ教育的価値の高い楽曲群から構成されている。
具体的には、「Paganini\cite{Paganini24Violin}」、「Kayser\cite{36ViolinStudies}」、「Wohlfahrt\cite{FreeSheetMusic}」の作品を中心としたヴァイオリンソロのMIDIデータを用いた。

Paganini の作品は、高度な演奏技巧や急激な音高変化、速いパッセージを多く含み、演奏表現の幅が非常に広い。
一方で、Kayser および Wohlfahrt の練習曲は、音階練習や基本的な運弓・音程感覚の習得を目的とした楽曲が多く、比較的安定した音高変化と明瞭なフレーズ構造を持つ。

MIDIデータは、演奏音の音高、音価、発音タイミングなどを厳密に数値情報として保持しているため、音響特徴量抽出において再現性が高く、楽曲間の比較にも適している。
特に、これらの練習曲・技巧曲は、音楽教育の現場でも広く使用されており、ヴァイオリン演奏の典型的な音域・音色変化・フレーズ構造を網羅している点で、本研究の補助的データとして有用である。

\subsubsection{音響特徴量の抽出}　\label{sec:otomidi}

まず、MIDIデータは直接音響特徴量を算出することができないため、音声信号へ変換する必要がある。
本研究では、ソフトウェアシンセサイザー FluidSynth\cite{FluidSynthSoftwareSynthesizer}を用い、MIDIファイルをWAV形式へ変換した。

生成されたWAVファイルに対し、Pythonライブラリ librosa を用いて音響特徴量を抽出した。
抽出する特徴量の種類および構成は、\ref{sec:modelthink}節で構築した照明予測モデルβの学習時に用いたデータセットと完全に一致させている。

\subsubsection{照明予測モデルβによる照明特徴量推定} \label{sec:lightmidi}

抽出された音響特徴量は、\ref{sec:modelthink}節で構築した 照明予測モデルβに入力される。
本モデルは、ライブ映像データから学習した「音響特徴量と照明特徴量の対応関係」を保持しており、MIDI由来の音響特徴量に対しても照明特徴量を推定することが可能である。
推定される照明特徴量は、RGB値、Brightness、点滅速度の3種類である。

\subsubsection{CSV形式での出力}

また、各MIDIファイルについて、以下の情報を1つのCSVデータとして出力した(\tabref{tab:csvmidi})。

\begin{enumerate}
	\item[(1)]時間(秒)
\end{enumerate}
\begin{enumerate}
	\item[(2)]音響特徴量
\end{enumerate}
\begin{enumerate}
	\item[(3)]照明予測モデルβによる推定照明特徴量
\end{enumerate}

\begin{table*}[t]
\centering
\small
\caption{MIDIデータの音響特徴量と推定された照明特徴量のCSVデータ(一部抜粋)}
\label{tab:csvmidi}
\begin{tabular}{c|cccccccccc}
\hline
\multicolumn{11}{c}{Wohlfahrt\_Op45-40\_BernardChevalier\_ijMGhEYjP98-0000-0057} \\
\hline
\hline
Time [s] & Centroid & Contrast & RMS &　MFCC\_1 & Chroma\_1 & R & G & B
 & Brightness & Blink Speed  \\
\hline
1.0 & 1609 & 47.2 & 0.000 & -751.4 & 0.65 & 118.0 & 119.4 & 142.9 & 123.2 & 0.23 \\
2.0 & 2760 & 15.7 & 0.002 & -651.4 & 0.09 & 76.1 & 60.0 & 91.5 & 67.7 & 0.24 \\
3.0 & 2827 & 14.8 & 0.001 & -632.2 & 0.14 & 93.2 & 82.7 & 104.9 & 88.7 & 0.28 \\
4.0 & 2389 & 13.0 & 0.002 & -637.3 & 0.14 & 81.2 & 66.6 & 102.4 & 75.0 & 0.23 \\
5.0 & 1978 & 14.7 & 0.001 & -630.9 & 0.04 & 85.2 & 81.8 & 113.6 & 87.5 & 0.24 \\
6.0 & 1900 & 14.2 & 0.001 & -671.0 & 0.04 & 88.9 & 88.7 & 114.8 & 92.6 & 0.23 \\
7.0 & 2177 & 15.3 & 0.002 & -640.0 & 0.00 & 90.0 & 71.8 & 98.5 & 80.4 & 0.26 \\
8.0 & 2832 & 15.6 & 0.002 & -634.9 & 0.13 & 75.8 & 52.8 & 81.5 & 62.8 & 0.24 \\
\hline
\multicolumn{11}{c}{Paganini\_Op01-23\_ItzhakPerlman\_aP15fyVkPVg-0000-0284} \\
\hline
\hline
Time [s] & Centroid & Contrast & RMS &　MFCC\_1 & Chroma\_1 & R & G & B
 & Brightness & Blink Speed  \\
\hline
1.0 & 2280 & 31.7 & 0.002 & -676.8 & 0.36 & 95.4 & 76.2 & 115.9 & 87.4 & 0.23 \\
2.0 & 2837 & 12.5 & 0.008 & -544.3 & 0.00 & 89.0 & 66.7 & 97.4 & 76.5 & 0.26 \\
3.0 & 1993 & 12.9 & 0.005 & -565.4 & 0.14 & 78.9 & 56.3 & 86.7 & 66.3 & 0.26 \\
4.0 & 2060 & 13.0 & 0.007 & -533.0 & 0.05 & 74.5 & 52.9 & 81.5 & 62.3 & 0.23 \\
5.0 & 2804 & 12.8 & 0.007 & -552.2 & 0.00 & 90.5 & 65.8 & 102.8 & 77.4 & 0.24\\
6.0 & 2492 & 12.7 & 0.009 & -532.7 & 0.02 & 83.5 & 60.9 & 89.5 & 70.8 & 0.25 \\
7.0 & 1746 & 13.3 & 0.004 & -572.1 & 0.15 & 76.2 & 51.9 & 82.2 & 62.5 & 0.23 \\
8.0 & 2512 & 13.2 & 0.007 & -533.6 & 0.02 & 77.0 & 54.7 & 91.5 & 65.5 & 0.23 \\
\hline
\multicolumn{11}{c}{Kayser\_Op20-09\_BernardChevalier\_j1AYnoTq0RM-0001-0100} \\
\hline
\hline
Time [s] & Centroid & Contrast & RMS &　MFCC\_1 & Chroma\_1 & R & G & B
 & Brightness & Blink Speed  \\
\hline
1.0 & 1736 & 23.3 & 0.031 & -657.4 & 0.51 & 85.5 & 89.2 & 124.9 & 93.2 & 0.24 \\
2.0 & 1479 & 12.0 & 0.033 & -587.6 & 0.54 & 79.2 & 62.8 & 92.7 & 71.3 & 0.23 \\
3.0 & 1481 & 13.3 & 0.033 & -565.4 & 0.41 & 94.2 & 74.4 & 106.3 & 84.3 & 0.22 \\
4.0 & 2080 & 12.2 & 0.059 & -540.0 & 0.18 & 78.0 & 58.9 & 91.2 & 68.4 & 0.23 \\
5.0 & 2246 & 14.7 & 0.062 & -587.9 & 0.47 & 85.1 & 65.6 & 97.0 & 75.0 & 0.23 \\
6.0 & 2281 & 14.8 & 0.064 & -582.4 & 0.47 & 95.5 & 71.8 & 111.3 & 84.7 & 0.23 \\
7.0 & 2536 & 17.7 & 0.084 & -571.8 & 0.16 & 84.4 & 59.5 & 96.6 & 71.5 & 0.24 \\
8.0 & 2984 & 16.4 & 0.097 & -567.1 & 0.35 & 86.8 & 62.7 & 104.5 & 74.6 & 0.23 \\
\hline
\end{tabular}
\end{table*}

\subsection{照明予測モデルの学習} \label{sec:midimodel}

本節では、\ref{sec:midi}節で生成したデータベースをもとに、照明予測モデルの学習を行った。

\subsubsection{入力変数と出力変数}

照明予測モデルへの入力として使用した音響特徴量のデータセットは\ref{sec:otomidi}節で述べた通りである。

照明予測モデルの出力として使用した照明特徴量のデータセットは\ref{sec:lightmidi}節で述べた通りである。

\subsubsection{モデルの構造と学習の実行}

学習手法としては、\ref{sec:modelthink}節と同様に Random Forest Regressor を採用した。
モデル構造や主要なハイパーパラメータも、同一に設定している。

データセットは、モデルの汎化性能を評価するため、全体の80\%を学習用データ、20\%をテスト用データに分割した。
分割はランダムに行い、乱数シードを固定することで実験の再現性を確保した。

\begin{enumerate}
	\item[(1)]学習データ：モデルの学習に使用
\end{enumerate}
\begin{enumerate}
	\item[(2)]テストデータ：モデルの性能評価に使用
\end{enumerate}

拡張された学習データセットを用いて Random Forest モデルの学習を実行し、音響特徴量と照明特徴量の対応関係を再度獲得した。
学習が完了したモデルは、 joblib を用いてファイルとして保存し、リアルタイム照明制御システムに直接組み込んだ。

\subsection{照明予測モデルの評価}

本章では、\ref{sec:midimodel}節で構築した 照明予測モデルの性能評価について述べる。

\subsubsection{評価方法}

評価には、機械学習における回帰問題で一般的に用いられる決定係数（R²スコア）を採用した。

R²スコアは、モデルの予測値が実測値の分散をどの程度説明できているかを示す指標であり、以下の式で定義される。

\[
\text{決定係数 } R^2
= 1 - \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}
           {\sum_{i=1}^{n} (y_i - \bar{y})^2}
\]

\[
\begin{aligned}
y_i &: \text{実測値} \\
\hat{y}_i &: \text{予測値} \\
\bar{y} &: \text{実測値の平均}
\end{aligned}
\]

R²スコアは 1 に近いほど予測精度が高く、0 は平均値予測と同等、負の値は平均予測より劣ることを示す。

また、評価には、学習時に使用していないテストデータを用いた。
テストデータには、ライブ映像由来データを中心に含めることで、実際の照明演出との整合性を重視した評価を行った。

評価は、RGB値、Brightness、点滅速度といった複数の照明特徴量を同時に含む多出力回帰問題として実施し、全出力を総合したR²スコアを算出した。

\subsubsection{評価結果}

最終照明予測モデルに対する評価の結果、R²スコアは 0.85 を示した(\figref{fig:r2-result})。

\begin{figure}[tb]
	\centering
	\includegraphics[width=\hsize]{../fig/r2-result.png}
	\caption{決定係数 (R²) の結果}
	\label{fig:r2-result}
\end{figure}

この結果は、音響特徴量から照明特徴量への変換において、モデルが実測照明データの分散の約85\%を説明できていることを意味する。
すなわち、照明の色、明るさ、点滅といった複合的な要素を、演奏音のみから高精度に推定できていることが確認された。


\section{実演} \label{sec:play}

本章では、本研究で構築したリアルタイム照明制御システムを用いて実施した実演について述べる。
本研究では、異なる楽曲および異なる環境条件下におけるシステムの適応性と表現の妥当性を検討するため、計2回の実演を実施している。
実演は、制御条件を限定した環境下で基礎的な検証を行う「大学教室における検証的実演」と、実環境における観客を伴う状況でシステムの実用性と表現効果を評価する「新町商店街における公開実演」の順で実施した。

\subsection{第1回実演：君に乗せて(検証的実演)}

\subsubsection{実演環境} \label{sec:ensou}

最初の実演は、福知山公立大学の3201教室において実施した。
教室内は外光や環境音の影響が比較的少なく、安定した条件で実験を行うことが可能である。
そのため、音響入力、運弓動作取得、照明制御が正しく連動しているかを確認する環境として適している。

室内には演奏者を中央に配置し、その周囲四方向にDMX対応照明機器を設置した。
照明は床方向から天井に向けて照射される構成とし、四方から演奏者を照らすような光環境を作り出した。
システムは以下の機器構成で運用した(\tabref{tab:kizai})。また、実演における各機材の配置図(上面図)は\figref{fig:setting}の通りである。

\begin{table}[tb]
\centering
\small
\caption{実演機材構成}
\label{tab:kizai}
\begin{tabular}{c}
\hline
\hline
ヴァイオリン(演奏者：筆者)　\\
PC(Windows 11)　\\
コンデンサマイク(音声入力)  \\
スマートフォン(ZIG-SIM用) \\
USB-DMXインタフェース  \\
DMX対応照明機器4台  \\
\hline
\end{tabular}
\end{table}

\subsubsection{実演曲目}

実演に使用した楽曲は、『君に乗せて\cite{JunwonoseteJingShangazuminoGeCiTuteiine}』(作曲：久石譲)である。
本曲はアニメ『天空の城ラピュタ』の主題歌として広く知られており、旋律が明瞭でテンポ変化が比較的少ない楽曲である。
ヴァイオリンソロアレンジ版を用い、演奏時間は約1分30秒である。
演奏はAメロからサビまでをカバーしている。
\figref{fig:takeyouwithme-score}に実演で使用した楽譜を示す。

\begin{figure}[tb]
	\centering
	\includegraphics[width=\hsize]{../fig/takeyouwithme-score.png}
	\caption{実演曲目『君に乗せて』楽譜}
	\label{fig:takeyouwithme-score}
\end{figure}

\subsubsection{実演内容と観察結果}

この実演では、来場者を想定せず、研究者自身による検証を目的として演奏を行った(\figref{fig:takeyouwithme-Amelody-Bmelody},\figref{fig:takeyouwithme-chorus})。
図のように、演奏音の入力に応じて照明の色、明るさおよび点滅速度が変化すること、運弓方向に応じて照明の上下動作が発生することを確認した。

\begin{figure}[tb]
	\centering
	\includegraphics[width=\hsize]{../fig/takeyouwithme-Amelody-Bmelody.png}
	\caption{『君に乗せて』実演中の照明変化の様子(Aメロ～Bメロ)}
	\label{fig:takeyouwithme-Amelody-Bmelody}
\end{figure}

\begin{figure}[tb]
	\centering
	\includegraphics[width=\hsize]{../fig/takeyouwithme-chorus.png}
	\caption{『君に乗せて』実演中の照明変化の様子(サビ)}
	\label{fig:takeyouwithme-chorus}
\end{figure}

\subsection{第2回実演：紅蓮華（公開実演）}

\subsubsection{実演環境}

次の実演は、京都府福知山市の新町商店街にある「Tsunaga Room\cite{TsunagaRoomOJiaYiShangTuShuGuanWeiMan}」にて実施した。
この実演は、一般来場者を含む環境で行われた公開実演である。
また、使用機材の配置や照射方法は\ref{sec:ensou}節と同様である。

\subsubsection{実演曲目}

実演に使用した楽曲は、『紅蓮華\cite{HongLianHuaLiSAnoGeCiTuteiine}』（作曲：草野華余子）である。
本曲はアニメ『鬼滅の刃』のオープニングテーマとして広く知られており、力強いリズムと感情豊かなメロディが特徴である。
ヴァイオリンソロアレンジ版を用い、演奏時間は約1分30秒である。
演奏はイントロから1番のサビまでをカバーしている。
\figref{fig:gurenge-score}に実演で使用した楽譜を示す。

\subsubsection{実演の進行方法}

第2回実演では、来場者に対して本研究の趣旨および実演内容について簡単な説明を行った上で、ヴァイオリン演奏と照明演出の実演を実施した。

実演開始前に、演奏者（筆者）から来場者に対して、以下の内容を口頭で説明した。

まず、本実演ではヴァイオリン演奏の音そのものが照明を制御していることを説明した。
具体的には、演奏音の音量や音色、響き方といった要素がリアルタイムに解析され、それに基づいて照明の色や明るさ、点滅が自動的に変化する仕組みであることを伝えた。
また、照明は事前に演出をプログラムしているものではなく、演奏内容に応じて毎回異なる変化が生じることを説明した。

次に、音響情報に加えて、演奏者の運弓動作が照明の動きに反映されていることを説明した。
演奏者の腕の動きをスマートフォンで取得し、弓を上げる動作と下げる動作に応じて照明が上下に動く仕組みであることを示した。
この説明により、来場者が演奏中の身体動作と照明の動きを対応づけて観察できるよう配慮した。

説明後、楽曲『紅蓮華』の実演を開始した。

\begin{figure}[tb]
	\centering
	\includegraphics[width=\hsize]{../fig/gurenge-score.png}
	\caption{実演曲目『紅蓮華』楽譜}
	\label{fig:gurenge-score}
\end{figure}

\subsubsection{実演中の照明変化の様子}

\figref{fig:gurenge-play}に、実演中の照明変化の様子を示す。
これらの画像は、前述した実演環境にて、ヴァイオリン演奏曲『紅蓮華』を演奏した際の照明挙動を記録したものである。

\begin{figure*}[tb]
	\centering
	\includegraphics[width=\hsize]{../fig/gurenge-play.png}
	\caption{『君に乗せて』実演中の照明変化の様子(左から「イントロ」、「Aメロ～Cメロ」、「サビ」、「サビ」)}
	\label{fig:gurenge-play}
\end{figure*}

\begin{enumerate}
	\item[\textbf{(1)}]\textbf{イントロ部分における照明の様子}
\end{enumerate}

演奏開始直後のイントロ部分では、演奏音のRMS値が全体を通して低い水準で推移しており、音量変動も小さい区間である。
このため、機械学習モデルの出力として得られるBrightness値は低く抑えられ、照明は全体的に暗い状態が維持されている。

色成分については、RGBのうちB成分が相対的に高く、RおよびG成分は低い値に制御されている。
その結果、照明は青色系を基調とした色調となり、急激な色相変化は発生していない。

また、この区間では点滅速度は低い値に設定されており、照明はほぼ連続点灯状態を保っていた。
そのため、点滅による視覚的変化はほとんど認識されず、静的な照明表現が支配的であった。

運弓方向に連動した照明の上下動作についても、イントロ部分では運弓速度が比較的緩やかであるため、照明の上下移動は演奏動作と概ね一致していた。

このような制御により、楽曲の導入部に適した静的かつ安定した視覚環境が形成されている。

\begin{enumerate}
	\item[\textbf{(2)}]\textbf{Aメロ～Cメロ部分における照明の様子}
\end{enumerate}

演奏が進行し、中盤部分に入ると、RMS値の平均および最大値が徐々に増加し、それに伴ってBrightnessも段階的に上昇していることが確認できる。

また、音色の変化に対応してRGB成分にも変化が生じており、特にR成分の増加が顕著である。
その結果、照明の色調は青系から赤色を含む暖色系へと移行している。
これにより、演奏者の身体およびヴァイオリンが赤系の光によって強調され、楽曲中盤における緊張感やエネルギーの高まりが視覚的に表現されている。

点滅速度については、音響特徴量の変化に応じて内部的には変動しているものの、その変化幅は小さく、Brightnessや色成分の変化と比較すると視覚的にはあまり顕著には現れていなかった。
そのため、この区間においても点滅速度は補助的な要素に留まり、主な視覚的変化は色調および輝度の変化によって構成されている。

運弓方向に連動した照明の上下動作は、中盤部においても引き続き確認できたが、演奏テンポが上がるにつれて、照明の上下移動と運弓動作との間に生じるラグがやや顕在化した。

\begin{enumerate}
	\item[\textbf{(3)}]\textbf{サビ部分における照明の様子}
\end{enumerate}

さらに、楽曲がサビ部分に近づくにつれて、RMS値は高い値を維持するとともに、短時間内での変動幅も大きくなっている。
このような音響的特徴を受けて、モデルはBrightnessを高水準に設定され、照明は高輝度状態で制御されている。

加えて、音響特徴量の変動に応じてRGB成分が時間的に大きく変化しており、紫色や黄色など複数の色がフレーム単位で切り替わる挙動が確認できる。
これにより、照明は静的な演出から動的な演出へと移行し、楽曲の盛り上がりに同期した視覚的な変化が強調されている。
その結果、演奏音と照明演出が高い時間的整合性をもって連動し、没入感の高い演奏空間が実現されている。

一方で、点滅速度については、音響的には最も変動が大きい区間であるにもかかわらず、照明表現として顕著な点滅が確認される場面は少なかった。

運弓方向に連動した照明の上下動作については、サビ部分では運弓速度が大きくなるため、照明の移動が追従できない場面が複数確認された。
上下動作には明確なラグが生じ、結果として照明の動きと演奏動作の対応関係が視覚的に把握しにくくなる傾向が見られた。

\section{考察} \label{sec:think}

前章では、音響特徴量に基づく照明制御の挙動を、楽曲構造ごとに示した。本節では、それらの結果を基に、本システムの制御特性を設計意図との対応関係および実演環境における実用性の観点から詳細に考察する。

\subsection{意図通りに制御できた点}

本実験結果から、音響特徴量に基づく照明制御が、楽曲構造および演奏表現と整合性をもって機能していることが確認できた

\subsubsection{イントロ部分における照明制御}

まず、演奏導入部においては、RMS値が低い区間でBrightnessが低く制御され、青色系を基調とした落ち着いた照明表現が生成されていた。
これは、音量が小さい区間では視覚的刺激を抑制するという照明予測モデルの意図と一致しており、楽曲の導入部として適切な雰囲気を形成できている。

\subsubsection{Aメロ～Cメロ部分における照明制御}

次に、中盤部では、音量の増加に伴ってBrightnessが段階的に上昇し、同時にRGB成分のうちR成分が強調されることで、暖色系の照明へと遷移していた。
このような色調および輝度の連続的変化は、音響特徴量の変化を滑らかに視覚表現へ反映するという本システムの設計方針を満たしている。

さらに本システムの特徴として、運弓動作に応じて照明位置が上下方向に変化する挙動もしっかり確認された。
実演では、上げ弓となる区間で照明が上方へ移動し、下げ弓となるの区間では下方に留まる傾向が確認された。

\subsubsection{サビ部分における照明制御}

サビ部分では、音量および音響特徴量の時間的変動が大きくなるのに対応して、高輝度かつ動的な色変化が発生していた。
複数色が時間的に切り替わる照明演出は、楽曲の盛り上がりやエネルギー感を強調する効果があり、演奏音と照明演出がリアルタイムに同期していることを示している。
以上より、音響情報に基づく照明制御は、意図した方向性で機能していると評価できる。

\subsection{実演において明らかとなった課題}

一方で、実演結果からはいくつかの課題も確認された。

\subsubsection{照明制御における課題}

第1に、サビ部分など音響特徴量の変動が急激な区間では、RGB成分が短時間で大きく変化するため、照明の色変化がやや過度に感じられる場面が見られた。
これは、音響特徴量の瞬時値を直接照明制御に反映していることが要因であり、時間方向の平滑化処理が不十分である可能性が示唆される。
また、この影響により照明の点滅速度が聴感上の自然さと一致せず、過度に速い、あるいは不規則に感じられる場合があった。

第2に、照明制御が主に音量（RMS値）に強く依存しているため、音量変化が小さいが音楽的に重要なフレーズ（抑揚や表情付け）に対して、照明変化が十分に反映されない場合があった。
この点は、音量以外の音響特徴量（スペクトル重心や倍音構造など）の寄与が限定的であることに起因していると考えられる。

第3に、実演中の照明挙動を観察した結果、点滅速度の変化が明確に認識される場面は全体として少なかった。
点滅速度は、\ref{sec:blink}節でも述べたように、音響特徴量のうちchromaの時間的変動を主な入力として制御されている。
しかし、本実演で用いたヴァイオリン演奏においては、chroma特徴量の時間的変動が比較的小さかったため、点滅速度が照明表現として顕著に知覚されるには至らなかったと考えられる。
『紅蓮華』のメロディは主に単音旋律で構成されており、和声的な転調や急激な音高構造の変化が少ない。
そのため、chromaの各成分の分布は演奏全体を通して大きく変動せず、点滅速度制御に反映される変化量も小さくなったと考えられる。

\subsubsection{演奏動作取得における課題}

ZIG-SIMを用いた運弓方向推定は概ね有効であったが、演奏中の細かな動作や急激な動きに対して、誤判定が生じる可能性も確認された。
特に、弓の動きが小さい場合や、角速度が閾値付近にある場合には、上下動作が不安定になることがあった。

また、実演全体を通して、照明の上下動作には演奏動作に対してわずかな時間遅れ（ラグ）が生じていることが確認された。
特に、テンポが速く、運弓速度が大きくなる区間では、照明の移動が演奏動作に追従しきれず、上下動作が遅延あるいは追従不能となる場面が見られた。
その結果、運弓方向と照明動作の対応関係が視覚的に把握しづらくなる傾向があった。

このラグは、ジャイロセンサの解析処理、および照明機器の物理的応答に要する時間が累積したことによるものと考えられる。

この課題に対しては、センサ情報の平滑化や、複数軸情報を用いた判定など、より安定した動作推定手法の導入が必要である。

\subsection{本研究における実演システム構築の意義}

本研究においてヴァイオリン演奏に合わせて照明が自動制御されるシステムを構築し、実際に実演まで行ったことの意義を、研究的観点、実践的観点、の二つの側面から明確に位置付ける。

\subsubsection{研究的観点からの意義}

研究的観点において、本研究の意義は、音響特徴量に基づく照明制御を単なるシミュレーションやオフライン解析に留めず、リアルタイムかつ実演環境で検証した点にある。

これにより、音響特徴量の変動が照明挙動にどのような影響を与えるのかを、実演という制約条件下で観察することが可能となった。

\subsubsection{実践的観点からの意義}

実践的観点では、本研究は演奏者が事前に照明操作を意識することなく、演奏に集中できる環境を実現した点に意義がある。

照明制御を自動化することで、演奏中に照明オペレータを必要とせず、単独演奏や小規模な演奏環境においても視覚演出を付加できる可能性を示した。


\section{結び} \label{sec:result}

本研究では、ヴァイオリン演奏に基づいて照明をリアルタイムに制御するインタラクティブ照明演出システムの構築を目的とし、音響特徴量および演奏動作情報を用いた照明制御手法を提案・実装・実演を通して検証した。

提案手法では、ヴァイオリン演奏音から抽出した多次元の音響特徴量を入力として、照明の色（RGB）、明るさ、点滅といった複数の照明パラメータを同時に予測する機械学習モデルを構築した。
また、ZIG-SIMを用いて演奏者の運弓動作を取得し、照明の物理的な上下動作に反映させることで、音響情報だけでは表現しきれない身体的表現を照明演出に取り込んだ。

照明予測モデルの構築においては、ライブ映像およびMIDI音源を用いたデータ生成手法を採用し、Random Forest回帰モデルによって音響特徴量と照明特徴量の関係を学習した。

実演では、屋内会場において楽曲『君に乗せて』と『紅蓮華』を演奏し、照明演出の表現力およびリアルタイム性を検証した。
これらの実演を通して、演奏音および演奏動作に応じて照明が自律的に変化し、演奏と照明が同期する演出が実現できることを確認した。

以上より、本研究は、音楽演奏と照明演出をリアルタイムに結びつける一つの実践的手法を提示し、演奏者の表現を視覚的に拡張する可能性を示した点に意義があると結論づけられる。


\begin{acknowledgment}
本論文を執筆するにあたり、多くの方々からご指導・ご支援を賜りました。
ここに深く感謝の意を表します。
まず、本研究を進めるにあたり、研究の方向性から終始丁寧なご指導と貴重なご助言をくださった橋田光代准教授に心より御礼申し上げます。
また、研究室の4年生の皆様には、日頃より温かく接していただき、研究の相談にも快く応じていただきました。
皆様との和やかな会話は、研究生活の大きな支えとなりました。
心より感謝いたします。
最後に、本研究に関わってくださったすべての方々に、厚く御礼申し上げます。
\end{acknowledgment}

% !TEX root = _main.tex
% ========================================
% 卒業論文　本文
% ========================================

\section{はじめに}

舞台演出やライブパフォーマンスにおいて、照明は音楽表現を視覚的に拡張し、観客の没入感を高める重要な要素である。
照明の色や明暗、動きは演奏の意図や感情を強調し、音響のみでは伝わりにくい表現を補完する役割を果たす。
しかし、従来の舞台照明は照明オペレーターによる手動操作に大きく依存しており、演奏内容に即応した柔軟な制御が難しいという課題がある。
特に、小規模な演奏環境や個人演奏においては、専任の照明オペレーターを配置することが困難であり、演奏者自身の表現意図を照明に反映させる手段が限られている。

このような背景から、演奏そのものを入力として照明を自動制御する仕組みが求められている。
音楽演奏には音響的特徴だけでなく、身体動作に基づく表現も含まれており、これらを照明制御に統合することで、より一体感のある演出が実現できると考えられる。

そこで本研究では、ヴァイオリン演奏に着目し、音響信号と身体動作を組み合わせたリアルタイム照明制御システムの構築を目的とする。
具体的には、マイクから入力されたヴァイオリン音響信号から、スペクトル重心、スペクトル帯域幅、MFCC、ZCR などの音響特徴量を抽出し、機械学習モデルを用いて照明の色、明るさ、点滅速度といった照明特徴量を予測する。
また、スマートフォンのモーションセンシングアプリ ZIG-SIMを用いて、演奏者のヴァイオリン演奏における弓の上下動作を角速度として取得し、照明の上下動作に直接反映させる。

なお、本研究の着想は、筆者自身の「自分のヴァイオリン演奏だけで照明を直接動かしてみたい」という個人的関心に基づいている。
ヴァイオリンを対象とした理由として、筆者自身が演奏経験を有しており、演奏者の立場からシステムの反応性や表現性を評価できる点が挙げられる。

以上より本研究では、ヴァイオリン演奏音から抽出した音響特徴量に基づいて照明の色、明るさ、点滅速度を制御すると同時に、演奏者の運弓動作を照明の上下動作として直接反映することで、音楽表現と身体表現が一体となった照明演出の実現を目指す。


\section{関連研究} \label{sec:RelatedResearch}

音楽ライブやコンサートにおける照明演出は、従来は照明デザイナーやオペレータの経験や感性に基づいて設計されてきた。
しかし、小規模公演や即興性の高い演奏では十分な準備時間や人員を確保することが難しく、音楽情報処理技術を用いた照明演出の自動化が有効な手段として注目されている。

\subsection{音響信号に基づく照明演出の自動化}

音響信号に基づく照明制御に関する研究として、月東らは楽曲音源からリズムの強調箇所を検出し、それを照明演出へ反映する手法を提案している\cite{tsukihigashi25}。
この研究では、楽曲をドラム、ベース、ギター、ボーカルの各パートに音源分離し、それぞれのオンセットエンベロープを算出した上で、複数パートで同時に強いオンセットが観測されるタイミングを「キメ」と定義している。
検出されたキメを基点として、照明のフラッシュや明滅といった瞬間的な演出を付与し、主観評価実験によりリズムの一体感や迫力の向上を確認している。

この手法は高い時間解像度でリズム的アクセントを捉えられる点に特徴がある一方で、照明制御はキメの有無に依存した離散的なものに留まっており、演奏中の表現変化を連続的に反映することは難しい。
また、演奏者の身体的動作は入力として扱われていない。

\subsection{楽曲の印象・意味内容に基づく照明生成}

楽曲全体の印象や意味内容を照明演出に反映する研究も多く報告されている。
神野らは、歌詞および曲調の印象に基づいて照明演出を自動生成するシステムを提案している\cite{kanno21}。

この研究では、歌詞を形態素解析した上で分散表現ベクトルに変換し、言語イメージスケールを用いて照明色候補を選択する。
さらに、楽曲の長短調やテンポに基づいて色の優先度や配置を決定し、BPMや歌詞のネガティブ・ポジティブ度を用いて明度を調整することで、照明の色、配置、明るさを総合的に決定している。
この手法により、照明デザイナーの感性に依存しない一貫性のある演出が実現されている。

一方で、照明変化は小節やフレーズ単位といった比較的長い時間スケールで行われるため、演奏中の瞬間的な表現変化や演奏者の身体動作を直接反映することは想定されていない。

\subsection{深層学習を用いた照明演出生成}

近年では、照明演出を生成タスクとして捉え、深層学習モデルを用いて照明パラメータを自動生成する研究も行われている。
月東らの研究内でも言及されている Zhao らの手法では、楽曲特徴量系列を入力として照明の色や明るさの時系列データを生成する生成モデルが提案されている\cite{Zhao25}。

このようなアプローチは、表現力や多様性の高い照明演出を実現できる可能性を示しているが、学習データへの依存度が高く、生成過程の解釈が困難であるという課題がある。
また、演奏者の身体動作は入力として扱われていない。

\subsection{演奏者操作によるリアルタイム照明制御}

演奏者自身が照明演出に直接関与する研究として、瀧口らはドラム音やMIDIフットコントローラーを用いて演奏中に照明を操作可能なシステムを提案している\cite{takiguchi25}。
この研究では、バスドラム音の音量が閾値を超えた際に照明を点灯させる手法や、足元のコントローラー操作によって照明の色や輝度、ストロボ効果を変更する仕組みが実装されている。

この手法により、即興演奏に柔軟に対応できる照明演出が実現されている一方で、演奏者が照明操作を意識する必要があり、演奏負荷が増加する可能性がある。
また、演奏動作そのものを自動的に取得・利用する仕組みは含まれていない。

\subsection{身体動作と照明演出の連動}

演奏者の身体動作と照明演出を連動させる研究として、浅田らはオンラインライブ空間において、演奏者の身体情報を用いて照明や振動装置を制御するシステムを提案している\cite{asada20}。
この研究では、ギター演奏者の腕の角度に応じてムービングライトの向きを変化させるなど、身体動作を照明の物理的な動きに対応付けることで、演奏者と観客の一体感を高めている。

しかし、音響特徴量と身体動作を統合的に扱う設計にはなっておらず、音楽表現と照明表現の対応関係は限定的である。

\subsection{本研究の位置づけ}

以上の関連研究から、音響特徴量に基づく照明制御、楽曲印象に基づく照明生成、演奏者操作型および身体動作連動型の照明演出はそれぞれ発展してきたものの、音響情報と演奏者の身体動作を同時に統合し、連続的かつリアルタイムに照明を制御する研究は十分に行われていないことが分かる。

これを踏まえ、本研究ではヴァイオリン演奏音の特徴量と演奏者の運弓動作に応じてステージ上の照明がリアルタイムに変化する照明制御システムを提案する。


\section{リアルタイム照明制御システム} \label{sec:system}

\subsection{プロトタイプシステムの実装}

システムの実装に先立ち、まずは簡易的なプロトタイプシステムの実装を行った。

このプロトタイプを実装した理由は、筆者自身が実際にヴァイオリンを演奏しながら照明を動かすという体験を試行的に行い、演奏と照明演出が同時に存在する状況を身体感覚として把握することで、システム設計の方向性を検討するためである。

本プロトタイプは、ヴァイオリン演奏中の演奏者が足元に配置した3つのフットペダルを操作することで、演奏者周囲に設置された照明機材の照明色を制御するシステムである。

各フットペダルは、光の三原色であるRGB（Red：赤、Green：緑、Blue：青）の各成分（以下、「RGB値」）の切り替えに対応しており、これらを組み合わせることで照明のRGB値を変化させることができる。
また、演奏時の音量に応じて照明の明るさ（以下、「Brightness」）が変化する仕組みを備えており、演奏表現と照明演出を連動させることを可能としている。

そして、このプロトタイプを用いた試行を通して以下の課題が明らかとなった。

まずは、照明のRGB値の変化がペダル操作に対応した離散的な切り替えに限定されているため、演奏中の音色変化や強弱といった連続的な音楽表現が照明に十分反映されておらず、主観的な違和感が生じた点である。

次に、演奏中に照明操作を行う必要があるため、演奏と操作の両立が困難であり、演奏者に身体的および認知的負荷が生じる点が課題として挙げられる。

さらに、演奏と照明の制御系の間に起因する遅延により、演奏と照明のタイミングが主観的にずれて感じられる場面が確認された。


\subsection{設計方針} \label{sec:design-policy}

前節で述べた課題を踏まえ、本システムでは以下の設計方針を定めた。

\begin{enumerate}
	\item \textbf{演奏表現を連続的に反映可能な照明制御を実現すること}　\\
	    本研究では、演奏中の音量変化や音色の推移といった連続的な音楽表現を照明演出に反映することを重視する。
		そのため、単純な閾値処理やルールベース制御ではなく、音響特徴量と照明特徴量との対応関係を学習する機械学習モデルを導入した。
		この設計により、演奏内容に応じて照明の色、明るさ、点滅速度が連続的に変化し、演奏表現と視覚演出の一体化を図ることができる。
	\item \textbf{演奏者の操作を必要としない自律的な照明制御を行うこと}　\\
	    本システムでは、演奏者が演奏中に照明操作を行うことを前提としない設計とした。
		演奏音および演奏動作そのものを入力情報とすることで、演奏者が照明制御を意識することなく、演奏表現が自動的に照明へ反映される構成を目指している。
		これにより、演奏行為と照明制御を分離し、演奏への集中を妨げない照明演出を実現する.
	\item \textbf{リアルタイム性を重視した低遅延なシステム構成とすること}　\\
	    演奏と照明の一体感を確保するため、本研究ではリアルタイム性を重視したシステム設計を行った。
		具体的には、演奏音および演奏動作の取得から照明出力までの処理を高頻度で更新し、演奏と照明の時間的なずれを極力抑える構成とした。
		更新周期は約 10 ms とし、演奏表現の変化が即座に照明に反映されるよう設計している。
		一般に、人間は 20～30ms 未満の視覚的遅延を明確には知覚しにくいとされており\cite{Kadowaki19}、10ms 程度の更新周期であれば、演奏と照明がほぼ同時に変化しているように感じられると考えられる。
		この知見に基づき、演奏と照明の一体感を損なわない更新周期として 10ms を採用した。
\end{enumerate}

\subsection{システム概要}

\ref{sec:design-policy}節の設計方針に基づき、本研究ではヴァイオリン演奏に基づくリアルタイム照明制御システムを構築した。

本システムは、ヴァイオリン演奏に伴う音響特徴量および演奏動作情報を入力とし、機械学習モデルによって推定された照明特徴量を、舞台照明分野で標準的に用いられている DMX512 プロトコルを用いて照明機器へ出力する構成となっている(\figref{fig:system-structure})。

DMX512 は、照明機器のRGB値やBrightness、動作を数値データとして制御する一方向通信方式であり、1 チャンネルあたり 0～255 の値によって各制御パラメータを指定する。
本研究では、この DMX512 を用いることで、演奏に基づく制御信号を実際の照明機器に直接反映可能とし、現実のステージ環境に即したリアルタイム照明演出を実現している。

\begin{figure}[tb]
	\centering
	\includegraphics[width=\hsize]{../fig/system-structure.png}
	\caption{リアルタイム照明制御システム処理フロー図}\label{fig:system-structure}
\end{figure}

\subsection{ハードウェアおよびソフトウェア構成}

本システムは\tabref{tab:hardwea}に示すハードウェアで構成される。
マイクは演奏音をリアルタイムで取得するために使用し、スマートフォンはヴァイオリン演奏時の運弓動作を取得するために使用する。
照明機器はステージ周囲の地面に四方から配置し、床面から天井方向へ向けて、演奏者を中心に照射する構成とした（\figref{fig:setting}）。
\figref{fig:setting}において、ケーブルによる有線接続は実線で示し、無線通信は点線で示している。

\begin{table}[tb]
	\centering
	\small
	\caption{ハードウェア構成}\label{tab:hardwea}
	\begin{tabular}{c}
		\hline
		\hline
		PC（Windows 11）                        \\
		コンデンサマイク（演奏音入力）                       \\
		スマートフォン（演奏動作取得）                       \\
		USB-DMXインタフェース（\figref{fig:dmx-light}）\\
		DMX対応照明機器4台（\figref{fig:dmx-light}）   \\
		\hline
	\end{tabular}
\end{table}

\begin{figure}[tb]
	\centering
	\includegraphics[width=0.35\textwidth]{../fig/dmx-light.png}
	\caption{USB-DMXインタフェース（左）とDMX対応照明機器（右）}\label{fig:dmx-light}
\end{figure}

\begin{figure}[tb]
	\centering
	\includegraphics[width=\hsize]{../fig/setting.png}
	\caption{機材配置図（上面図）}\label{fig:setting}
\end{figure}

また、本システムはPythonで実装され、すべての処理を単一プログラム上で統合している。
なお、使用した主なライブラリは\tabref{tab:library}に示す通りである。

\begin{table}[tb]
	\centering
	\small
	\caption{使用ライブラリと役割}\label{tab:library}
	\begin{tabular}{c|c}
		\hline
		\hline
		ライブラリ名       & 役割         \\
		\hline
		sounddevice  & 演奏音取得      \\
		librosa      & 音響特徴量抽出    \\
		numpy        & 数値処理、データ管理 \\
		scikit-learn & 機械学習モデル推論  \\
		socket       & UDP通信      \\
		ftd2xx       & DMX信号出力    \\
		\hline
	\end{tabular}
\end{table}

\subsection{演奏音入力と音響特徴量抽出} \label{sec:sound}

まず、ヴァイオリンの演奏音は Python の sounddevice ライブラリを用いて1秒ごとに取得する。
音声入力には InputStream を使用し、サンプリング周波数を 22,050 Hz、モノラル入力として設定した。

次に、取得された 1 秒分の音声フレームに対して、librosa ライブラリを用いて音響特徴量の抽出を行う。
本研究では、ヴァイオリン演奏音に含まれる特徴を数値として表現するため、10種類の音響特徴量を算出している。
特徴量は「スペクトル系特徴量」、「時間領域系特徴量」、「MFCC系特徴量」、「ハーモニー系特徴量」に大別される。

\subsubsection{スペクトル系特徴量}

\begin{itemize}
	\item \textbf{スペクトル重心（Spectral Centroid）}　\\
	    スペクトル重心は、音の「明るさ」を表す特徴量である。
		高音成分が多く含まれる音ほど値が大きくなり、低音成分が中心の音では値が小さくなる。
		例えば、高い音域で演奏されたヴァイオリンの音ではスペクトル重心は大きくなり、低音域中心の楽器音では小さくなる。
	\item \textbf{スペクトル帯域幅（SpectralBandwidth）}　\\
	    スペクトル帯域幅は、音に含まれる周波数成分の「広がり」を表す特徴量である。
		さまざまな高さの音が同時に含まれている場合は値が大きくなり、単一の音高が明確な場合は値が小さくなる。
	\item \textbf{スペクトルコントラスト（SpectralContrast）}　\\
	    スペクトルコントラストは、音の中の「強い成分」と「弱い成分」の差を表す特徴量である。
		値が大きい場合はメリハリのある音であり、アクセントがはっきりしていることを示す。
		一方、値が小さい場合は全体的に均一で平坦な音である。
	\item \textbf{スペクトルフラットネス（SpectralFlatness）}　\\
	    スペクトルフラットネスは、音がどの程度ノイズ的であるかを示す特徴量である。
		値が大きい場合はホワイトノイズのようにザラザラした音であり、値が小さい場合は楽器音のように滑らかで音高が明確な音であることを示す。
		ヴァイオリン演奏においては、弓の擦れ音が強い場面などで変化が現れる。
	\item \textbf{スペクトルロールオフ（SpectralRolloff）}　\\
	    スペクトルロールオフは、音に含まれる高音成分がどのあたりまで存在しているかを示す特徴量である。
	    値が大きい場合は高音成分が多く含まれており、値が小さい場合は低音成分が中心の音であることを意味する。
\end{itemize}

\subsubsection{時間領域系特徴量}

\begin{itemize}
	\item \textbf{音量（Root Mean Square）}　\\
	    音量は、音の「大きさ」を表す特徴量である。
        値が大きいほど強い音であり、値が小さいほど弱く静かな音であることを示す。
	\item \textbf{ゼロ交差率}　\\
	    ゼロ交差率は、音の波形が上下に切り替わる回数を表す特徴量である。
		高速に変化する音やノイズ的な音では値が大きくなり、滑らかで安定した音では小さくなる。
\end{itemize}

\subsubsection{MFCC系特徴量}

\begin{itemize}
	\item \textbf{メル周波数ケプストラム係数（MFCC）}　\\
	    メル周波数ケプストラム係数は、人間の聴覚特性に近い形で音色を数値化した特徴量である。
       「人の声っぽい」「金属的」「柔らかい」といった音の質感を捉えることに優れており、複数の数値の組み合わせによって構成されている。
        一言で表すと、音の「質感」を表す特徴量である。
\end{itemize}

\subsubsection{ハーモニー系特徴量}

\begin{itemize}
	\item \textbf{クロマ特徴量（Chroma）}　\\
        クロマ特徴量は、12 音階（ド・レ・ミなど）それぞれの音の強さを表す特徴量である。
        どの音階がどの程度鳴っているかを示すため、和音や調性感の把握に用いられる。
    \item \textbf{トーナルネットワーク特徴量（Tonnetz）}　\\
        トーナルネットワーク特徴量は、音階同士の関係性を表す特徴量であり、和音構造や調性の傾向を捉えることができる。
        この特徴量により、明るい調（メジャー）か暗い調（マイナー）かを数値的に表現することが可能となる。
\end{itemize}

\subsection{演奏動作取得}

本システムでは、演奏者の弓の上下運動（上げ弓・下げ弓）（\figref{fig:up}）を取得するため、スマートフォンをZIG-SIMセンサとして利用している。
スマートフォンにはジャイロセンサが搭載されており、X軸方向の角速度を取得することで弓の前後運動を検出する。

スマートフォンは演奏者の右腕に装着し、弓を持つ手首付近（\figref{fig:up}の青丸部分）に配置する。
この位置に装着することで、弓の上下運動が腕の回転運動として反映されるため、ジャイロセンサで容易に検出可能となる。

右手首付近に装着したスマートフォンから取得したジャイロデータは、UDP通信を用いてPCに送信される。
UDPはTCPに比較して通信遅延が小さいため、本システムではリアルタイム性を重視してUDPを採用している。

\begin{figure}[tb]
	\centering
	\includegraphics[width=\hsize]{../fig/updown.png}
	\caption{上げ弓(左)と下げ弓(右)}\label{fig:up}
\end{figure}

PCに送信されたx軸のジャイロデータをもとに、弓方向の判定を行う。
弓方向の判定にあたっては、「角速度の値に基づく判定」と「角速度変化量に基づく判定」を併用している。

\subsubsection{角速度の値による運弓方向判定}

閾値を0.01に設定し、ジャイロデータが0.01より大きい場合は上げ弓、ジャイロデータが-0.01より小さい場合は下げ弓と判定している。

この値は理論的に決定したものではなく、実際の筆者の演奏動作を用いた試行錯誤によって決定した経験的な値である。
具体的には、以下の手順で検討を行った。

\begin{enumerate}
	\item \textbf{弓を完全に静止させた状態での計測}　\\
	    弓を完全に静止させた状態で数十秒間データを取得し、静止時に発生するジャイロセンサの角速度のばらつきを確認した。
	\item \textbf{非常にゆっくりと弓を動かした場合の計測}　\\
	    次に、弓を意図的に非常にゆっくり動かし、その際に観測される角速度を計測した。これにより、ノイズと実際の動作との境界付近の値を把握した。
	\item \textbf{通常の演奏速度における角速度分布を確認}　\\
        最後に、通常の演奏速度で弓を動かした際の角速度分布を確認し、実際の演奏動作における角速度の典型的な範囲を調査した。
\end{enumerate}

その結果、「静止時の角速度は概ね±0.005以下に収まる」一方で、「意図的な弓の動きでは±0.01を明確に超える」という、2つの傾向が確認された。

以上より、0.01 という値は「ノイズと実際の演奏動作を分離できる最小限の境界値」として妥当であると判断した。

\subsubsection{角速度変化量による運弓方向判定}

角速度変化量とは、「今の角速度が直前と比べてどれだけ変わったか」を表す値である。
本研究では、連続する 2 時刻におけるジャイロセンサ X 軸角速度の差を用いて角速度変化量を算出している。
この値が大きい場合、「弓が急に動き始めた」、あるいは「弓の方向が切り替わった」ことを意味する。

本研究では、角速度変化量が 0.09 を超えた場合に上げ弓、-0.09 を下回った場合に下げ弓と判定している。
この閾値も理論的に定めたものではなく、演奏時に取得したジャイロセンサの時系列データを用いた実験的検討により決定した。
具体的には、以下の手順で閾値の検討を行った。

\begin{enumerate}
	\item \textbf{演奏時ジャイロデータの取得}　\\
	    筆者自身が一定のテンポおよび強弱で演奏を行い、その際のジャイロセンサ X 軸角速度の時系列データを取得した。
	\item \textbf{複数閾値候補による運弓方向判定}　\\
	    取得したデータに対し、角速度変化量の閾値として 0.05～0.15 の複数の候補値を設定し、それぞれについて運弓方向判定を行った。
	\item \textbf{実際の演奏動作との比較評価}　\\
        各閾値における判定結果を実際の演奏動作と比較し、弓方向の切り替わり時にのみ角速度変化量が大きく変化するか、また微小な手振れやセンサノイズによる誤検出が最小となるかを評価した。
\end{enumerate}

その結果、0.09 を閾値とした場合に、弓方向の変化を最も安定して検出できることが確認された。
以上より、本研究では角速度変化量による運弓方向判定の閾値として 0.09 を最終的に採用した。

\subsection{機械学習モデルによる照明特徴量推定} \label{sec:light}

抽出された音響特徴量を用いて、機械学習モデルにより照明特徴量を推定する。
モデルは「RGB値」、「Brightness」、「点滅速度」の3つの照明特徴量を出力する。
推定値は0〜1の正規化値として出力され、DMX出力時に0〜255の整数値へスケーリングされる。
また、機械学習モデルの学習方法については次章で詳述する。

\subsection{DMX信号生成と照明制御}

本節では、機械学習モデルおよび演奏動作推定によって得られた照明特徴量を、実際の照明機器に反映させるためのDMX信号生成および制御方法について詳細に述べる。

DMX信号の出力には、USB接続のDMXインタフェース（\figref{fig:dmx-light}・左）を使用した。
本研究ではFTDI社製チップを搭載したデバイスを用い、Pythonからftd2xxライブラリを介して制御を行っている。
また、本システムでは、4台の照明機器を同時に制御する構成とした。

\subsubsection{照明特徴量からDMX値への変換}

照明特徴量（RGB値,Brightness,点滅速度）は、DMXチャンネルに対応する値へ変換される(\tabref{tab:channel})。

これらは連続値として出力されるため、DMXチャンネルに割り当てる際には0〜255の整数値へ正規化・クリッピング処理を行う。
具体的には、各値に対して以下の処理を適用する。

\begin{enumerate}
	\item \textbf{正規化：}各特徴量の最小値・最大値を基に0〜1の範囲にスケーリング
	\item \textbf{スケーリング：}0〜1の値を255倍して0〜255の範囲に変換
	\item \textbf{クリッピング：}小数点以下を切り捨て、整数値に変換
\end{enumerate}

\begin{table}[tb]
	\centering
	\small
	\caption{チャンネルの割り当て}\label{tab:channel}
	\begin{tabular}{c|c}
		\hline
		\hline
		チャンネル & 役割         \\
		\hline
		CH1   & 未使用        \\
		CH2   & 上下動作       \\
		CH3   & 点滅速度       \\
		CH4   & R成分        \\
		CH5   & G成分        \\
		CH6   & B成分        \\
		CH7   & Brightness \\
		\hline
	\end{tabular}
\end{table}

\subsubsection{演奏動作情報のDMX値への変換}

音響特徴量から推定された照明特徴量とは別に、ZIG-SIMによって取得した運弓方向情報をDMX制御に直接反映させている。
具体的には、運弓方向に応じて上下動作のDMXチャンネル（チャンネル2）の値を変更し、照明の上下動作を演奏者の弓の動きに同期させている。

\begin{itemize}
	\item \textbf{上げ弓：}DMX値を120に設定（チルト約60度上昇）（\figref{fig:lightupdown}参照）
	\item \textbf{下げ弓：}DMX値を0に設定（チルト約60度下降）（\figref{fig:lightupdown}参照）
\end{itemize}
とすることで、演奏者の身体動作と照明の物理的動きを同期させている。
この設計により、音だけでなく演奏動作そのものが照明演出に影響を与える構造となっている。

\begin{figure}[tb]
	\centering
	\includegraphics[width=0.2\textwidth]{../fig/lightupdown.png}
	\caption{照明上下動作}\label{fig:lightupdown}
\end{figure}

\subsection{音響特徴量と照明特徴量の対応関係} \label{sec:analysis}

これまでに述べたように、本システムでは、音響特徴量から照明特徴量を予測する機械学習モデルを用いて照明の制御を行っている。
本節では、これらの音響特徴量と照明特徴量の間にどのような対応関係が見られるのかを整理し、分析する。

これらの対応関係を解析するため、本研究では Random Forest における特徴量重要度を用いた。
Random Forest では、各決定木において分岐に使用された特徴量がどれだけ予測誤差の低減に寄与したかを基に、特徴量重要度が算出される。
この値は、モデルが予測を行う際にどの特徴量を重視しているかを示す指標である。

本研究では、照明特徴量ごとに個別の Random Forest 回帰モデルを学習し、それぞれの出力に対して音響特徴量の重要度を算出し、寄与度の高い上位5特徴量を抽出した。
これにより、照明のRGB値、Brightness、点滅速度といった各要素が、どの音響特徴量と強く結びついているかを独立に分析できる。
結果の可視化には円グラフを用い、各特徴量が予測にどの程度寄与しているかを直感的に把握できるようにした。

\noindent \textbf{(1) 音響特徴量と照明色（R成分）の対応関係}

解析の結果、$\mathrm{RMS}$（音量）が最も高い重要度を示し、次いで $\mathrm{Tonnetz\_6}$、$\mathrm{MFCC\_5}$、$\mathrm{MFCC\_19}$、$\mathrm{Spectral Contrast}$ の順となった（\figref{fig:rlightstyle}）。

\begin{figure}[tb]
	\centering
	\includegraphics[width=\hsize]{../fig/rlightstyle.png}
	\caption{照明色（R成分）に対する音響特徴量}
	\label{fig:rlightstyle}
\end{figure}

R成分の予測においてはRMSが最も高い重要度を示した。このことから、赤色成分の強度は音量の大きさに強く影響されていることが分かる。

一方で、 $\mathrm{Tonnetz\_6}$ や$\mathrm{MFCC\_5}$ 、 $\mathrm{MFCC\_19}$ といった特徴量も比較的高い重要度を示しており、R成分は音量だけでなく、音楽的構造や音色的要素も反映して決定されていることが示唆される。
また、Spectral Contrast も上位に含まれていることから、音の輪郭の明瞭さが赤色成分の強調に一定の影響を与えていると考えられる。

\noindent \textbf{(2) 音響特徴量と照明色（G成分）の対応関係}

\begin{figure}[tb]
	\centering
	\includegraphics[width=\hsize]{../fig/glightstyle.png}
	\caption{照明色（G成分）に対する音響特徴量}
	\label{fig:glightstyle}
\end{figure}

解析の結果、RMS が全体の約86.8\% と極めて高い重要度を示し、他の特徴量（$\mathrm{Tonnetz\_6}$、$\mathrm{Chroma\_7}$、$\mathrm{MFCC\_5}$、$\mathrm{Spectral Contrast}$）はいずれも1〜7\%程度に留まった（\figref{fig:glightstyle}）。

G成分において RMS が圧倒的な重要度を示したことから、緑色成分の強度は他の要因に比べて音量にほぼ支配的に決定されていることが明らかとなった。

RMS に次いで $\mathrm{Tonnetz\_6}$や$\mathrm{Chroma\_7}$ 、 $\mathrm{MFCC\_5}$ が上位に含まれているものの、それらの寄与率は比較的小さく、G成分は主として音圧変化を直接反映する成分として機能していると解釈できる。

\noindent \textbf{(3) 音響特徴量と照明色（B成分）の対応関係}

解析の結果、RMS が約70.2\% と最も高い重要度を示した。
次いで  $\mathrm{MFCC\_19}$（約14.9\%）が比較的高い寄与を示し、 $\mathrm{Chroma\_7}$、$\mathrm{Tonnetz\_6}$、$\mathrm{MFCC\_5}$ はそれぞれ約5\%前後であった（\figref{fig:blightstyle}）。

\begin{figure}[tb]
	\centering
	\includegraphics[width=\hsize]{../fig/blightstyle.png}
	\caption{照明色（B成分）に対する音響特徴量}
	\label{fig:blightstyle}
\end{figure}

B成分においても RMS が最も支配的な特徴量であることから、青色照明も音量変化に大きく依存していることが分かる。
音量が大きい場面では青色成分が強調され、音量が小さい場面では抑制される傾向をモデルが学習していると考えられる。
ただし、G成分と比較すると RMS の割合はやや低下しており、B成分では音量以外の要素も一定程度考慮されていることが示唆される。

$\mathrm{MFCC\_19}$ が約15\%と比較的高い重要度を示した点は、B成分が音色や高次スペクトル成分に影響されやすいことを示している。
$\mathrm{Chroma\_7}$ および $\mathrm{Tonnetz\_6}$ が上位に含まれていることから、B成分は音量や音色だけでなく、音楽的構造とも関連していることが分かる。

\noindent \textbf{(4) 音響特徴量とBrightnessの対応関係}

解析の結果、RMS が約77.8\% と最も高い重要度を示した。次いで $\mathrm{Tonnetz\_6}$（約10.4\%）、$\mathrm{MFCC\_19}$（約5.3\%）、$\mathrm{MFCC\_5}$（約3.7\%）、$\mathrm{Spectral Contrast}$（約2.8\%）が続いた（\figref{fig:brightnessstyle}）。

RMS が約8割近い寄与率を示したことから、照明の明るさは主に音量に基づいて決定されていることが明らかとなった。
本モデルでは、演奏音が強くなるにつれて照明が明るくなり、弱くなるにつれて暗くなるという対応関係が学習されている。

\begin{figure}[tb]
	\centering
	\includegraphics[width=\hsize]{../fig/brilightstyle.png}
	\caption{Brightnessに対する音響特徴量}
	\label{fig:brightnessstyle}
\end{figure}

RMS に次いで $\mathrm{Tonnetz\_6}$の重要度が高かった。
この結果は、Brightnessが単純な音量変化だけでなく、音楽的な構造要素にも影響を受けていることを示している。
また、$\mathrm{MFCC\_19}$ および $\mathrm{MFCC\_5}$ が上位に含まれていることから、Brightnessは音量だけでなく、音の質感や倍音構成にも影響されていることが分かる。

\noindent \textbf{(5) 音響特徴量と照明の点滅速度の対応関係}

解析の結果、$\mathrm{Chroma\_7}$ が約30.6\% と最も高い重要度を示し、次いで $\mathrm{MFCC\_5}$（約21.9\%）、$\mathrm{Chroma\_1}$（約19.7\%）、$\mathrm{MFCC\_1}$（約15.6\%）、RMS（約12.1\%）という順となった（\figref{fig:blinkstyle}）。

\begin{figure}[tb]
	\centering
	\includegraphics[width=\hsize]{../fig/blinklightstyle.png}
	\caption{照明の点滅速度に対する音響特徴量}
	\label{fig:blinkstyle}
\end{figure}

点滅速度において最も大きな影響を与えているのは Chroma 系特徴量であり、特に $\mathrm{Chroma\_7}$ および $\mathrm{Chroma\_1}$ が全体の約50\%を占めている。
この結果は、照明の点滅が音量の大小よりも、音楽の和声的・音高的構造に強く依存していることを示している。

Chroma の変化が大きい場面では、照明の点滅が速くなり、音楽的な動きや緊張感を視覚的に強調する演出が行われていると考えられる。
$\mathrm{MFCC\_5}$ および $\mathrm{MFCC\_1}$ も比較的高い重要度を示しており、点滅速度が音色の変化にも影響を受けていることが分かる。

RMS は点滅速度においても一定の影響を持つものの、その重要度は約12.1\%に留まっている。
これは、照明の明るさとは異なり、点滅という時間的変化の制御においては、音量そのものよりも音楽的構造や音高変化が重視されていることを示唆している。

このように、RGB 各成分およびBrightnessでは RMS の重要度が支配的であったのに対し、点滅速度では Chroma や MFCC といった音楽的特徴量が主導的な役割を果たしている。
この違いは、照明特徴量ごとに参照される音響情報の種類が異なることを明確に示している。


\section{照明予測モデル}\label{sec:model}

本章では、\ref{sec:light}節で述べたヴァイオリン演奏音から照明特徴量を予測する機械学習モデルの構築方法について詳述する。

本研究では、音響特徴量を入力とし、照明特徴量を予測するモデルを用いて照明の制御を行っている。
最終的に構築されたモデルは、リアルタイムの演奏音を入力として照明を自動制御する「照明予測モデル」として機能する。
照明予測モデルの構築は以下の生成フローから成り立っている。

\begin{enumerate}
	\item \textbf{データベースの構築（照明予測モデル$\beta$）}
	\item \textbf{照明予測モデル$\beta$の学習}
	\item \textbf{データベースの構築（照明予測モデル）}
	\item \textbf{照明予測モデルの学習}
\end{enumerate}

また、\figref{fig:dataflow}に、照明予測モデル構築のためのデータ生成フローを示す。
この図に基づき、各ステップを詳細に説明する。

\begin{figure}[tb]
	\centering
	\includegraphics[width=\hsize]{../fig/dataflow.png}
	\caption{照明予測モデル構築のためのデータ生成フロー}\label{fig:dataflow}
\end{figure}

\subsection{データベースの構築（照明予測モデル$\beta$）} \label{sec:database}

本研究では、最終的にリアルタイム照明制御に用いる照明予測モデルを構築する前段階として、照明予測モデル$\beta$を学習する。

照明予測モデル$\beta$は、実際のヴァイオリン演奏におけるライブ映像から取得した実測データのみを用い、音響特徴量から照明特徴量の予測を行うモデルである。
しかし、本研究で学習に用いるライブ演奏映像は 3 種類と限られており、このデータのみでは十分な学習データ量を確保することが難しい。
そこで、本研究では、照明予測モデル$\beta$を照明予測モデルの学習のための中間データとして位置付けた。
具体的な説明は\ref{sec:midi}節にて後述する。

まず本節では、この照明予測モデル$\beta$を学習させるために構築したデータベースについて述べる。

\subsubsection{映像データの収集}

\begin{figure}[tb]
	\centering
	\includegraphics[width=\hsize]{../fig/senbonzakura.png}
	\caption{『千本桜』の演奏映像}
	{\small 出典：\cite{Ayakoishikawatv21}}
	\label{fig:senbonzakura}
\end{figure}

\begin{figure}[tb]
	\centering
	\includegraphics[width=\hsize]{../fig/sonata.png}
	\caption{『ソナタ第三番「バラード」』の演奏映像}
	{\small 出典：\cite{aiolinofficialLIVEHikaritoViolinSolo2019}}
	\label{fig:sonata}
\end{figure}

\begin{figure}[tb]
	\centering
	\includegraphics[width=\hsize]{../fig/splash.png}
	\caption{『スプラッシュ』の演奏映像}
	{\small 出典：\cite{violinistshogoViolinistSHOGOSorosutezi2017}}
	\label{fig:splash}
\end{figure}

まずは、データソースとして、実際のヴァイオリンソロ演奏におけるライブ映像データを用意した。
これらは実際の照明演出を伴う演奏映像であり、音楽と照明の対応関係を直接的に取得できる点で有用である。
本研究では、入手可能な映像素材が限られているという制約のもと、音質および照明演出が比較的明瞭に記録されており、学習データとして十分な情報量を有すると判断された以下の3種類の映像を選定した。

\begin{itemize}
  \item \textbf{曲目：}千本桜(\figref{fig:senbonzakura})　\\
      \textbf{演奏時間：}3分50秒
  \item \textbf{曲目：}ヴァイオリンソナタ第三番「バラード」(\figref{fig:sonata})
      \textbf{演奏時間：}2分31秒
  \item \textbf{曲目：}スプラッシュ(\figref{fig:splash})　\\
      \textbf{演奏時間：}4分12秒
\end{itemize}

これらの映像に対して、音響特徴量と照明特徴量を抽出するための処理を行い、照明予測モデル$\beta$の学習データを生成する。

\subsubsection{音響特徴量の抽出} \label{sec:soundmodel}

\noindent \textbf{(1) ヴァイオリン音源の分離}

ライブ映像には、会場の反響音、観客ノイズ、伴奏音など、ヴァイオリン以外の音が含まれる。
音響特徴量は不要な音が混入したまま学習するとモデルの精度劣化を招く。

そこで音源分離モデルDemucs\cite{FacebookresearchDemucs2026}を用い、音声からヴァイオリン成分を抽出した。

\noindent \textbf{(2) ヴァイオリン特徴量の抽出}

各映像から抽出したヴァイオリン音声を音声ファイルとして保存し、それらに対して1秒ごとに音響特徴量を算出した。

まずは、音声ファイルを Pythonのlibrosaライブラリ を用いて読み込む。
読み込んだ音声データから音声全体の長さ（秒）を算出し、その長さに応じて 0 秒から終端まで 1 秒刻みのセグメントに分割する。
そして、各 1 秒セグメントごとに\ref{sec:sound}節で述べた音響特徴量と同様の特徴量を算出する。
算出された特徴量は CSV 形式で保存され、学習用 CSV データとして構築される。

\subsubsection{照明特徴量の抽出} \label{sec:lightmodel}

続いて、各映像からRGB値、Brightness、点滅速度の 3 種類の照明特徴量を1秒ごとに抽出し、学習用CSVデータとして構築した。
本節では、照明特徴量の抽出手法について詳述する。

\begin{table*}[t]
\centering
\small
\caption{各映像の音響特徴量と照明特徴量のCSVデータ(一部抜粋)}
\label{tab:csv-live}
\begin{tabular}{c|cccccccccc}
\hline
\multicolumn{11}{c}{千本桜} \\
\hline
\hline
Time [s] & Centroid & Contrast & RMS &　MFCC\_1 & Chroma\_1 & R & G & B
 & Brightness & Blink Speed  \\
\hline
1.0 & 8894 & 14.5 & 0.001 & -948.6 & 0.67 & 90.1 & 45.7 & 62.1 & 59.8 & 0.40 \\
2.0 & 6722 & 17.9 & 0.005 & -661.4 & 0.76 & 93.9 & 60.2 & 74.1 & 69.3 & 0.33 \\
3.0 & 5257 & 23.2 & 0.033 & -385.2 & 0.57 & 89.8 & 60.7 & 70.0 & 87.0 & 0.29 \\
4.0 & 4459 & 21.7 & 0.023 & -398.0 & 0.75 & 84.2 & 42.5 & 53.6 & 53.5 & 0.25 \\
5.0 & 3987 & 19.5 & 0.028 & -358.8 & 0.28 & 84.8 & 32.2 & 45.8 & 47.7 & 0.44 \\
6.0 & 3678 & 18.3 & 0.050 & -347.5 & 0.37 & 130.6 & 16.2 & 31.1 & 53.3 & 0.40 \\
7.0 & 3521 & 17.8 & 0.039 & -410.6 & 0.30 & 125.8 & 126.4 & 136.2 & 56.7 & 0.40 \\
8.0 & 3400 & 16.7 & 0.008 & -471.5 & 0.43 & 124.7 & 35.9 & 38.0 & 63.3 & 0.20 \\
\hline
\multicolumn{11}{c}{ソナタ第三番"バラード"} \\
\hline
\hline
Time [s] & Centroid & Contrast & RMS &　MFCC\_1 & Chroma\_1 & R & G & B
 & Brightness & Blink Speed  \\
\hline
1.0 & 1947 & 9.04 & 0.001 & -1068.0 & 0.04 & 50.8 & 45.1 & 68.7 & 50.2 & 0.40 \\
2.0 & 2898 & 20.1 & 0.000 & -809.0 & 0.12 & 52.3 & 45.6 & 69.7 & 49.8 & 0.33 \\
3.0 & 1695 & 19.9 & 0.001 & -675.9 & 0.35 & 54.5 & 46.9 & 71.6 & 52.9 & 0.29 \\
4.0 & 1560 & 20.1 & 0.002 & -618.0 & 0.43 & 53.9 & 46.2 & 70.7 & 51.1 & 0.25 \\
5.0 & 1728 & 19.1 & 0.002 & -606.8 & 0.56 & 65.0 & 58.0 & 81.3 & 63.2 & 0.22\\
6.0 & 1887 & 19.5 & 0.002 & -604.9 & 0.28 & 120.8 & 119.2 & 131.2 & 124.0 & 0.20 \\
7.0 & 2113 & 19.0 & 0.002 & -585.3 & 0.22 & 128.8 & 126.4 & 136.2 & 130.9 & 0.20 \\
8.0 & 2475 & 19.9 & 0.002 & -602.9 & 0.08 & 139.3 & 137.1 & 145.3 & 140.5 & 0.20 \\
\hline
\multicolumn{11}{c}{スプラッシュ} \\
\hline
\hline
Time [s] & Centroid & Contrast & RMS &　MFCC\_1 & Chroma\_1 & R & G & B
 & Brightness & Blink Speed  \\
\hline
1.0 & 1002 & 0.00 & 0.000 & -1131.0 & 0.08 & 91.6 & 70.0 & 76.5 & 77.5 & 0.40 \\
2.0 & 3582 & 8.98 & 0.008 & -1055.0 & 0.27 & 54.1 & 36.1 & 50.5 & 35.3 & 0.33 \\
3.0 & 1603 & 2.24 & 0.001 & -1109.0 & 0.16 & 17.8 & 11.4 & 23.9 & 19.2 & 0.29 \\
4.0 & 1564 & 15.0 & 0.032 & -774.5 & 0.23 & 45.1 & 36.0 & 66.9 & 43.4 & 0.25 \\
5.0 & 1798 & 18.4 & 0.054 & -641.0 & 0.37 & 239.0 & 238.1 & 242.0 & 242.3 & 0.22 \\
6.0 & 1893 & 20.1 & 0.055 & -667.7 & 0.30 & 189.4 & 201.3 & 208.0 & 200.0 & 0.20 \\
7.0 & 2060 & 20.1 & 0.057 & -634.1 & 0.25 & 191.6 & 223.9 & 233.7 & 215.8 & 0.20 \\
8.0 & 759.7 & 19.8 & 0.062 & -621.3 & 0.25 & 200.0 & 203.9 & 199.8 & 202.8 & 0.20 \\
\hline
\end{tabular}
\end{table*}

まずは、照明特徴量の抽出における映像への前処理として、YOLOv8\cite{YOLOv8StateoftheArtComputer} を用いた人物領域の除外を行う。
人物領域を含んだままRGB値やBrightnessを算出すると、肌色や衣服の色、演奏者の動きによる輝度変化が混入し、照明そのものの変化を正確に捉えることができない。

そこで本研究では、人物検出後に膨張処理を施し、演奏者周囲も含めて背景から除外した。
そのうえで、背景画素のうち明度上位10\%のみを抽出し、照明器具に最も近い画素群を対象としてRGB値およびBrightnessを算出している。

\begin{enumerate}
	\item \textbf{RGB値の算出と各映像のCSVデータ}　\\
        映像のフレーム画像を B,G,R チャンネルに分解し、背景マスクかつ上位10\%領域に該当する画素を抽出し、それぞれの平均値を計算した。
        これらの値をフレーム単位でバッファに蓄積し、1秒ごとに平均を取ることで、1秒単位のRGB値の照明特徴量としてCSVに書き出した。
	\item \textbf{Brightnessの算出と各映像のCSVデータ}　\\
	    Brightnessは、グレースケール画像（または HSVのV成分）から、同じく上位10\%の明度領域を抽出して平均値を求めた。
		また、RGB値と同様に、1秒ごとに平均を取ることで、1秒単位のBrightness値としてCSVに書き出した。
    \item \textbf{点滅速度の算出と各映像のCSVデータ}　\\
        照明の点滅速度を抽出するために、1秒ごとの平均Brightness値を時系列信号として蓄積し、
        前後5秒の局所FFT を行うことで点滅に対応する周波数成分を推定し、CSVに書き出した。
\end{enumerate}

\subsubsection{学習用CSVデータの構築} \label{sec:csvoutput}

このようにして、各映像から音響特徴量と照明特徴量のペアを1秒ごとに抽出し、「時間（秒）」、「音響特徴量」、「照明特徴量」を学習用CSVデータとして出力した（\tabref{tab:csv-live}）。

\subsection{照明予測モデル$\beta$の学習} \label{sec:modelthink}

\ref{sec:database}節で収集した学習用CSVデータを用いて、照明予測モデル$\beta$の学習を行った。

\begin{itemize}
	\item \textbf{入力：}
        照明予測モデル$\beta$への入力として使用した音響特徴量のデータセットは\ref{sec:soundmodel}項で述べた通りである。
    \item \textbf{出力：}
        照明予測モデル$\beta$の出力として学習した照明特徴量のデータセットは\ref{sec:lightmodel}項で述べた通りである。
    \item \textbf{学習方法：}
        照明予測モデル$\beta$には、回帰モデルとして Random Forest Regressorを採用した。
        Random Forest は複数の決定木を組み合わせたアンサンブル学習手法である。
        決定木の本数は100とし、学習精度と計算コストのバランスを考慮した。
	\item \textbf{学習の実行：}
        データセットを用いて Random Forest モデルを学習し、音響特徴量と照明特徴量の対応関係を獲得した。
\end{itemize}

\subsection{データベースの構築（照明予測モデル）} \label{sec:midi}

本節では、照明予測モデルを学習するためのデータベースの構築手法について詳述する。

本研究では、約600種類のMIDIデータを音声信号へ変換した上で、音響特徴量を抽出し、照明予測モデル$\beta$によって照明特徴量を推定するという処理フローを採用した。
これにより、MIDIデータから得られた音響特徴量と対応する照明特徴量のデータベースを構築することができる。

\subsubsection{MIDIデータを用いる目的}

まずは、演奏音の多様性を拡張するためである。
MIDIデータには、音高変化、フレーズ構造、音価の違いなど、実演奏に近い音楽的情報が含まれている。
そのため、MIDIデータを用いることで、ライブ映像データのみでは不足しがちな演奏パターンを補完し、演奏音のバリエーションを大幅に増加させることができる。

次に、モデルの汎化性能を向上させるためである。
特定の演奏映像や照明環境に学習が偏ると、未知の演奏音に対する照明予測性能が低下する恐れがある。
そこで、MIDIデータを用いて多様な音響特徴量を学習させることで、モデルが特定条件に過度に依存することを防ぎ、未知の演奏音に対しても安定した照明予測を可能にする。

\subsubsection{MIDIデータセットの概要}

本研究で使用したMIDIデータは、ヴァイオリン演奏において代表的かつ教育的価値の高い楽曲群から構成されている。
具体的には、「Paganini\cite{Paganini24Violin}」、「Kayser\cite{36ViolinStudies}」、「Wohlfahrt\cite{FreeSheetMusic}」の作品を中心としたヴァイオリンソロのMIDIデータを用いた。
各作品の特徴は以下の通りである。

\begin{itemize}
	\item \textbf{Paganini：}
	    高度な演奏技巧や急激な音高変化、速いパッセージを多く含み、演奏表現の幅が非常に広い。
	\item \textbf{Kayser・Wohlfahrt：}
        音階練習や基本的な運弓・音程感覚の習得を目的とした楽曲が多く、比較的安定した音高変化と明瞭なフレーズ構造を持つ。
\end{itemize}

これらのMIDIデータは、音楽教育において広く使用されており、ヴァイオリン演奏の典型的な音域・音色変化・フレーズ構造を網羅している点で、本モデルのデータソースとして有用である。

\subsubsection{音響特徴量の抽出} \label{sec:otomidi}

MIDIデータは直接音響特徴量を算出することができないため、音声信号へ変換する必要がある。
本研究では、ソフトウェアシンセサイザー FluidSynth\cite{FluidSynthSoftwareSynthesizer}を用い、MIDIファイルをWAV形式へ変換した。

生成されたWAVファイルに対し、Pythonライブラリ librosa を用いて音響特徴量を抽出した。
抽出する特徴量の種類および構成は、\ref{sec:modelthink}節で構築した照明予測モデル$\beta$の学習時に用いたデータセットと完全に一致させている。

\begin{table*}[t]
\centering
\small
\caption{MIDIデータの音響特徴量と推定された照明特徴量のCSVデータ（一部抜粋）}\label{tab:csvmidi}
	\begin{tabular}{c|cccccccccc}	
		\hline
		\multicolumn{11}{c}{Wohlfahrt\_Op45-40\_BernardChevalier\_ijMGhEYjP98-0000-0057}                         \\
		\hline
		\hline
		Time [s] & Centroid   & Contrast    & RMS   & MFCC\_1 & Chroma\_1 & R     & G     & B
		         & Brightness & Blink Speed                                                                      \\
		\hline
		1.0      & 1609       & 47.2        & 0.000 & -751.4  & 0.65      & 118.0 & 119.4 & 142.9 & 123.2 & 0.23 \\
		2.0      & 2760       & 15.7        & 0.002 & -651.4  & 0.09      & 76.1  & 60.0  & 91.5  & 67.7  & 0.24 \\
		3.0      & 2827       & 14.8        & 0.001 & -632.2  & 0.14      & 93.2  & 82.7  & 104.9 & 88.7  & 0.28 \\
		4.0      & 2389       & 13.0        & 0.002 & -637.3  & 0.14      & 81.2  & 66.6  & 102.4 & 75.0  & 0.23 \\
		5.0      & 1978       & 14.7        & 0.001 & -630.9  & 0.04      & 85.2  & 81.8  & 113.6 & 87.5  & 0.24 \\
		6.0      & 1900       & 14.2        & 0.001 & -671.0  & 0.04      & 88.9  & 88.7  & 114.8 & 92.6  & 0.23 \\
		7.0      & 2177       & 15.3        & 0.002 & -640.0  & 0.00      & 90.0  & 71.8  & 98.5  & 80.4  & 0.26 \\
		8.0      & 2832       & 15.6        & 0.002 & -634.9  & 0.13      & 75.8  & 52.8  & 81.5  & 62.8  & 0.24 \\
		\hline
		\multicolumn{11}{c}{Paganini\_Op01-23\_ItzhakPerlman\_aP15fyVkPVg-0000-0284}                             \\
		\hline
		\hline
		Time [s] & Centroid   & Contrast    & RMS   & MFCC\_1 & Chroma\_1 & R     & G     & B
		         & Brightness & Blink Speed                                                                      \\
		\hline
		1.0      & 2280       & 31.7        & 0.002 & -676.8  & 0.36      & 95.4  & 76.2  & 115.9 & 87.4  & 0.23 \\
		2.0      & 2837       & 12.5        & 0.008 & -544.3  & 0.00      & 89.0  & 66.7  & 97.4  & 76.5  & 0.26 \\
		3.0      & 1993       & 12.9        & 0.005 & -565.4  & 0.14      & 78.9  & 56.3  & 86.7  & 66.3  & 0.26 \\
		4.0      & 2060       & 13.0        & 0.007 & -533.0  & 0.05      & 74.5  & 52.9  & 81.5  & 62.3  & 0.23 \\
		5.0      & 2804       & 12.8        & 0.007 & -552.2  & 0.00      & 90.5  & 65.8  & 102.8 & 77.4  & 0.24 \\
		6.0      & 2492       & 12.7        & 0.009 & -532.7  & 0.02      & 83.5  & 60.9  & 89.5  & 70.8  & 0.25 \\
		7.0      & 1746       & 13.3        & 0.004 & -572.1  & 0.15      & 76.2  & 51.9  & 82.2  & 62.5  & 0.23 \\
		8.0      & 2512       & 13.2        & 0.007 & -533.6  & 0.02      & 77.0  & 54.7  & 91.5  & 65.5  & 0.23 \\
		\hline
		\multicolumn{11}{c}{Kayser\_Op20-09\_BernardChevalier\_j1AYnoTq0RM-0001-0100}                            \\
		\hline
		\hline
		Time [s] & Centroid   & Contrast    & RMS   & MFCC\_1 & Chroma\_1 & R     & G     & B
		         & Brightness & Blink Speed                                                                      \\
		\hline
		1.0      & 1736       & 23.3        & 0.031 & -657.4  & 0.51      & 85.5  & 89.2  & 124.9 & 93.2  & 0.24 \\
		2.0      & 1479       & 12.0        & 0.033 & -587.6  & 0.54      & 79.2  & 62.8  & 92.7  & 71.3  & 0.23 \\
		3.0      & 1481       & 13.3        & 0.033 & -565.4  & 0.41      & 94.2  & 74.4  & 106.3 & 84.3  & 0.22 \\
		4.0      & 2080       & 12.2        & 0.059 & -540.0  & 0.18      & 78.0  & 58.9  & 91.2  & 68.4  & 0.23 \\
		5.0      & 2246       & 14.7        & 0.062 & -587.9  & 0.47      & 85.1  & 65.6  & 97.0  & 75.0  & 0.23 \\
		6.0      & 2281       & 14.8        & 0.064 & -582.4  & 0.47      & 95.5  & 71.8  & 111.3 & 84.7  & 0.23 \\
		7.0      & 2536       & 17.7        & 0.084 & -571.8  & 0.16      & 84.4  & 59.5  & 96.6  & 71.5  & 0.24 \\
		8.0      & 2984       & 16.4        & 0.097 & -567.1  & 0.35      & 86.8  & 62.7  & 104.5 & 74.6  & 0.23 \\
		\hline
	\end{tabular}
\end{table*}

\subsubsection{照明予測モデル$\beta$による照明特徴量推定} \label{sec:lightmidi}

抽出された音響特徴量は、\ref{sec:modelthink}節で構築した 照明予測モデル$\beta$に入力される。
本モデルは、ライブ映像データから学習した「音響特徴量と照明特徴量の対応関係」を保持しており、MIDI由来の音響特徴量に対しても照明特徴量を推定することが可能である。
推定される照明特徴量は、RGB値、Brightness、点滅速度の3種類である。

\subsubsection{学習用CSVデータの構築}

各MIDIファイルについて、「時間（秒）」、「音響特徴量」、「照明予測モデル$\beta$による推定照明特徴量」を1つのCSVデータとして出力した（\tabref{tab:csvmidi}）。

\subsection{照明予測モデルの学習} \label{sec:midimodel}

本節では、\ref{sec:midi}節で生成したデータベースをもとに、照明予測モデルの学習を行った。

\begin{itemize}
	\item \textbf{入力：}
		照明予測モデルへの入力として使用した音響特徴量のデータセットは\ref{sec:otomidi}項で述べた通りである。
	\item \textbf{出力：}
		照明予測モデルの出力として学習した照明特徴量のデータセットは\ref{sec:lightmidi}項で述べた通りである。
	\item \textbf{学習方法：}
		学習手法としては、\ref{sec:modelthink}節と同様に Random Forest Regressor を採用した。
        モデル構造や主要なハイパーパラメータも、同一に設定している。
        データセットは、モデルの汎化性能を評価するため、全体の80\%を学習用データ、20\%をテスト用データに分割した。
        分割はランダムに行い、学習用データとテスト用データが重複しないようにした。
	\item \textbf{学習の実行：}
		学習用データを用いて Random Forest モデルの学習を実行し、音響特徴量と照明特徴量の対応関係を獲得した。
        学習が完了したモデルは、 joblib を用いてファイルとして保存し、リアルタイム照明制御システムに直接組み込んだ。
\end{itemize}

\subsection{照明予測モデルの評価}
\begin{itemize}
	\item \textbf{評価指標：}
	    モデルの評価には、機械学習における回帰問題で一般的に用いられる決定係数（$R^2$スコア）を採用した。
        $R^2$スコアは、モデルの予測値が実測値の分散をどの程度説明できているかを示す指標であり、以下の式で定義される。
        \[
	    \text{決定係数 } R^2
	     = 1 - \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}
	    {\sum_{i=1}^{n}(y_i - \bar{y})^2}
        \]

        \[
		y_i : \text{実測値} \quad
		\hat{y}_i : \text{予測値} \quad
		\bar{y} : \text{実測値の平均}
		\]
		$R^2$スコアは 1 に近いほど予測精度が高く、0 は平均値予測と同等、負の値は平均予測より劣ることを示す。
	\item \textbf{評価データ：}
	    評価には、学習時に使用していないテスト用データを用いた(\ref{sec:midimodel}節参照)。
    \item \textbf{評価手順：}
		学習済みの照明予測モデルに対し、テスト用データの音響特徴量を入力として与え、対応する照明特徴量を予測させた。
		その後、予測された照明特徴量と実測値を比較し、$R^2$スコアを計算した。
	\item \textbf{評価結果：}
        照明予測モデルに対する評価の結果、$R^2$スコアは 0.85 を示した（\figref{fig:r2-result}）。
        この結果は、音響特徴量から照明特徴量への変換において、モデルが照明データの分散の約85\%を説明できていることを意味する。
        すなわち、照明のRGB値、Brightness、点滅速度といった複合的な要素を、演奏音のみから高精度に推定できていることが確認された。
\end{itemize}

\begin{figure}[tb]
	\centering
	\includegraphics[width=\hsize]{../fig/r2-result.png}
	\caption{決定係数（$R^2$）の結果}\label{fig:r2-result}
\end{figure}


\section{実演} \label{sec:play}

本章では、本研究で構築したリアルタイム照明制御システムを用いて実施した実演について述べる。
本研究では、異なる楽曲および異なる環境条件下におけるシステムの適応性と表現の妥当性を検討するため、計2回の実演を実施している。
1回目の実演は、大学教室において同ゼミ生(約20名)を対象に実施し、2回目の実演は、新町商店街において一般の観客を伴う公開実演として実施した。

\subsection{第1回実演『君をのせて』}

\subsubsection{実演環境} \label{sec:ensou}

最初の実演は、福知山公立大学の3201教室において実施した。
室内には演奏者を中央に配置し、その周囲四方向にDMX対応照明機器を設置した。
照明は床方向から天井に向けて照射される構成とし、四方から演奏者を照らすような光環境を作り出した。
システムは\tabref{tab:kizai}に示す機器構成で運用した。
また、実演における各機材の配置図（上面図）は\figref{fig:setting}に示す通りである。

\begin{table}[tb]
	\centering
	\small
	\caption{実演機材構成}\label{tab:kizai}
	\begin{tabular}{c}
		\hline
		\hline
		ヴァイオリン（演奏者：筆者）   \\
		PC（Windows 11）   \\
		コンデンサマイク（音声入力）   \\
		スマートフォン（ZIG-SIM用）\\
		USB-DMXインタフェース    \\
		DMX対応照明機器4台       \\
		\hline
	\end{tabular}
\end{table}

\subsubsection{実演曲目}

実演に使用した楽曲は、『君をのせて』（作曲：久石譲）である。
ヴァイオリンソロアレンジ版を用い、演奏時間は約1分30秒である。
演奏はAメロからサビまでをカバーしている。
また、\figref{fig:takeyouwithme-score}に実演で使用した楽譜を示す。

\begin{figure}[tb]
	\centering
	\includegraphics[width=\hsize]{../fig/takeyouwithme-score.png}
	\caption{実演曲目『君をのせて』楽譜}\label{fig:takeyouwithme-score}
\end{figure}

\subsubsection{実演中の照明変化の様子} \label{sec:takeyouwithmeplay}

\figref{fig:takeyouwithme-Amelody-Bmelody}と\figref{fig:takeyouwithme-chorus}に、実演中の照明変化の様子を示す。

\noindent \textbf{(1) Aメロ～Bメロ部分における照明の様子}

AメロからBメロにかけての区間では、Brightnessは比較的低から中程度に抑えられており、照明は白色光を中心とした控えめな発光状態であった。
この区間では、RGB成分の変化は小さく、特定の色成分が強調される様子はほとんど見られなかった。

また、Aメロ〜Bメロ区間では照明の点滅はほとんど確認されず、連続点灯に近い安定した照明状態が維持されていた。

照明の上下動作については、演奏者の運弓動作に同期して、上げ弓の際には照明が上方を向き、下げ弓の際には下方を向く動作が繰り返し行われていた。

\noindent \textbf{(2) サビ部分における照明の様子}

サビ部分では、演奏音量および音の密度が大きくなるのに伴い、Brightnessが明確に上昇した。
また、照明は紫色を基調とした発光へと変化し、赤成分および青成分が相対的に強調された色調となった。

さらにサビ区間では、Brightnessの上昇に加えて照明の点滅速度にも変化が見られた。
点滅は高速なストロボ状ではなく、比較的短い周期で明暗が変化する緩やかな点滅となっていた。

さらに、この区間においても運弓動作に応じた照明の上下動作は継続していたが、演奏のテンポが上がるにつれて時間のずれが生じ、完全な同期は維持されなかった。

\begin{figure}[tb]
	\centering
	\includegraphics[width=\hsize]{../fig/takeyouwithme-Amelody-Bmelody.png}
	\caption{『君をのせて』実演中の照明変化の様子（Aメロ～Bメロ）}\label{fig:takeyouwithme-Amelody-Bmelody}
\end{figure}

\begin{figure}[tb]
	\centering
	\includegraphics[width=\hsize]{../fig/takeyouwithme-chorus.png}
	\caption{『君をのせて』実演中の照明変化の様子（サビ）}\label{fig:takeyouwithme-chorus}
\end{figure}

\subsection{第2回実演『紅蓮華』}

\subsubsection{実演環境}

次の実演は、京都府福知山市の新町商店街にある「Tsunaga Room\cite{TsunagaRoom}」にて実施した。
この実演は、一般来場者を含む環境で行われた公開実演である。
また、使用機材の配置や照射方法は\ref{sec:ensou}項と同様である。

\subsubsection{実演曲目}

実演に使用した楽曲は、『紅蓮華』（作曲：草野華余子）である。
ヴァイオリンソロアレンジ版を用い、演奏時間は約1分30秒である。
演奏はイントロから1番のサビまでをカバーしている。
また、\figref{fig:gurenge-score}に実演で使用した楽譜を示す。

\begin{figure}[tb]
	\centering
	\includegraphics[width=\hsize]{../fig/gurenge-score.png}
	\caption{実演曲目『紅蓮華』楽譜}\label{fig:gurenge-score}
\end{figure}

\subsubsection{実演中の照明変化の様子} \label{sec:gurengeplay}

\figref{fig:gurenge-play}に、実演中の照明変化の様子を示す。

\begin{figure*}[tb]
	\centering
	\includegraphics[width=\hsize]{../fig/gurenge-play.png}
	\caption{『紅蓮華』実演中の照明変化の様子（左から「イントロ」、「Aメロ～Cメロ」、「サビ」、「サビ」）}\label{fig:gurenge-play}
\end{figure*}

\noindent \textbf{(1) イントロ部分における照明の様子}

演奏開始直後のイントロ部分では、演奏音の音量が全体を通して低い水準で推移しており、音量変動も小さい区間である。
このため、機械学習モデルの出力として得られるBrightness値は低く抑えられ、照明は全体的に暗い状態が維持されている。

RGB成分については、RGB値のうちB成分が相対的に高く、RおよびG成分は低い値に制御されている。
その結果、照明は青色系を基調とした色調となり、急激な色相変化は発生していない。

また、この区間では点滅速度は低い値に設定されており、照明はほぼ連続点灯状態を保っていた。
そのため、点滅による視覚的変化はほとんど認識されず、静的な照明表現が支配的であった。

運弓方向に連動した照明の上下動作についても、イントロ部分では運弓速度が比較的緩やかであるため、照明の上下移動は演奏動作と概ね一致していた。

\noindent \textbf{(2) Aメロ～Cメロ部分における照明の様子}

演奏が進行し、中盤部分に入ると、音量の平均および最大値が徐々に増加し、それに伴ってBrightnessも段階的に上昇していることが確認できる。

また、音色の変化に対応してRGB成分にも変化が生じており、特にR成分の増加が顕著である。
その結果、照明の色調は青系から赤系へと移行している。

点滅速度については、音響特徴量の変化に応じて内部的には変動しているものの、その変化幅は小さく、BrightnessやRGB成分の変化と比較すると視覚的にはあまり顕著には現れていなかった。
そのため、この区間においても点滅速度は補助的な要素に留まり、主な視覚的変化は色調および輝度の変化によって構成されている。

運弓方向に連動した照明の上下動作は、中盤部においても引き続き確認できたが、演奏テンポが上がるにつれて、照明の上下移動と運弓動作との間に生じるラグがやや顕在化した。

\noindent \textbf{(3) サビ部分における照明の様子}

さらに、楽曲がサビ部分に近づくにつれて、音量は高い値を維持するとともに、短時間内での変動幅も大きくなっている。
このような音響的特徴を受けて、モデルはBrightnessを高水準に設定され、照明は高輝度状態で制御されている。

加えて、音響特徴量の変動に応じてRGB成分が時間的に大きく変化しており、紫色や黄色など複数の色がフレーム単位で切り替わる挙動が確認できる。

一方で、点滅速度については、音響的には最も変動が大きい区間であるにもかかわらず、照明表現として顕著な点滅が確認される場面は少なかった。

運弓方向に連動した照明の上下動作については、サビ部分では運弓速度が大きくなるため、照明の移動が追従できない場面が複数確認された。
上下動作には明確なラグが生じ、結果として照明の動きと演奏動作の対応関係が視覚的に把握しにくくなる傾向が見られた。


\section{考察} \label{sec:think}

\subsection{実演における照明変化と照明予測モデルの整合性}

本節では、\ref{sec:takeyouwithmeplay}項と\ref{sec:gurengeplay}項で示した実演における照明変化の様子を、\ref{sec:analysis}節で整理した音響特徴量と照明特徴量の対応関係に基づいて評価する。

\subsubsection{音量変化とBrightness制御の対応}

演奏の強弱に応じて Brightness が滑らかに増減していることが確認された。
特にサビ部分など音量が大きくなる区間では照明全体の明度が上昇し、音量が抑えられる区間では相対的に暗くなる傾向が見られた。

この挙動は、\ref{sec:analysis}節で示した Brightness が RMS（音量）に強く依存するという照明予測モデルの特性と一致している。
したがって、本実演においては、音量変化に基づく照明の明暗制御が、学習された音響特徴量と照明特徴量の対応関係に概ね沿って動作していたといえる。

\subsubsection{照明色の変化と演奏内容との関係}

計2回の実演結果から、照明色が楽曲の進行に伴って単調に変化するものではないことが確認された。
特に、『紅蓮華』の実演結果では、音量変化が小さい場面においても照明色が変化する箇所が確認された。

このことから、照明色の変化は音量のみで決定されているわけではないことが考えられる。
\ref{sec:analysis}節で示したように、RGB 各成分は音量に加え、音色や和声構造に関連する音響特徴量の影響を受ける。
実演において観察された色調変化は、これら複数の音響特徴量に基づく照明予測モデルの挙動と整合的である。

\subsubsection{点滅速度の変化に関する実演結果の解釈}

『紅蓮華』の実演結果からは、照明の点滅速度が楽曲全体を通じて大きな視覚的変化として現れにくい傾向が確認された。

イントロ部分では、音量および音量変動も小さかったため、点滅速度は低い値に設定され、照明はほぼ連続点灯状態を維持していた。
この区間では、点滅による明確な視覚的変化はほとんど認識されず、静的な照明表現が支配的であった。

AメロからCメロにかけては、音量および音色の変化に伴い内部的には点滅速度が変動していた推測される。
しかし、その変化幅は小さく、Brightness や RGB 成分の変化と比較すると視覚的な影響は限定的であった。
そのため、この区間においても点滅速度は主要な表現要素とはならず、照明表現は主に輝度および色調の変化によって構成されていた。

さらにサビ部分では、音量および音響特徴量の変動が最も大きい区間であったにもかかわらず、顕著な点滅表現が明確に確認される場面は少なかった。

以上より、演奏する楽曲によっては、点滅速度が視覚的に顕著な表現要素として機能しにくい場合があることが示唆された。

\subsection{実演を通じたシステムの有効性の評価}

本節では、\ref{sec:design-policy}節で示した設計方針と、計2回の実演結果を踏まえ、本研究で構築したリアルタイム照明制御システムの有効性について考察する。

\subsubsection{音響特徴量と照明特徴量の対応に基づく制御の妥当性}

\ref{sec:design-policy}節（1）で述べたように、本システムでは演奏表現を連続的に反映可能な照明制御を実現するため、音響特徴量と照明特徴量の対応関係を学習する照明予測モデルを導入した。

実演結果からは、演奏の強弱などに応じて照明の色や明るさが滑らかに変化している様子が観察された。
特に、音量が増加する場面ではBrightnessやRGB成分が強調され、音楽的な盛り上がりが視覚的にも明確に表現されていた。

これらの結果から、本システムにおける照明制御は単純なルールベースではなく、演奏内容の多様な側面を反映した連続的な制御として機能しており、設計方針に基づく制御の妥当性が実演を通して確認されたといえる。

\subsubsection{演奏者操作を必要としない照明制御の有効性}

\ref{sec:design-policy}節（2）で述べたように、本システムでは演奏中に人手操作を必要としないことを重要な設計方針としている。

実演結果から、演奏者が照明操作を一切行うことなく、演奏音および運弓動作のみによって照明が制御される構成が実際の演奏環境で機能することが確認された。
特に、演奏者が演奏表現に集中した状態でも照明が追従して変化しており、演奏と照明が分離した操作対象としてではなく、一体的な表現要素として知覚された点は重要である。

この結果から、本システムは小規模演奏や個人演奏において照明オペレーターを必要とせず、演奏者の負担を増加させることなく照明演出を実現できる点で有効であると評価できる。

\subsubsection{リアルタイム照明制御としての実用性}

\ref{sec:design-policy}節（3）で述べたように、本システムでは演奏と照明の同期を損なわないため、処理遅延を極力抑える設計を採用している。
更新周期は約10msに設定されており、人間が視覚的遅延を知覚しにくい範囲に収まっている。

実演においても、演奏音の特徴量の変化に関しては、照明が即応して変化する様子が確認された。
これにより、演奏と照明がリアルタイムに同期している印象が強く、視覚的な一体感が得られている。

このことから、本システムはリアルタイム照明制御として実用的な性能を有していると評価できる。

\subsection{実演を通じて明らかになった課題}

一方で、実演結果からはいくつかの課題も確認された。

\subsubsection{照明制御における課題}

まず、音量変化が小さい一方で音楽的に重要なフレーズ（抑揚や表情付け）に対して、照明変化が十分に反映されない部分が一部で確認された。
この要因として、音量以外の音響特徴量（スペクトル重心や倍音構造など）が照明制御に与える寄与が限定的であったことが考えられる。
すなわち、本手法では音量情報への依存度が相対的に高く、微細な音楽的表現を照明変化として反映するには不十分である可能性が示唆される。

さらに、『紅蓮華』の実演中の照明挙動を観察した結果、『君をのせて』の実演時と比較して、点滅速度の変化が明確に認識される場面が全体として少なかった。
本システムにおける点滅速度は、音響特徴量のうち chroma の時間的変動を主な入力として制御されている。
『紅蓮華』のメロディは主に単音旋律で構成されており、和声的な転調や急激な音高構造の変化が少ない。
そのため、chroma 各成分の分布は演奏全体を通して大きく変動せず、結果として点滅速度制御に反映される変化量も小さくなったと考えられる。

このように、演奏する楽曲によっては、特定の照明特徴量が十分に変化しない可能性がある点は課題として挙げられる。

\subsubsection{演奏動作取得における課題}

ZIG-SIMを用いた運弓方向推定は概ね有効であったが、演奏中の細かな動作や急激な動きに対して、誤判定が生じる可能性も確認された。
特に、弓の動きが小さい場合や、角速度が閾値付近にある場合には、上下動作が不安定になることがあった。

また、2回の実演を通して、テンポが速く、運弓速度が大きくなる区間では、照明の移動が演奏動作に追従しきれず、上下動作が遅延あるいは追従不能となる場面が見られた。
その結果、運弓方向と照明動作の対応関係が視覚的に把握しづらくなる傾向があった。
これは、ジャイロセンサの解析処理、および照明機器の物理的応答に要する時間が累積したことによるものと考えられる。

\subsection{今後の展望}

実演を通じて明らかになった課題を踏まえると、本研究は今後さらなる発展の余地がある。
以下に、今後の展望として考えられる方向性を示す。

第1に、照明予測モデルの高度化が挙げられる。
本研究では、Random Forest を用いて音響特徴量から照明特徴量を推定したが、本モデルは時間的な文脈を明示的に扱うものではない。
そのため、演奏フレーズ全体の流れや、クレッシェンド・デクレッシェンドといった中長期的な音楽表現を十分に反映できていない可能性がある。
今後は、LSTM や Transformer などの時系列モデルを導入し、過去の演奏状態を考慮した照明制御を行うことで、より音楽的文脈に即した照明演出が実現できると考えられる。

第2に、照明特徴量の多様化が挙げられる。
本研究では、弓の上下動作のみを照明の上下動作に反映したが、ヴァイオリン演奏には、弓速、弓圧、ビブラート動作、演奏姿勢の変化など、多様な身体表現が存在する。
今後、加速度センサや姿勢推定技術などを用いてこれらの動作情報を取得し、照明の動きや広がり、フォーカス制御などに対応付けることで、身体表現と照明表現の結びつきをさらに強化できると考えられる。


\section{結び} \label{sec:result}

本研究では、ヴァイオリン演奏に基づいて照明をリアルタイムに制御するインタラクティブ照明演出システムの構築を目的とし、音響特徴量および演奏動作情報を用いた照明制御手法を提案・実装・実演を通して検証した。

提案手法では、ヴァイオリン演奏音から抽出した音響特徴量を入力として、照明のRGB値、Brightness、点滅といった複数の照明パラメータを同時に予測する機械学習モデルを構築した。
また、ZIG-SIMを用いて演奏者の運弓動作を取得し、照明の物理的な上下動作に反映させることで、音響情報だけでは表現しきれない身体的表現を照明演出に取り込んだ。

実演では、屋内会場において楽曲『君をのせて』と『紅蓮華』を演奏し、照明演出の表現力およびリアルタイム性を検証した。
これらの実演を通して、演奏音および演奏動作に応じて照明が自律的に変化し、演奏と照明が同期する演出が実現できることを確認した。

以上より、本研究は、音楽演奏と照明演出をリアルタイムに結びつける一つの実践的手法を提示し、演奏者の表現を視覚的に拡張する可能性を示した点に意義があると結論づけられる。


\begin{acknowledgment}
	本論文を執筆するにあたり、多くの方々からご指導・ご支援を賜りました。
	ここに深く感謝の意を表します。
	まず、本研究を進めるにあたり、研究の方向性から終始丁寧なご指導と貴重なご助言をくださった橋田光代准教授に心より御礼申し上げます。
	また、研究室の4年生の皆様には、日頃より温かく接していただき、研究の相談にも快く応じていただきました。
	皆様との和やかな会話は、研究生活の大きな支えとなりました。
	心より感謝いたします。
	最後に、本研究に関わってくださったすべての方々に、厚く御礼申し上げます。
\end{acknowledgment}

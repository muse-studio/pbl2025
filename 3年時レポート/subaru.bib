@misc{24,
  title = {{【無料で使える】AIが自動で作曲してくれるサイト・アプリ・ソフト14選！音楽を作るコツも紹介}},
  year = 2024,
  month = dec,
  urldate = {2025-01-07},
  abstract = {AI作曲ツールを利用すれば、短期間で質の高い楽曲を簡単に作成できます。本記事ではおすすめのAI作曲ツールや注意点などを紹介します。},
  howpublished = {https://jitera.com/ja/insights/47128},
  langid = {japanese}
}

@article{Ariza,
  title = {{{MODELING BEATS}}, {{ACCENTS}}, {{BEAMS}}, {{AND TIME SIGNATURES HIERARCHICALLY WITH MUSIC21 METER OBJECTS}}},
  author = {Ariza, Christopher and Cuthbert, Michael Scott},
  abstract = {The music21 TimeSignature object represents meters hierarchically, through independent display, beam, beat, and accent attributes capable of unlimited partitioning and nesting. This model, designed for applications in computer-aided musicology, accommodates any variety of compound, complex, or additive meters, can report beat position and accent levels, and can algorithmically perform multi-level beaming or various types of metrical analysis. As part of the music21 Python toolkit, the meter module can read input from Humdrum and MusicXML and output to MusicXML and Lilypond.},
  langid = {english}
}

@article{Ariza11,
  title = {{{ANALYTICAL AND COMPOSITIONAL APPLICATIONS OF A NETWORK-BASED SCALE MODEL IN MUSIC2}}},
  author = {Ariza, Christopher and Cuthbert, Michael Scott},
  year = 2011,
  abstract = {Like many artifacts of music notation and theory, musical scales are easy to represent in software for simple cases, but rapidly become very difficult for more complex cases (e.g., melodic minor scale, Indian ragas, or microtonal scales). The BoundIntervalNetwork and Scale objects of the open-source music21 toolkit provide new and powerful tools for abstracting and manipulating scales as used in actual compositions. Using a novel application of a constrained node-and-edge graph model, with intervals on edges and probability weightings on nodes, music21 interval networks aid analysts in searching and annotating otherwise difficult-to-find moments in musical pieces (regardless of representational encoding) and can help composers in writing new pieces that conform to complex, asymmetrical, nonoctave-bound, and even non-deterministic scalar models. This paper introduces the low-level BoundIntervalNetwork and high-level Scale objects, and, through numerous examples both in Python code and musical notation, demonstrates their usage and potential.},
  langid = {english}
}

@article{Cuthbert,
  title = {{{FEATURE EXTRACTION AND MACHINE LEARNING ON SYMBOLIC MUSIC USING THE}} Music21 {{TOOLKIT}}},
  author = {Cuthbert, Michael Scott and Ariza, Christopher and Friedland, Lisa},
  abstract = {Machine learning and artificial intelligence have great potential to help researchers understand and classify musical scores and other symbolic musical data, but the difficulty of preparing and extracting characteristics (features) from symbolic scores has hindered musicologists (and others who examine scores closely) from using these techniques. This paper describes the ``feature'' capabilities of music21, a general-purpose, open source toolkit for analyzing, searching, and transforming symbolic music data. The features module of music21 integrates standard featureextraction tools provided by other toolkits, includes new tools, and also allows researchers to write new and powerful extraction methods quickly. These developments take advantage of the system's built-in capacities to parse diverse data formats and to manipulate complex scores (e.g., by reducing them to a series of chords, determining key or metrical strength automatically, or integrating audio data). This paper's demonstrations combine music21 with the data mining toolkits Orange and Weka to distinguish works by Monteverdi from works by Bach and German folk music from Chinese folk music.},
  langid = {english}
}

@article{Cuthbert11,
  title = {{{FEATURE EXTRACTION AND MACHINE LEARNING}}},
  author = {Cuthbert, Michael Scott and Ariza, Christopher and Friedland, Lisa},
  year = 2011,
  journal = {Poster Session},
  abstract = {Machine learning and artificial intelligence have great potential to help researchers understand and classify musical scores and other symbolic musical data, but the difficulty of preparing and extracting characteristics (features) from symbolic scores has hindered musicologists (and others who examine scores closely) from using these techniques. This paper describes the ``feature'' capabilities of music21, a general-purpose, open source toolkit for analyzing, searching, and transforming symbolic music data. The features module of music21 integrates standard featureextraction tools provided by other toolkits, includes new tools, and also allows researchers to write new and powerful extraction methods quickly. These developments take advantage of the system's built-in capacities to parse diverse data formats and to manipulate complex scores (e.g., by reducing them to a series of chords, determining key or metrical strength automatically, or integrating audio data). This paper's demonstrations combine music21 with the data mining toolkits Orange and Weka to distinguish works by Monteverdi from works by Bach and German folk music from Chinese folk music.},
  langid = {english}
}

@article{Demopoulos,
  title = {Music {{Information Retrieval}}: {{A Survey}} of {{Issues}} and {{Approaches}}},
  author = {Demopoulos, Ryan J and Katchabaw, Michael J},
  langid = {english}
}

@misc{Infinitebach,
  title = {Infinite-Bach},
  author = {{jamesrobertlloyd}},
  year = 2025,
  month = jan,
  urldate = {2026-01-15},
  howpublished = {https://github.com/jamesrobertlloyd/infinite-bach}
}

@article{Kumamoto06,
  title = {Design, {{Implementation}}, and {{Opening}} to the {{Public}} of an {{Impression-Based Music Retrieval System}}},
  author = {Kumamoto, Tadahiko and Ohta, Kimiko},
  year = 2006,
  journal = {Transactions of the Japanese Society for Artificial Intelligence},
  volume = {21},
  pages = {310--318},
  issn = {1346-0714, 1346-8030},
  urldate = {2025-10-21},
  abstract = {Impression-based music retrieval helps users in finding musical pieces that suit their preferences, feelings, or mental states from the huge volume of a music database. We have therefore developed an impression-based music retrieval system that enables this. Users are asked to select one or more pairs of impression words from the multiple pairs presented by the system and estimate each of the selected pairs on a seven-step scale in order to input their impressions into the system. For instance, if they want to locate musical pieces that will create a happy impression, they should check the radio button ``Happy'' in the impression scale, ``Very happy -- Happy -- A little happy -- Neutral -- A little sad -- Sad -- Very sad,'' where a pair of impression words with a seven-step scale is called an ``impression scale'' in this paper. The system would measure the distance between the impressions of every musical piece in a user-specified music database and the impressions inputted by the user, and determine candidate musical pieces to be presented as retrieval results. In this paper, we define the form of vectors that numerically express impressions of musical pieces, and propose a method of generating such a vector from a musical piece. The most significant attribute of this method is that it uses n-gram statistics of information on pitch, strength, and length of every tone in that musical piece as features extracted from it. We also present the results of evaluating the performance of the system.},
  langid = {english}
}

@misc{MAESTRODataset2018,
  title = {The {{MAESTRO Dataset}}},
  year = 2018,
  month = oct,
  journal = {Magenta},
  urldate = {2026-01-16},
  abstract = {MAESTRO (MIDI and Audio Edited for Synchronous TRacks and Organization) is a dataset composed of about 200 hours of virtuosic piano performances captured wit...},
  howpublished = {https://magenta.withgoogle.com/datasets/maestro},
  langid = {english}
}

@article{Simonetta23,
  title = {{{OPTIMIZING FEATURE EXTRACTION FOR SYMBOLIC MUSIC}}},
  author = {Simonetta, Federico and Llorens, Ana and Serrano, Mart{\'i}n and {Garc{\'i}a-Portugu{\'e}s}, Eduardo and Torrente, {\'A}lvaro},
  year = 2023,
  abstract = {This paper presents a comprehensive investigation of existing feature extraction tools for symbolic music and contrasts their performance to determine the set of features that best characterizes the musical style of a given music score. In this regard, we propose a novel feature extraction tool, named musif, and evaluate its efficacy on various repertoires and file formats, including MIDI, MusicXML, and **kern. Musif approximates existing tools such as jSymbolic and music21 in terms of computational efficiency while attempting to enhance the usability for custom feature development. The proposed tool also enhances classification accuracy when combined with other sets of features. We demonstrate the contribution of each set of features and the computational resources they require. Our findings indicate that the optimal tool for feature extraction is a combination of the best features from each tool rather than those of a single one. To facilitate future research in music information retrieval, we release the source code of the tool and benchmarks.},
  langid = {english}
}

@article{Temperley99,
  title = {What's {{Key}} for {{Key}}? {{The Krumhansl-Schmuckler Key-Finding Algorithm Reconsidered}}},
  shorttitle = {What's {{Key}} for {{Key}}?},
  author = {Temperley, David},
  year = 1999,
  month = oct,
  journal = {Music Perception},
  volume = {17},
  number = {1},
  pages = {65--100},
  issn = {0730-7829},
  urldate = {2025-11-11},
  abstract = {This study examines the Krumhansl-Schmuckler key-finding model, in which the distribution of pitch classes in a piece is compared with an ideal distribution or "key profile" for each key. Several changes are proposed. First, the formula used for the matching process is somewhat simplified. Second, alternative values are proposed for the key profiles themselves. Third, rather than summing the durations of all events of each pitch class, the revised model divides the piece into short segments and labels each pitch class as present or absent in each segment. Fourth, a mechanism for modulation is proposed; a penalty is imposed for changing key from one segment to the next. An implementation of this model was subjected to two tests. First, the model was tested on the fugue subjects from Bach's Well-Tempered Clavier; the model's performance on this corpus is compared with the performances of other models. Second, the model was tested on a corpus of excerpts from the Kostka and Payne harmony textbook (as analyzed by Kostka). Several problems with the modified algorithm are discussed, concerning the rate of modulation, the role of harmony in key finding, and the role of pitch "spellings." The model is also compared with Huron and Parncutt's exponential decay model. The tests presented here suggest that the key-profile model, with the modifications proposed, can provide a highly successful approach to key finding.},
  langid = {english}
}

@article{Typke,
  title = {A {{SURVEY OF MUSIC INFORMATION RETRIEVAL SYSTEMS}}},
  author = {Typke, Rainer and Wiering, Frans and Veltkamp, Remco C},
  abstract = {This survey paper provides an overview of content-based music information retrieval systems, both for audio and for symbolic music notation. Matching algorithms and indexing methods are briefly presented. The need for a TREC-like comparison of matching algorithms such as MIREX at ISMIR becomes clear from the high number of quite different methods which so far only have been used on different data collections. We placed the systems on a map showing the tasks and users for which they are suitable, and we find that existing content-based retrieval systems fail to cover a gap between the very general and the very specific retrieval tasks.},
  langid = {english}
}

@misc{zotero-14311,
  title = {Orpheus {{Ver}}. 3.25 \ding{110} 自動作曲システム オルフェウス, {{Aug}} 2024},
  urldate = {2025-01-07},
  howpublished = {https://www.orpheus-music.org/index.php}
}

@misc{zotero-14312,
  title = {Jukebox},
  urldate = {2025-01-07},
  abstract = {We're introducing Jukebox, a neural net that generates music, including rudimentary singing, as raw audio in a variety of genres and artist styles. We're releasing the model weights and code, along with a tool to explore the generated~samples.},
  howpublished = {https://openai.com/index/jukebox/},
  langid = {english}
}

@misc{zotero-14313,
  title = {{{styleSeq}} v1.1.0},
  urldate = {2025-01-07},
  howpublished = {https://styleseq.mamenone.com/help/help\_ja.html}
}

@misc{zotero-14314,
  title = {{{MuseNet}}},
  urldate = {2025-01-07},
  abstract = {We've created MuseNet, a deep neural network that can generate 4-minute musical compositions with 10 different instruments, and can combine styles from country to Mozart to the Beatles. MuseNet was not explicitly programmed with our understanding of music, but instead discovered patterns of harmony, rhythm, and style by learning to predict the next token in hundreds of thousands of MIDI files. MuseNet uses the same general-purpose unsupervised technology as~GPT-2, a large-scale~transformer~model trained to predict the next token in a sequence, whether audio or~text.},
  howpublished = {https://openai.com/index/musenet/},
  langid = {english}
}

@misc{zotero-14315,
  title = {{{進化する自動作曲CREEVO}}},
  urldate = {2025-01-07},
  abstract = {CREEVO（クリーボ）を使えば、ツイート文などの歌詞からオリジナルなメロディーを簡単に創れます。},
  howpublished = {https://creevo-music.com/},
  langid = {english}
}

@misc{zotero-14316,
  title = {Suno},
  urldate = {2025-01-07},
  abstract = {Suno is building a future where anyone can make great music.},
  howpublished = {https://suno.com/},
  langid = {english}
}

@misc{zotero-14317,
  title = {{Udio \textbar{} AI Music Generator - Official Website}},
  journal = {Udio},
  urldate = {2025-01-07},
  abstract = {Discover, create, and share music with the world. Use the latest technology to create AI music in seconds.},
  howpublished = {https://www.udio.com},
  langid = {japanese}
}

@misc{zotero-14538,
  title = {アイヴァ},
  urldate = {2025-02-01},
  howpublished = {https://creators.aiva.ai/}
}

@misc{zotero-item-14310,
  title = {{{IRT Corporation}}},
  urldate = {2025-01-07},
  howpublished = {https://irtnet.jp/music/sakkyoku4/}
}

@misc{zotero-item-14849,
  title = {The {{Lost Sunflower Field}}}
}

@misc{zotero-item-14850,
  title = {The {{Lost Sunflower Field}}}
}

@misc{zotero-item-14851,
  title = {Eternal {{Light}}}
}

@misc{zotero-item-14852,
  title = {Eternal {{Reflections}}}
}

@misc{zotero-item-14853,
  title = {Neon {{Dreams}}}
}

@misc{zotero-item-14854,
  title = {New {{Composition}} \#8}
}

@misc{zotero-item-14855,
  title = {Whispers of {{Tomorrow}} Ext v1.2.1.2.2}
}

@misc{zotero-item-14856,
  title = {Whispers of {{Tomorrow}}}
}

@misc{zotero-item-14857,
  title = {Whispers in {{Daydreams}}}
}

@misc{zotero-item-14858,
  title = {流星の軌跡フル}
}

@misc{zotero-item-14859,
  title = {流星の軌跡2025-02-18 09-51-35}
}

@misc{zotero-item-14860,
  title = {流星の軌跡}
}

@misc{zotero-item-14861,
  title = {流星}
}

@article{zotero-item-14862,
  title = {Aaaa+}
}

@article{zotero-item-15328,
  title = {{Journal of Music Perception and Cognition 30(1): 3-18 (2024)}},
  abstract = {A computational model for three-note chord（triad）perception called Psychophysical Model of Chord Perception（PMCP）was proposed by Cook and Fujisawa（2006）to quantitatively estimate the listening impression from the acoustic features. We extended this model to four-note chords. In addition, we have validated the results using a listening experiment to evaluate the validity of extending PMCP for triads to four-note chords. The results of factor analysis and correlation analysis on the data obtained from the experiment suggest that the impression of four-note chords are well associated with PMCP, similarly to the cases of triads.},
  langid = {japanese}
}
